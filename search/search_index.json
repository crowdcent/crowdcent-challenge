{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CrowdCent Challenge Documentation","text":"<p>Welcome to the documentation for the <code>crowdcent-challenge</code>. The CrowdCent Challenge is a data science competition designed for machine learning engineers, data scientists, and other technical professionals to hone their skills in a real-world setting.</p> <p>Get started, learn about how we score submissions, explore the python client, or dive into the API reference.</p> <ul> <li> <p> Getting Started</p> <p>Make your first submission to the CrowdCent Challenge.</p> <p> Get Started</p> </li> <li> <p> Scoring System</p> <p>Learn how submissions are evaluated and how leaderboard rankings work.</p> <p> View Scoring Details</p> </li> <li> <p> Python Client Quick Start</p> <p>Install the client library and make your first submission programmatically.</p> <p> Quick Start Guide</p> </li> <li> <p> FAQ</p> <p>Frequently asked questions about the challenge, API Key, and more.</p> <p> View FAQ</p> </li> </ul>"},{"location":"#have-questions","title":"Have Questions?","text":"<p>If you need immediate assistance, please email us at info@crowdcent.com or join our Discord server.</p>"},{"location":"#want-to-contribute","title":"Want to Contribute?","text":"<p>These docs and the python client are open source! Anyone can submit to this documentation or improve the python client and CLI for the <code>crowdcent-challenge</code> API. Please see the contributing guidelines for more information. If you'd like to join the conversation, please join our Discord server.</p>"},{"location":"about/","title":"About CrowdCent","text":"<p>CrowdCent is on a mission to decentralize investment management by changing the way investment funds make decisions and allocate capital. We are the machine learning and coordination layer for online investment communities looking to turn their data into actionable, investable portfolios.</p> <p>More information about CrowdCent can be found on crowdcent.com. </p>"},{"location":"ai-agents-mcp/","title":"Using AI Agents","text":"<p>CrowdCent provides a Model Context Protocol (MCP) server that enables direct interaction with the CrowdCent Challenge API from AI agents and assistants like Cursor or Claude Desktop. This allows you to use natural language to perform challenge-related-tasks such as downloading data, training models, and submitting predictions.</p> <p>The MCP server is a separate, open-source project.</p> <ul> <li>Repository: crowdcent/crowdcent-mcp</li> </ul> <p></p>"},{"location":"ai-agents-mcp/#example-prompts","title":"Example Prompts","text":"<p>Once the server is running and your agent is configured, you can use natural language prompts.</p> <p>Example Prompts:</p> <pre><code>\"Download the latest CrowdCent training data and show me the first 5 rows.\"\n</code></pre> <pre><code>\"Submit predictions from 'predictions.parquet' to the CrowdCent challenge.\"\n</code></pre>"},{"location":"ai-agents-mcp/#installation","title":"Installation","text":"<ol> <li> <p>Clone the server repository:     <pre><code>git clone https://github.com/crowdcent/crowdcent-mcp.git\ncd crowdcent-mcp\n</code></pre></p> </li> <li> <p>Install dependencies:     The project uses <code>uv</code> for package management.     <pre><code>uv pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"ai-agents-mcp/#configuration","title":"Configuration","text":""},{"location":"ai-agents-mcp/#api-key","title":"API Key","text":"<p>The MCP server requires your CrowdCent API key.</p> <ol> <li>Create a <code>.env</code> file in the <code>crowdcent-mcp</code> directory.</li> <li>Add your key to the file:     <pre><code>CROWDCENT_API_KEY=your_api_key_here\n</code></pre></li> </ol> <p>You can get an API key from your CrowdCent Account Page.</p>"},{"location":"ai-agents-mcp/#agent-setup","title":"Agent Setup","text":"<p>You'll need to point your AI agent to the local MCP server.</p> CursorClaude Desktop <p>To integrate the MCP server with Cursor:</p> <ol> <li>Open your Cursor settings (<code>~/.cursor/mcp.json</code> or through the UI).</li> <li> <p>Add the following server configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"crowdcent-mcp\": {\n      \"command\": \"/path/to/your/uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\", \"/path/to/crowdcent-mcp\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n</code></pre> </li> </ol> <p>For Claude Desktop, add the following to your configuration file:</p> <ul> <li>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></li> <li>Linux: <code>~/.config/Claude/claude_desktop_config.json</code></li> </ul> <pre><code>{\n  \"mcpServers\": {\n    \"crowdcent-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\", \"/path/to/crowdcent-mcp\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Use Absolute Paths</p> <p>In all configurations, replace <code>/path/to/your/uv</code> and <code>/path/to/crowdcent-mcp</code> with the absolute paths on your system. For example, <code>uv</code> is often located at <code>~/.cargo/bin/uv</code>.</p> <p>For detailed troubleshooting, please refer to the crowdcent-mcp repository.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#013-2025-05-05","title":"0.1.3 (2025-05-05)","text":"<ul> <li>Added <code>symmetric_ndcg</code> scoring metric.</li> </ul>"},{"location":"changelog/#010-2025-04-15","title":"0.1.0 (2025-04-15)","text":"<ul> <li>Initial release.</li> <li>Added <code>ChallengeClient</code> with methods for datasets and submissions.</li> <li>Added CLI interface (<code>crowdcent-challenge</code>).</li> <li>Added basic documentation with MkDocs. </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! The <code>crowdcent-challenge</code> API client and documentation are open-source projects and contributions can be as simple as a fact check or as complex as a new feature.</p> <p>Here's a breakdown of the standard GitHub workflow:</p> <ol> <li>Fork the repository</li> <li>Clone your fork:    <pre><code>git clone https://github.com/&lt;your-username&gt;/crowdcent-challenge.git\ncd crowdcent-challenge\nuv pip install -e .[dev]\n</code></pre></li> <li>Branch off:    <pre><code>git checkout -b my-feature\n</code></pre></li> <li>Make changes &amp; test</li> <li>Commit with clear messages:    <pre><code>git commit -m \"feat: Add new feature\"\n</code></pre></li> <li>Push &amp; open a PR from your fork     <pre><code>git push origin my-feature\n</code></pre></li> </ol> <p>Include tests where appropriate. Keep PRs focused - one feature/fix per PR.</p>"},{"location":"disclaimer/","title":"Disclaimer","text":"<p>Under no circumstances should any information provided in this software \u2014 or on associated distribution outlets \u2014 be construed as an offer soliciting the purchase or sale of any security or interest in any pooled investment vehicle sponsored, discussed, or mentioned by CrowdCent LLC or affiliates. Nor should it be construed as an offer to provide investment advisory services; an offer to invest in a CrowdCent investment vehicle will be made separately and only by means of the confidential offering documents of the specific pooled investment vehicles \u2014 which should be read in their entirety, and only to those who, among other requirements, meet certain qualifications under federal securities laws. Such investors, defined as accredited investors and qualified purchasers, are generally deemed capable of evaluating the merits and risks of prospective investments and financial matters. There can be no assurances that CrowdCent's investment objectives will be achieved or investment strategies will be successful. Any investment in a vehicle managed by CrowdCent involves a high degree of risk including the risk that the entire amount invested is lost. Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by CrowdCent and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results.</p>"},{"location":"equity-nlp/","title":"Equity NLP &#128683;","text":"<p>Coming soon!</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#general-getting-started","title":"General &amp; Getting Started","text":""},{"location":"faq/#what-is-the-crowdcent-challenge","title":"What is the CrowdCent Challenge?","text":"<p>The CrowdCent Challenge is a series of open data science competitions (challenges) focused on predicting investment/market outcomes. Participants use various datasets to build machine learning models that predict future returns over various time horizons. Submissions are used to create meta-models that can be turned into investable portfolios.</p>"},{"location":"faq/#how-do-i-get-started","title":"How do I get started?","text":"<p>Refer to the Installation &amp; Quick Start Guide for detailed steps.</p> <ol> <li>Install the client library: We recommend using uv: <code>uv pip install crowdcent-challenge</code>. Alternatively, use <code>pip install crowdcent-challenge</code>.</li> <li>Get an API Key: Visit your CrowdCent profile page and generate a new key. Save it securely.</li> <li>Set up Authentication: Provide your API key either when initializing the Python <code>ChallengeClient</code>, setting the <code>CROWDCENT_API_KEY</code> environment variable, or placing it in a <code>.env</code> file (<code>CROWDCENT_API_KEY=your_key_here</code>) in your project directory.</li> <li>Explore: Use the Python client or the <code>crowdcent</code> CLI to list available challenges (<code>crowdcent list-challenges</code>).</li> <li>Download Data: Choose a challenge and download the training and inference data using the client or CLI (e.g., <code>crowdcent download-training-data &lt;challenge_slug&gt; latest</code>).</li> <li>Build &amp; Submit: Train your model and submit your predictions in the required format.</li> </ol>"},{"location":"faq/#who-can-participate","title":"Who can participate?","text":"<p>The challenge is open to anyone interested in data science, machine learning, and finance. Check the terms of service for more details.</p>"},{"location":"faq/#api-key-authentication","title":"API Key &amp; Authentication","text":""},{"location":"faq/#how-do-i-get-an-api-key","title":"How do I get an API Key?","text":"<p>Go to your profile page on the CrowdCent website after logging in. Click the \"Generate New Key\" button.</p>"},{"location":"faq/#how-do-i-use-my-api-key","title":"How do I use my API Key?","text":"<p>The <code>crowdcent-challenge</code> library (both Python client and CLI) automatically looks for your API key in the following order:</p> <ol> <li>Passed directly to the <code>ChallengeClient</code> initializer (<code>api_key=...</code>).</li> <li>The <code>CROWDCENT_API_KEY</code> environment variable.</li> <li>A <code>.env</code> file in your current working directory containing <code>CROWDCENT_API_KEY=your_key_here</code>.</li> </ol>"},{"location":"faq/#what-if-my-api-key-doesnt-work","title":"What if my API key doesn't work?","text":"<p>Ensure you copied the key correctly and included the <code>ApiKey</code> prefix if using tools like Swagger UI directly (the client library handles this automatically). Verify it hasn't been revoked on your profile page. If issues persist, generate a new key or contact support.</p>"},{"location":"faq/#python-client-vs-cli","title":"Python Client vs. CLI","text":""},{"location":"faq/#whats-the-difference-between-the-challengeclient-python-and-the-crowdcent-cli","title":"What's the difference between the <code>ChallengeClient</code> (Python) and the <code>crowdcent</code> CLI?","text":"<p>They both interact with the same CrowdCent API.</p> <ul> <li><code>ChallengeClient</code> (Python): Designed for programmatic use within your modeling scripts or notebooks. Ideal for automating data downloads, processing, and prediction uploads.</li> <li><code>crowdcent</code> (CLI): A command-line tool for manual operations like listing challenges, downloading specific data files, checking submission status, etc., directly from your terminal.</li> </ul>"},{"location":"faq/#which-one-should-i-use","title":"Which one should I use?","text":"<p>Use the <code>ChallengeClient</code> within your Python code for automation and integration with your modeling workflow. Use the <code>crowdcent</code> CLI for quick checks, manual downloads, or exploring the available challenges and data without writing Python code.</p>"},{"location":"faq/#data","title":"Data","text":""},{"location":"faq/#what-format-is-the-data-provided-in","title":"What format is the data provided in?","text":"<p>All datasets (training data, inference features, meta-models) and submission files are in the Apache Parquet (<code>.parquet</code>) format. This columnar format is efficient for the type of data used in the challenge.</p>"},{"location":"faq/#what-kind-of-features-are-included","title":"What kind of features are included?","text":"<ul> <li>Important Note: Some challenges or training datasets might only provide target labels and identifiers (<code>id</code>, <code>date</code>, <code>target_10d</code>, <code>target_30d</code>). In these cases, participants are expected to source or engineer their own relevant features.</li> <li>Features are often renamed to simple names like <code>feature_1</code>, <code>feature_2</code>, etc. Refer to the specific challenge rules for details on the data provided for each competition.</li> </ul>"},{"location":"faq/#what-am-i-predicting","title":"What am I predicting?","text":"<p>Your goal is to to predict relative performance metrics (e.g., returns) for different future time horizons based on the provided features. The required prediction columns vary by challenge, but may look like <code>pred_10d</code>, <code>pred_30d</code>. Always check the specific challenge rules for the exact requirements.</p>"},{"location":"faq/#whats-the-difference-between-training-and-inference-data","title":"What's the difference between Training and Inference data?","text":"<ul> <li>Training Data: Used to train your models. Contains historical data, including target variables (e.g., <code>target_10d</code>, <code>target_30d</code>, ...) and identifiers (<code>id</code>, dates). It may also contain pre-computed features, but sometimes you will need to generate your own features based on the provided IDs and timestamps. Training datasets are versioned (e.g. v1.0, v1.1, etc.).</li> <li>Inference Data: Contains features (if provided by the challenge) and identifiers (<code>id</code>) for a new period but without the target labels. This is the data you use (along with any features you generate) to make predictions for submission. Inference data is released periodically.</li> </ul>"},{"location":"faq/#what-is-the-meta-model","title":"What is the 'Meta-Model'?","text":"<p>The meta-model typically represents an aggregation (e.g., an average or ensemble) of all valid user submissions for past inference periods within a specific challenge. It can serve as a benchmark or potentially as an additional feature for your own models. You can download it via the client or CLI.</p>"},{"location":"faq/#submissions","title":"Submissions","text":""},{"location":"faq/#what-format-does-my-submission-file-need-to-be","title":"What format does my submission file need to be?","text":"<p>Your submission must be a Parquet file containing an <code>id</code> column that matches the IDs from the corresponding inference dataset, and all the required prediction columns (e.g., <code>pred_1M</code>, <code>pred_3M</code>, etc.). All prediction columns must contain numeric values, and no missing values are allowed.</p>"},{"location":"faq/#how-do-i-submit-my-predictions","title":"How do I submit my predictions?","text":"<ul> <li>Python: Use <code>client.submit_predictions(\"path/to/your/predictions.parquet\")</code>.</li> <li>CLI: Use <code>crowdcent submit &lt;challenge_slug&gt; path/to/your/predictions.parquet</code>. Submissions are automatically associated with the currently active inference period for the specified challenge.</li> </ul>"},{"location":"faq/#how-often-can-i-submit","title":"How often can I submit?","text":"<p>You can typically submit multiple times for an active inference period. Your latest valid submission before the deadline is the one that counts for scoring. Check the specific challenge rules.</p>"},{"location":"faq/#how-do-i-check-my-submission-status","title":"How do I check my submission status?","text":"<ul> <li>Python: Use <code>client.list_submissions()</code> (optionally filter by <code>period='current'</code> or <code>period='YYYY-MM-DD'</code>) and <code>client.get_submission(&lt;submission_id&gt;)</code>.</li> <li>CLI: Use <code>crowdcent list-submissions &lt;challenge_slug&gt;</code> (optionally filter with <code>--period current</code> or <code>--period YYYY-MM-DD</code>) and <code>crowdcent get-submission &lt;challenge_slug&gt; &lt;submission_id&gt;</code>. Statuses include \"pending\", \"processing\", \"evaluated\" (or \"scored\"), and \"error\" (or \"failed\").</li> </ul>"},{"location":"faq/#environment-troubleshooting","title":"Environment &amp; Troubleshooting","text":""},{"location":"faq/#what-version-of-python-should-i-use","title":"What version of Python should I use?","text":"<p>The client library requires Python 3.10 or higher (as specified in <code>pyproject.toml</code>).</p>"},{"location":"faq/#uv-or-pip","title":"<code>uv</code> or <code>pip</code>?","text":"<p>We recommend using <code>uv</code> for faster dependency management (<code>uv pip install ...</code>). However, standard <code>pip install ...</code> also works.</p>"},{"location":"faq/#where-can-i-get-help","title":"Where can I get help?","text":"<ul> <li>Discord: Join the CrowdCent Discord server.</li> <li>Email: Contact info@crowdcent.com.</li> </ul>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute","title":"How can I contribute?","text":"<p>Contributions to the <code>crowdcent-challenge</code> client library and its documentation are welcome! Please see the Contributing Guidelines for details on the standard GitHub workflow (fork, branch, commit, PR). </p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#register-for-a-crowdcent-account","title":"Register for a CrowdCent account","text":"<p>Sign up for a CrowdCent account here or sign in with your GitHub account. We require an email verification step to ensure the account is real. If you'd like to work with the challenge programmatically, you'll need to generate an API key from your user profile. See the client quickstart for more details.</p>"},{"location":"getting-started/#explore-challenges","title":"Explore Challenges","text":"<p>Once logged in, you'll land on the Challenge List page. Browse through the available challenges to find one that interests you. Each challenge card will give you a brief overview. Click on a challenge to see more details.</p> <p></p>"},{"location":"getting-started/#download-data","title":"Download Data","text":"<p>On the detail page for your chosen challenge (e.g. hyperliquid-ranking), you will find:</p> <ul> <li>A section to download the latest Training Data. You'll need this to train your model.</li> <li>Information about the current or most recent Inference Data period. If a period is active, you can download the inference features here.</li> </ul>"},{"location":"getting-started/#build-a-model","title":"Build a Model","text":"<p>Using the downloaded training data, build a model to predict the challenge target(s). You can refer to our tutorial notebooks (if available in the challenge description or docs) for examples.</p>"},{"location":"getting-started/#submit-predictions-during-an-inference-period","title":"Submit predictions during an Inference Period","text":"<ul> <li>The Challenge Detail page will display information about the current Inference Data period, including its release date and submission deadline.</li> <li>You have a specific number of submission slots (e.g., up to 5) for each inference period. You can choose which slot to use for each submission.</li> </ul> <p> The submission panel on the Challenge Detail page shows active inference periods and your submission slots.</p> <p>There are two main ways to submit your predictions:</p>"},{"location":"getting-started/#1-via-the-website-ui","title":"1. Via the Website (UI)","text":"<ul> <li>During an active inference period, go to the Challenge Detail page.</li> <li>In the submission panel, select an available slot.</li> <li>Upload your prediction file (typically a Parquet file).</li> <li>If you upload a file to a slot that already has a prediction, it will be overwritten.</li> </ul>"},{"location":"getting-started/#2-programmatically-via-api","title":"2. Programmatically (via API)","text":"<ul> <li>Go to your User Profile page (accessible from the top navigation bar when logged in).</li> <li>In the \"API Keys\" section, you can generate a new API key. Give it a descriptive name. Store this key securely as it will not be shown again.</li> <li>Use this API key with the <code>crowdcent-challenge</code> Python package to submit your predictions. See our client quickstart guide for more details.</li> </ul>"},{"location":"getting-started/#3-via-ai-agents-mcp-server","title":"3. Via AI Agents (MCP Server)","text":"<p>It's also possible to interact with challenges using AI agents like Cursor or Claude Desktop. This provides a natural language interface for downloading data, submitting, and more. See our AI Agents (MCP) guide for setup instructions.</p>"},{"location":"getting-started/#wait-for-scores","title":"Wait for Scores","text":"<p>After an inference period's submission deadline passes, predictions will be evaluated. Your submission status and scores will be updated on your profile and the challenge leaderboard.</p> <p>For more details on how scores are calculated and what the scores mean, see the Scoring page.</p>"},{"location":"getting-started/#check-the-leaderboard","title":"Check the Leaderboard","text":"<p>Navigate to the Leaderboard page to see how your submissions rank against other participants for each challenge. You can switch the leaderboard by challenge, sort by different scores, and view results by user or by individual submission slots.</p> <p></p>"},{"location":"getting-started/#watch-the-meta-model","title":"Watch the Meta-Model","text":"<p>For some challenges, the meta-model is published after an inference period ends. For now, this is only available for the hyperliquid-ranking challenge and may be subject to change. The meta-model represents relative signals for the investable universe.</p> <p>Meta-Model Disclaimer</p> <p>The meta-model represents CrowdCent's aggregation of participant submissions into a single model. While we strive to create robust meta-models, please note:</p> <ul> <li>Meta-models are provided for informational purposes only and should not be construed as investment advice</li> <li>Past performance of meta-models is not indicative of future results</li> <li>Meta-model methodologies may change over time without notice</li> </ul> <p></p>"},{"location":"hyperliquid-ranking/","title":"Hyperliquid Ranking","text":"<p>Zero to Submission in 20 Seconds</p> <p>Go from setup to your first prediction submission with our end-to-end tutorial notebook.</p>"},{"location":"hyperliquid-ranking/#objective","title":"Objective","text":"<p>The Hyperliquid Ranking Challenge requires participants to rank crypto assets on the Hyperliquid decentralized derivatives exchange by their expected relative returns over the next 10 and 30 days. The challenge universe comprises approximately 165-175 (and likely more in the future) liquid tokens on the Hyperliquid protocol. This universe may change periodically, with tokens added or removed to ensure it remains as actionable as possible. If a token does not have enough volume or liquidity, it will likely be removed from the universe.</p> <pre><code>from crowdcent_challenge import ChallengeClient\nclient = ChallengeClient(challenge_slug=\"hyperliquid-ranking\")\nclient.get_challenge() # Get more challenge details\n</code></pre>"},{"location":"hyperliquid-ranking/#training-data","title":"Training data","text":"<p>The training dataset is created just to get you started. Simple models can be built with just the features and targets, but don't expect to win the challenge with them. We recommend building your own training datasets with sources like ccxt, eodhd, coingecko, or yfinance.</p> <p>You can download our training data, including features and targets from crowdcent.com/challenge/hyperliquid-ranking or via the CrowdCent client: <pre><code>client.download_training_dataset(\"latest\")\n</code></pre></p> id eodhd_id date feature_1_lag15 feature_1_lag10 ... feature_1_lag0 target_10d target_30d BABY BABY32198-USD.CC 2024-03-20 0.123 0.145 ... 0.823 0.15 0.25 OM OM-USD.CC 2024-03-20 0.456 0.423 ... 0.756 0.35 0.45 IOTA IOTA-USD.CC 2024-03-20 0.789 0.812 ... 0.923 0.55 0.65 MOODENG MOODENG33093-USD.CC 2024-03-20 0.234 0.267 ... 0.445 0.75 0.85 ENS ENS-USD.CC 2024-03-20 0.567 0.534 ... 0.678 0.95 0.88"},{"location":"hyperliquid-ranking/#asset-ids","title":"Asset IDs","text":"<p>We currently provide <code>id</code> (the hyperliquid id) and <code>eodhd_id</code> (the id to download via EODHD) for each asset. You can request we include additional id mappings from other data vendors in the inference data if data licenses allow.</p> <p>If you're using CrowdCent's training data to build your models, the inference data features will always match that of the latest training dataset version. The assets are generally the same as you would find in the training data, but new additions or removals are possible. For the inference data, we aim to track the listed and tradeable perps on Hyperliquid.</p>"},{"location":"hyperliquid-ranking/#features","title":"Features","text":"<p>The training data contains 80 total features following the pattern <code>feature_{n}_lag{lag}</code>:</p> <ul> <li>20 unique features (n = 1 to 20)</li> <li>4 lag values per feature (0, 5, 10, 15 days)</li> </ul> <p>Features with the same number represent the same metric at different time points. For example, <code>feature_1_lag0</code> through <code>feature_1_lag15</code> track the same underlying metric over time. This structure preserves temporal relationships, allowing you to build sequence models (LSTM, GRU, Transformer), identify trends/patterns across different time horizons, and engineer additional features based on temporal changes.</p>"},{"location":"hyperliquid-ranking/#targets","title":"Targets","text":"<p>Targets are the rankings of an asset's 10d and 30d forward relative returns (with a 1d lag). Targets do not currently take funding rate or any other factors (e.g. market cap, volume, etc.) into account. It's possible that the targets will be updated in the future to include such factors.</p> <p>Why the 1-day lag? For our purposes, the crypto universe has a close time of 24:00 UTC. At 14:00 UTC when predictions are made, only data through the previous day's 24:00 UTC close is available. The lag ensures predictions use only historical data while forecasting returns starting from tonight's close.</p> <p>Timeline: <pre><code>Day D-1: 24:00 UTC \u2192 Close price finalized (latest available data)\nDay D:   14:00 UTC \u2192 Inference pipeline starts\n         14:00-18:00 UTC \u2192 Inference period lasts 4 hours\n         24:00 UTC \u2192 Prediction/Scoring period starts (Day D close)\nDay D+10/30: 24:00 UTC \u2192 Prediction/Scoring period ends\n</code></pre></p> <ul> <li>Predictions rank assets by expected performance over the next 10/30 days starting from tonight's close</li> <li>Rankings are relative. They say nothing about the expected absolute performance of an asset.</li> </ul>"},{"location":"hyperliquid-ranking/#inference-data","title":"Inference data","text":"<ul> <li>Inference Period Open: The internal pipeline starts at 14:00\u00a0UTC. The file usually becomes available a few seconds to a few minutes later, once data quality checks pass.</li> <li>Inference Period Close: 4 hours after the actual release timestamp (typically around 18:00\u00a0UTC).</li> </ul> <p>Each day, an inference dataset is released containing the universe of tokens for which predictions are required. The inference data contains features but has no targets as they do not exist at the time of your submission. Your predictions will be scored against resolving targets from real market data in the future.</p> <p>Polling and parameter options</p> <p>Current vs Latest: Use <code>\"current\"</code> to download today's active inference period (for making submissions). Use <code>\"latest\"</code> to download the most recently published period (which may be from a previous day if today's isn't ready yet).</p> <p>Polling: The inference file may not exist the very instant the clock strikes 14:00\u00a0UTC. If you use <code>release_date=\"current\"</code> with default parameters, polling is handled automatically. For <code>release_date=\"latest\"</code>, no polling is needed since it always fetches the most recent available period.</p> <p>For manual polling, use <code>client.wait_for_inference_data(\"inference_data.parquet\")</code> or gently poll the API until <code>GET /current</code> stops returning <code>404</code>. </p> <p>To download inference data: <pre><code># Download the current active inference period (for today's submissions)\nclient.download_inference_data(\"current\") # THIS WILL FAIL IF THERE IS NOT AN OPEN INFERENCE PERIOD\n\n# Download the most recently available inference period\nclient.download_inference_data(\"latest\")\n\n# Download a specific date's inference data\nclient.download_inference_data(\"2024-12-15\")\n</code></pre></p> id eodhd_id feature_1_lag15 feature_1_lag10 feature_1_lag5 feature_1_lag0 ... BABY BABY32198-USD.CC 0.123 0.145 0.112 0.156 ... OM OM-USD.CC 0.456 0.423 0.467 0.401 ... IOTA IOTA-USD.CC 0.789 0.812 0.798 0.823 ... MOODENG MOODENG33093-USD.CC 0.234 0.267 0.223 0.289 ... ENS ENS-USD.CC 0.567 0.534 0.578 0.512 ... <p>Tip</p> <p>You do not need to use the features included in the inference data. You can use and are encouraged to use your own features. It may still be helpful to use our inference data and id mappings to use the same universe of assets.</p>"},{"location":"hyperliquid-ranking/#submitting-predictions","title":"Submitting predictions","text":"<p>Minimum of 80 ids from the inference data are required for a valid submission. The following columns are also required (no index):</p> <ul> <li><code>id</code>: The id of the asset on Hyperliquid.</li> <li><code>pred_10d</code>: A float between 0 and 1 representing the predicted rank for the 10-day horizon.</li> <li><code>pred_30d</code>: A float between 0 and 1 representing the predicted rank for the 30-day horizon.</li> </ul> <p>To submit predictions, you have 5 submission slots available. These can be used to submit multiple predictions for the same day and are defined by the <code>slot</code> parameter: <pre><code>client.submit_predictions(df=predictions_df, slot=1) # Submit a dataframe\nclient.submit_predictions(file_path=\"submission.parquet\", slot=2) # or a parquet file\n</code></pre></p> id pred_10d pred_30d BABY 0.2 0.3 OM 0.4 0.5 IOTA 0.1 0.2 MOODENG 1.0 1.0 ENS 0.3 0.4 <p>Note</p> <p>If you are submitting a dataframe, <code>id</code> must be a column in the dataframe, not the index. If you are submitting a file, all submissions must be in parquet format.</p>"},{"location":"hyperliquid-ranking/#scoring-and-evaluation","title":"Scoring and Evaluation","text":"<p>Before scoring, for each prediction timeframe, ids are uniform ranked [0, 1], and any missing ids are filled with 0.5.</p>"},{"location":"hyperliquid-ranking/#metrics","title":"Metrics","text":"<p>1) Symmetric Normalized Discounted Cumulative Gain (NDCG@40)</p> <p>When you see NDCG@40, think: \"how well did I rank the top 40 assets and how well did I rank the bottom 40 assets?\" With ~170 tokens in the universe, k=40 represents approximately the top/bottom 20-25% of assets. This metric equally rewards both:</p> <ul> <li>Top 40 identification: Finding the tokens that will have the highest returns (for long positions)</li> <li>Bottom 40 identification: Finding the tokens that will have the lowest returns (for short positions or avoidance)</li> </ul> <p>The logarithmic discount means getting the #1 ranked token correct is much more valuable than getting the #40 ranked token correct. A perfect NDCG@40 score of 1.0 means you perfectly ranked both tails of the distribution. This metric is particularly valuable for portfolio construction where you want to maximize exposure to the best performers while avoiding or shorting the worst.</p> <p>Random Baseline</p> <p>Random predictions score approximately 0.55 for NDCG@40 with ~170 tokens, not 0.5. See the detailed explanation for why this happens.</p> <p>2) Spearman Correlation</p> <p>Spearman's rank correlation (\u03c1) measures how well your predicted ranks align with the true ranks across the entire universe of ~170 tokens. Unlike NDCG@40 which focuses on the 40 extremes, \u03c1 treats all rank positions in the entire universe equally.</p>"},{"location":"hyperliquid-ranking/#composite-percentile","title":"Composite Percentile","text":"<p>During the initial warm-up phase of the challenge, the goal is to maximize all metrics across all timeframes equally. Since NDCG@40 (0-1 range) and Spearman correlation (-1 to 1 range) have different scales and distributions, we use a composite percentile for fair comparison.</p> <p>The composite percentile is calculated as the average of your percentile rankings across all four metrics:</p> \\[ \\text{Composite Percentile} = \\frac{1}{4} \\times \\left( \\begin{array}{l} \\text{percentile(NDCG@40}_{10d}) + \\\\ \\text{percentile(NDCG@40}_{30d}) + \\\\ \\text{percentile(spearman}_{10d}) + \\\\ \\text{percentile(spearman}_{30d}) \\end{array} \\right) \\] <p>Where each percentile represents your ranking (0-100) compared to other participants for that specific metric for a given day/inference period.</p> <p>Important: Composite percentiles are only calculated when ten (10) or more valid submissions (counted by submission slots, not users) are received for a given day. If fewer than ten submissions are present, the composite percentile will not be calculated, and you'll need to look at absolute metric scores instead.</p> <p>As we learn more about the challenge's metamodel, we may adjust the weighting or add/remove metrics.</p>"},{"location":"hyperliquid-ranking/#score-ranges-and-percentile-rankings","title":"Score Ranges and Percentile Rankings","text":"<p>Raw Score Ranges:</p> <ul> <li>NDCG@40: Ranges from 0.0 (worst possible) to 1.0 (perfect ranking of top and bottom 40 assets)</li> <li>\u03c1 (Spearman's Rank Correlation): Ranges from -1.0 (perfect inverse ranking) to 1.0 (perfect ranking), with 0.0 indicating random performance</li> </ul> <p>Why Are Scores Typically Low?</p> <p>Financial markets are characterized by extremely high noise-to-signal ratios. Seemingly \"low\" scores can be quite competitive in this domain. Additionally, market regimes shift over time, causing the distribution of achievable scores to fluctuate significantly.</p> <p>Percentile Rankings: Your Most Reliable Metric</p> <p>CrowdCent calculates percentile rankings that show where you stand relative to other participants. These percentiles are recalculated daily.</p> <p>Tracking your percentile scores over time is often more informative than focusing on absolute scores, as it accounts for evolving competition and regime shifts that affect all participants. A model that consistently ranks in the 75th percentile across different market conditions can often be more valuable than one that occasionally achieves top scores but performs poorly in other regimes.</p> <p>Minimum submissions for percentile</p> <p>Percentiles only calculated when ten (10) or more valid submissions are received for a given day. If fewer than ten submissions are present, you'll see individual metric scores, but no percentile.</p>"},{"location":"hyperliquid-ranking/#meta-model","title":"Meta-Model","text":"<p>The CrowdCent Meta-Model aggregates predictions from all participants, representing a \"wisdom of the crowd\" which is made available to all users with a valid CrowdCent account. This may change in the future with no notice.</p> <p>Meta-Model Disclaimer</p> <p>Meta-model signals are released for informational and educational purposes only. Not financial, investment, or trading advice. CrowdCent disclaims all liability for any losses, damages, or consequences arising from use of the meta-model. Users assume all risks.</p>"},{"location":"hyperliquid-ranking/#construction-methodology","title":"Construction Methodology","text":"<p>The meta-model is currently constructed daily using a simple, naive average of all submission slots:</p> <ol> <li>Uniform Ranking: Each individual submission's predictions are first converted to uniform rankings [0, 1] for each prediction column (<code>pred_10d</code>, <code>pred_30d</code>)</li> <li>Missing ID Handling: Any asset IDs missing from individual submissions are filled with neutral rankings of 0.5 after the uniform ranking step</li> <li>Average slots: Create a single prediction for each user by taking the arithmetic mean of all normalized rankings across all submission slots for each user.</li> <li>Average users: The final meta-model is created by taking the arithmetic mean of all normalized ranking across all users.</li> </ol> <p>Future Enhancements</p> <p>The current simple averaging approach is naive but effective. Future versions may incorporate more sophisticated weighting schemes based on historical performance, tail weighting, or other ensemble methods.</p>"},{"location":"hyperliquid-ranking/#access-and-downloads","title":"Access and Downloads","text":"<p>The meta-model is available through multiple channels:</p> <ul> <li>Via web: https://crowdcent.com/challenge/hyperliquid-ranking/meta-model/</li> <li>Via API: <code>client.download_meta_model(dest_path=\"meta_model.parquet\")</code></li> </ul> <p>The meta-model is a parquet file with the following columns. New predictions are added daily, creating a time series with multiple release dates as shown in this sample:</p> id pred_10d pred_30d release_date BTC 0.85 0.82 2024-12-15 ETH 0.74 0.78 2024-12-15 SOL 0.91 0.89 2024-12-15 BTC 0.85 0.82 2024-12-16 ETH 0.74 0.78 2024-12-16 SOL 0.91 0.83 2024-12-16 ... ... ... ..."},{"location":"install-quickstart/","title":"Install & Quick Start","text":""},{"location":"install-quickstart/#install-the-client","title":"Install the client","text":"Using uv (Recommended)Using pip <pre><code>uv add crowdcent-challenge\n</code></pre> <pre><code>pip install crowdcent-challenge\n</code></pre>"},{"location":"install-quickstart/#get-an-api-key","title":"Get an API Key","text":"<p>You need an API key to use the CrowdCent Challenge API. You can get your key by clicking \"Generate New Key\" on your profile page. Write it down, as you won't be able to access it after you leave the page.</p> <p></p>"},{"location":"install-quickstart/#authenticate-and-initialize-the-client","title":"Authenticate and Initialize the Client","text":"<p>The API client requires authentication using your API key. This can be provided directly or via environment variables. You can interact with the API using the Python client or the CLI.</p> PythonCLI <pre><code>from crowdcent_challenge import ChallengeClient, CrowdCentAPIError\n\nchallenge_slug = \"hyperliquid-ranking\"  # Replace with your challenge\napi_key = \"your_api_key_here\" # Replace with your actual key\nclient = ChallengeClient(challenge_slug=challenge_slug, api_key=api_key)\n</code></pre> <pre><code>export CROWDCENT_API_KEY=your_api_key_here # Set the environment variable\necho \"CROWDCENT_API_KEY=your_api_key_here\" &gt; .env # or create .env\n\n# Set the default challenge\ncrowdcent set-default-challenge hyperliquid-ranking\n\n# Check current default challenge\ncrowdcent get-default-challenge\n</code></pre> <p>Note</p> <p>With a default challenge set, you can run most commands without explicitly specifying the challenge. If you need to override the default for a specific command, use the <code>--challenge</code> or <code>-c</code> option.</p>"},{"location":"install-quickstart/#training-data","title":"Training Data","text":"<p>Access training datasets for a challenge, including listing available versions, getting the latest version, and downloading datasets.</p> PythonCLI <pre><code># List all training datasets for the current challenge\nclient.list_training_datasets()\n\n# Get details about the latest training dataset\nclient.get_training_dataset(\"latest\")\n\n# Download the training dataset file\nversion = \"latest\" # or specify a version like `1.0`\noutput_path = \"data/training_data.parquet\"\nclient.download_training_dataset(version, output_path)\n</code></pre> <pre><code># List all training datasets\ncrowdcent list-training-data\n\n# Get details about a specific training dataset version\ncrowdcent get-training-data 1.0\n\n# Download latest version\ncrowdcent download-training-data latest -o ./data/training_data.parquet\n\n# Or a specific version\ncrowdcent download-training-data 1.0 -o ./data/training_data.parquet\n</code></pre>"},{"location":"install-quickstart/#inference-data","title":"Inference Data","text":"<p>Manage inference periods and download inference features.</p> PythonCLI <pre><code># Get today's inference data (will wait/poll until published)\noutput_path = \"data/inference_features.parquet\"\nclient.download_inference_data(\"current\", output_path)  # polls every 30s by default\n\n# Get the most recent available data (no waiting)\nclient.download_inference_data(\"latest\", output_path)\n\n# Get data for a specific date\nclient.download_inference_data(\"2025-01-15\", output_path)\n</code></pre> <pre><code># List all inference data periods\n# Download today's inference data (will poll until available)\ncrowdcent download-inference-data current -o ./data/inference_features.parquet\n\n# Download most recent available data (no waiting)\ncrowdcent download-inference-data latest -o ./data/inference_features.parquet\n\n# Download specific date\ncrowdcent download-inference-data 2025-01-15 -o ./data/inference_features.parquet\n</code></pre> <p>Choosing the Right Option</p> <ul> <li>Use <code>\"current\"</code> in your daily prediction workflow when you need today's features</li> <li>Use <code>\"latest\"</code> for when you need immediate access to the most recent inference period even if it's closed</li> <li>Use <code>YYYY-MM-DD</code> when working with historical periods or debugging</li> </ul>"},{"location":"install-quickstart/#meta-model","title":"Meta-Model","text":"<p>Download the consolidated meta-model for a challenge. The meta-model typically represents an aggregation of all valid user submissions for past inference periods.</p> PythonCLI <pre><code>output_path = \"data/meta_model.parquet\"\nclient.download_meta_model(output_path)\n</code></pre> <pre><code>crowdcent download-meta-model -o ./data/meta_model.parquet\n</code></pre>"},{"location":"install-quickstart/#submitting-predictions","title":"Submitting Predictions","text":"<p>Submit your model's predictions for the current inference period. The file must include an <code>id</code> column and the specific prediction columns required by the challenge (e.g., <code>pred_10d</code>, <code>pred_30d</code> for some challenges, or <code>pred_1M</code>, <code>pred_3M</code>, etc., for others). Always check the specific challenge documentation for the exact column names.</p> PythonCLI <pre><code>import polars as pl\nimport numpy as np\nfrom joblib import load\n\n# Create or load your predictions\ninference_data = pl.read_parquet(\"inference_data.parquet\")\nmodel = load(\"model.joblib\")\npredictions = model.predict(inference_data)\npred_df = pl.from_numpy(predictions, [\"pred_10d\", \"pred_30d\"])\ninference_data = inference_data.with_columns(pred_df)\n\n# Save predictions to a Parquet file\nsubmission_file = \"submission.parquet\"\npredictions.write_parquet(submission_file)\n\n# You can specify a submission slot (1-5), default is 1\nclient.submit_predictions(file_path=submission_file, slot=2)\n\n# Or submit a DataFrame directly (without saving to file first)\nclient.submit_predictions(df=pred_df)\n</code></pre> <pre><code># Submit predictions to the default challenge (uses slot 1)\ncrowdcent submit submission.parquet\n\n# Submit to a specific slot (1-5)\ncrowdcent submit submission.parquet --slot 2\n\n# Submit to a specific challenge (overriding default)\ncrowdcent submit submission.parquet --challenge hyperliquid-ranking --slot 3\n</code></pre>"},{"location":"install-quickstart/#retrieving-submissions","title":"Retrieving Submissions","text":"<p>Manage and review your submissions for a challenge, including listing all submissions, filtering by period, and getting details for a specific submission.</p> PythonCLI <pre><code># List your submissions for the current challenge\nclient.list_submissions()\n\n# Filter submissions by period\n# Get submissions for the current period only\nclient.list_submissions(period=\"current\")\n\n# Or for a specific period\nclient.list_submissions(period=\"2025-01-15\")\n\n# Get details for a specific submission\nsubmission_id = 123  # Replace with actual submission ID\nsubmission = client.get_submission(submission_id)\nif submission['score_details']:\n    print(f\"Score Details: {submission['score_details']}\")\n</code></pre> <pre><code># List all submissions\ncrowdcent list-submissions\n\n# Filter by period\ncrowdcent list-submissions --period current\ncrowdcent list-submissions --period 2025-01-15\n\n# Get details about a specific submission\ncrowdcent get-submission 123\n</code></pre>"},{"location":"install-quickstart/#challenges","title":"Challenges","text":"<p>Get details for a challenge or switch between different challenges.</p> PythonCLI <pre><code>challenges = ChallengeClient.list_all_challenges()\n\n# Get details for the current challenge\nchallenge = client.get_challenge()\n\n# Switch to a different challenge\nnew_challenge_slug = \"another-challenge\"  # Replace with another actual challenge slug\nclient.switch_challenge(new_challenge_slug) # Now all operations will be for the new challenge\n</code></pre> <pre><code># List all available challenges\ncrowdcent list-challenges\n\n# Get details for the default challenge\ncrowdcent get-challenge\n\n# Or specify a challenge explicitly\ncrowdcent get-challenge --challenge hyperliquid-ranking\n\n# Switch to a different default challenge\ncrowdcent set-default-challenge another-challenge\n</code></pre> <p>If you need to work with multiple challenges simultaneously, we recommend using multiple client instances.</p> PythonCLI <pre><code># Initialize clients for different challenges\nclient_a = ChallengeClient(challenge_slug=\"challenge-a\")\nclient_b = ChallengeClient(challenge_slug=\"challenge-b\")\n\n# Use each client for its respective challenge\ndataset_a = client_a.get_training_dataset(\"latest\")\ndataset_b = client_b.get_training_dataset(\"latest\")\n</code></pre> <pre><code># Set default challenge training data\ncrowdcent get-training-data\n\n# Switch to a different challenge\ncrowdcent get-training-data --challenge challenge-a\ncrowdcent get-training-data --challenge challenge-b\n</code></pre> <p>CLI vs Python Approach</p> <p>The CLI doesn't have a direct equivalent to <code>client.switch_challenge()</code> because each CLI command is independent. Instead, use <code>set-default-challenge</code> to change your default, or use <code>--challenge</code> to override the default for specific commands. This approach is often more convenient for CLI usage.</p>"},{"location":"scoring/","title":"Scoring","text":"<p>All scoring functions used in the CrowdCent Challenge can be found in the <code>crowdcent_challenge.scoring</code> module.</p> <pre><code>from crowdcent_challenge.scoring import *\n</code></pre>"},{"location":"scoring/#symmetric-ndcgk","title":"Symmetric NDCG@k","text":"<p>One of the key metrics used in some challenges is Symmetric Normalized Discounted Cumulative Gain (Symmetric NDCG@k).</p>"},{"location":"scoring/#concept","title":"Concept","text":"<p>Normalized Discounted Cumulative Gain (NDCG@k) is a metric used to evaluate how well a model ranks items. It assesses the quality of the top k predictions by:</p> <ol> <li>Giving higher scores for ranking truly relevant items higher.</li> <li>Applying a logarithmic discount to items ranked lower (meaning relevance at rank 1 is more important than relevance at rank 10).</li> <li>Normalizing the score by the best possible ranking (IDCG) to get a value between 0 and 1.</li> </ol> <p>However, this commonly used metric only focuses on the top items in a list. In finance, identifying the worst performers (lowest true values) can be just as important as identifying the best.</p> <p>Our metric of Symmetric NDCG@k addresses this by evaluating ranking performance at both ends of the list:</p> <ol> <li>Top Performance: It calculates the standard <code>NDCG@k</code> based on your predicted scores (<code>y_pred</code>) compared to the actual true values (<code>y_true</code>). This measures how well you identify the items with the highest true values.</li> <li>Bottom Performance: It calculates another <code>NDCG@k</code> focused on the lowest ranks. It does this by:<ul> <li>Inverting both true values and predictions using <code>1 - value</code> transformation</li> <li>This makes originally low values (close to 0) become high values (close to 1), so standard NDCG rewards finding the originally lowest items</li> <li>Calculating <code>NDCG@k</code> for how well your lowest predictions match the items with the lowest true values</li> </ul> </li> <li>Averaging: The final <code>symmetric_ndcg_at_k</code> score is the simple average of the Top NDCG@k and the Bottom NDCG@k. <code>(NDCG_top + NDCG_bottom) / 2</code>.</li> </ol>"},{"location":"scoring/#calculation","title":"Calculation","text":"<p>The Symmetric NDCG@k is calculated as:</p> <ol> <li>Top NDCG@k: Calculate standard NDCG@k using true values and predicted scores</li> <li>Bottom NDCG@k: Invert both true values and predictions using <code>1 - value</code>, then calculate NDCG@k</li> <li>Final Score: Average of top and bottom NDCG@k scores: <code>(NDCG_top + NDCG_bottom) / 2</code></li> </ol> <p>The standard NDCG formula includes:</p> <ul> <li>DCG@k = \u03a3(relevance_i / log\u2082(i+1)) for i=1 to k</li> <li>IDCG@k = DCG@k for ideal ranking</li> <li>NDCG@k = DCG@k / IDCG@k</li> </ul>"},{"location":"scoring/#interpretation","title":"Interpretation","text":"<p>Notably, Symmetric NDCG@k does not give 0.5 for random predictions, but ~0.55 for our default k=40. Understanding the random baseline is crucial for interpreting your scores.</p> <ul> <li>NDCG@k = 1: perfect performance at identifying both the top k best and bottom k worst items according to their true values.</li> <li>NDCG@k = 0.55: random guessing.</li> <li>NDCG@k = 0: no overlap with top k or bottom k.</li> </ul> <p>How k Affects the Random Baseline:</p> <p>Key insights:</p> <ul> <li>Random baseline starts near 0.5 for small k and increases monotonically</li> <li>For Hyperliquid (k=40, n\u2248170-200), random predictions score ~0.55</li> </ul>"},{"location":"scoring/#usage","title":"Usage","text":"<pre><code>from crowdcent_challenge.scoring import symmetric_ndcg_at_k\nimport numpy as np\n\n# Example data\ny_true = np.array([0.1, 0.2, 0.9, 0.3, 0.7])\ny_pred = np.array([0.2, 0.1, 0.8, 0.4, 0.6])\nk = 3\n\nscore = symmetric_ndcg_at_k(y_true, y_pred, k)\n</code></pre> <p>This metric provides a more holistic view of ranking performance when both high and low extremes are important.</p>"},{"location":"scoring/#spearman-correlation","title":"Spearman Correlation","text":"<p>Spearman's rank correlation coefficient is a non-parametric measure of rank correlation that assesses how well the relationship between two variables can be described using a monotonic function.</p>"},{"location":"scoring/#concept_1","title":"Concept","text":"<p>In the context of ranking challenges, Spearman correlation measures how well your predicted rankings align with the true rankings. Unlike Pearson correlation (which measures linear relationships), Spearman correlation:</p> <ol> <li>Works with ranks: It converts both predicted and true values to ranks before computing correlation</li> <li>Captures monotonic relationships: Perfect Spearman correlation (\u00b11) means perfect monotonic relationship, even if not linear</li> <li>Robust to outliers: Since it uses ranks rather than raw values, extreme values have less influence</li> </ol>"},{"location":"scoring/#calculation_1","title":"Calculation","text":"<p>The Spearman correlation coefficient (\u03c1) is calculated as:</p> <ul> <li>First, convert both <code>y_true</code> and <code>y_pred</code> to ranks</li> <li>Then calculate the Pearson correlation coefficient on these ranks</li> <li>Formula: \u03c1 = 1 - (6 \u00d7 \u03a3d\u00b2) / (n \u00d7 (n\u00b2 - 1)), where d is the difference between paired ranks</li> </ul>"},{"location":"scoring/#interpretation_1","title":"Interpretation","text":"<ul> <li>\u03c1 = 1: Perfect positive correlation (your rankings perfectly match the true rankings)</li> <li>\u03c1 = 0: No correlation (your rankings are unrelated to the true rankings)  </li> <li>\u03c1 = -1: Perfect negative correlation (your rankings are exactly reversed)</li> </ul>"},{"location":"scoring/#usage_1","title":"Usage","text":"<pre><code>from scipy.stats import spearmanr\nimport numpy as np\n\n# Example data\ny_true = np.array([1.0, 0.5, 0.3, 0.2, 0.1])  # True values (will be ranked)\ny_pred = np.array([0.9, 0.6, 0.25, 0.22, 0.05])    # Predicted values (will be ranked)\n\n# Calculate Spearman correlation\ncorrelation, p_value = spearmanr(y_true, y_pred)\n</code></pre>"},{"location":"api-reference/cli/","title":"CLI Reference","text":"<p>This page provides auto-generated documentation for the <code>crowdcent</code> CLI for the CrowdCent Challenge.</p>"},{"location":"api-reference/cli/#crowdcent","title":"crowdcent","text":"<p>Command Line Interface for the CrowdCent Challenge.</p> <p>Usage:</p> <pre><code>crowdcent [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#download-inference-data","title":"download-inference-data","text":"<p>Usage:</p> <pre><code>crowdcent download-inference-data [OPTIONS] [RELEASE_DATE]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT     Challenge slug (uses default if not specified)\n  -o, --output TEXT        Output file path. Defaults to\n                           [challenge_slug]_inference_[release_date].parquet\n                           in current directory.\n  --no-poll                Disable polling when waiting for the current\n                           inference data to be published.\n  --poll-interval INTEGER  Seconds to wait between polling attempts when\n                           release_date='current'.  [default: 30]\n  --timeout INTEGER        Maximum seconds to wait when polling for current\n                           data (0 = wait indefinitely).  [default: 900]\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#download-meta-model","title":"download-meta-model","text":"<p>Usage:</p> <pre><code>crowdcent download-meta-model [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  -o, --output TEXT     Output file path. Defaults to\n                        [challenge_slug]_meta_model.parquet in current\n                        directory.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#download-training-data","title":"download-training-data","text":"<p>Usage:</p> <pre><code>crowdcent download-training-data [OPTIONS] [VERSION]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  -o, --output TEXT     Output file path. Defaults to\n                        [challenge_slug]_training_v[version].parquet in\n                        current directory.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-challenge","title":"get-challenge","text":"<p>Usage:</p> <pre><code>crowdcent get-challenge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-default-challenge","title":"get-default-challenge","text":"<p>Show the current default challenge slug.</p> <p>Usage:</p> <pre><code>crowdcent get-default-challenge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-inference-data","title":"get-inference-data","text":"<p>Usage:</p> <pre><code>crowdcent get-inference-data [OPTIONS] RELEASE_DATE\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-submission","title":"get-submission","text":"<p>Usage:</p> <pre><code>crowdcent get-submission [OPTIONS] SUBMISSION_ID\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-training-data","title":"get-training-data","text":"<p>Usage:</p> <pre><code>crowdcent get-training-data [OPTIONS] [VERSION]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-challenges","title":"list-challenges","text":"<p>Usage:</p> <pre><code>crowdcent list-challenges [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-inference-data","title":"list-inference-data","text":"<p>Usage:</p> <pre><code>crowdcent list-inference-data [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-submissions","title":"list-submissions","text":"<p>Usage:</p> <pre><code>crowdcent list-submissions [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --period TEXT         Filter submissions by period: 'current' or a date in\n                        'YYYY-MM-DD' format\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-training-data","title":"list-training-data","text":"<p>Usage:</p> <pre><code>crowdcent list-training-data [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#set-default-challenge","title":"set-default-challenge","text":"<p>Usage:</p> <pre><code>crowdcent set-default-challenge [OPTIONS] CHALLENGE_SLUG\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#submit","title":"submit","text":"<p>Usage:</p> <pre><code>crowdcent submit [OPTIONS] FILE_PATH\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --slot INTEGER        Submission slot number (1-based). Defaults to 1.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/python/","title":"Python API Client","text":"<p>This page provides auto-generated documentation from the client library's docstrings.</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient","title":"<code>crowdcent_challenge.client.ChallengeClient</code>","text":"<p>Client for interacting with a specific CrowdCent Challenge.</p> <p>Handles authentication and provides methods for accessing challenge data, training datasets, inference data, and managing prediction submissions for a specific challenge identified by its slug.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>class ChallengeClient:\n    \"\"\"\n    Client for interacting with a specific CrowdCent Challenge.\n\n    Handles authentication and provides methods for accessing challenge data,\n    training datasets, inference data, and managing prediction submissions for\n    a specific challenge identified by its slug.\n    \"\"\"\n\n    DEFAULT_BASE_URL = \"https://crowdcent.com/api\"\n    API_KEY_ENV_VAR = \"CROWDCENT_API_KEY\"\n\n    def __init__(\n        self,\n        challenge_slug: str,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the ChallengeClient for a specific challenge.\n\n        Args:\n            challenge_slug: The unique identifier (slug) for the challenge.\n            api_key: Your CrowdCent API key. If not provided, it will attempt\n                     to load from the CROWDCENT_API_KEY environment variable\n                     or a .env file.\n            base_url: The base URL of the CrowdCent API. Defaults to\n                      https://crowdcent.com/api.\n        \"\"\"\n        load_dotenv()  # Load .env file if present\n        self.api_key = api_key or os.getenv(self.API_KEY_ENV_VAR)\n        if not self.api_key:\n            raise AuthenticationError(\n                f\"API key not provided and not found in environment variable \"\n                f\"'{self.API_KEY_ENV_VAR}' or .env file.\"\n            )\n\n        self.challenge_slug = challenge_slug\n        self.base_url = (base_url or self.DEFAULT_BASE_URL).rstrip(\"/\")\n        self.session = requests.Session()\n        self.session.headers.update({\"Authorization\": f\"Api-Key {self.api_key}\"})\n        logger.info(\n            f\"ChallengeClient initialized for '{challenge_slug}' at URL: {self.base_url}\"\n        )\n\n    def _request(\n        self,\n        method: str,\n        endpoint: str,\n        params: Optional[Dict] = None,\n        json_data: Optional[Dict] = None,\n        files: Optional[Dict[str, IO]] = None,\n        stream: bool = False,\n        data: Optional[Dict] = None,\n        max_retries: int = 3,\n        retry_delay: float = 1.0,\n    ) -&gt; requests.Response:\n        \"\"\"\n        Internal helper method to make authenticated API requests.\n\n        Args:\n            method: HTTP method (e.g., 'GET', 'POST').\n            endpoint: API endpoint path (e.g., '/challenges/').\n            params: URL parameters.\n            json_data: JSON data for the request body.\n            files: Files to upload (for multipart/form-data).\n            stream: Whether to stream the response (for downloads).\n            data: Dictionary of form data to send with multipart requests.\n            max_retries: Maximum number of retry attempts for connection errors.\n            retry_delay: Initial delay between retries (seconds). Will use exponential backoff.\n\n        Returns:\n            The requests.Response object.\n\n        Raises:\n            AuthenticationError: If the API key is invalid (401).\n            NotFoundError: If the resource is not found (404).\n            ClientError: For other 4xx errors.\n            ServerError: For 5xx errors.\n            CrowdCentAPIError: For other request exceptions.\n        \"\"\"\n        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n        logger.debug(\n            f\"Request: {method} {url} Params: {params} JSON: {json_data is not None} \"\n            f\"Data: {data is not None} Files: {files is not None}\"\n        )\n\n        for attempt in range(max_retries + 1):\n            try:\n                response = self.session.request(\n                    method,\n                    url,\n                    params=params,\n                    json=json_data,\n                    files=files,\n                    stream=stream,\n                    data=data,\n                )\n                response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n                logger.debug(f\"Response: {response.status_code}\")\n                return response\n            except requests_exceptions.HTTPError as e:\n                status_code = e.response.status_code\n\n                # Try to parse standardized error format: {\"error\": {\"code\": \"ERROR_CODE\", \"message\": \"Description\"}}\n                try:\n                    error_data = e.response.json()\n                    if \"error\" in error_data and isinstance(error_data[\"error\"], dict):\n                        error_code = error_data[\"error\"].get(\"code\", \"UNKNOWN_ERROR\")\n                        error_message = error_data[\"error\"].get(\n                            \"message\", e.response.text\n                        )\n                    else:\n                        error_code = \"API_ERROR\"\n                        error_message = e.response.text\n                except requests_exceptions.JSONDecodeError:\n                    error_code = \"API_ERROR\"\n                    error_message = e.response.text\n\n                logger.error(\n                    f\"API Error ({status_code}): {error_code} - {error_message} for {method} {url}\"\n                )\n\n                if status_code == 401:\n                    raise AuthenticationError(\n                        f\"Authentication failed (401): {error_message} [{error_code}]\"\n                    ) from e\n                elif status_code == 404:\n                    raise NotFoundError(\n                        f\"Resource not found (404): {error_message} [{error_code}]\"\n                    ) from e\n                elif 400 &lt;= status_code &lt; 500:\n                    raise ClientError(\n                        f\"Client error ({status_code}): {error_message} [{error_code}]\"\n                    ) from e\n                elif 500 &lt;= status_code &lt; 600:\n                    raise ServerError(\n                        f\"Server error ({status_code}): {error_message} [{error_code}]\"\n                    ) from e\n                else:\n                    raise CrowdCentAPIError(\n                        f\"HTTP error ({status_code}): {error_message} [{error_code}]\"\n                    ) from e\n            except (\n                requests_exceptions.ConnectionError,\n                requests_exceptions.Timeout,\n            ) as e:\n                # Connection errors and timeouts are retryable\n                if attempt &lt; max_retries:\n                    delay = retry_delay * (2**attempt)  # Exponential backoff\n                    logger.warning(\n                        f\"Connection error: {e}. Retrying in {delay:.1f}s... \"\n                        f\"(attempt {attempt + 1}/{max_retries})\"\n                    )\n                    time.sleep(delay)\n                    continue\n                logger.error(\n                    f\"Request failed after {max_retries} retries: {e} for {method} {url}\"\n                )\n                raise CrowdCentAPIError(\n                    f\"Request failed after {max_retries} retries: {e}\"\n                ) from e\n            except requests_exceptions.RequestException as e:\n                logger.error(f\"Request failed: {e} for {method} {url}\")\n                raise CrowdCentAPIError(f\"Request failed: {e}\") from e\n\n    # --- Class Method for Listing All Challenges ---\n\n    @classmethod\n    def list_all_challenges(\n        cls, api_key: Optional[str] = None, base_url: Optional[str] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists all active challenges.\n\n        This is a class method that doesn't require a challenge_slug.\n        Use this to discover available challenges before initializing a ChallengeClient.\n\n        Args:\n            api_key: Your CrowdCent API key. If not provided, it will attempt\n                     to load from the CROWDCENT_API_KEY environment variable\n                     or a .env file.\n            base_url: The base URL of the CrowdCent API. Defaults to\n                      http://crowdcent.com/api.\n\n        Returns:\n            A list of dictionaries, each representing an active challenge.\n        \"\"\"\n        # Create a temporary session for this request\n        load_dotenv()\n        api_key = api_key or os.getenv(cls.API_KEY_ENV_VAR)\n        if not api_key:\n            raise AuthenticationError(\n                f\"API key not provided and not found in environment variable \"\n                f\"'{cls.API_KEY_ENV_VAR}' or .env file.\"\n            )\n\n        base_url = (base_url or cls.DEFAULT_BASE_URL).rstrip(\"/\")\n        session = requests.Session()\n        session.headers.update({\"Authorization\": f\"Api-Key {api_key}\"})\n\n        url = f\"{base_url}/challenges/\"\n        try:\n            response = session.get(url)\n            response.raise_for_status()\n            return response.json()\n        except requests_exceptions.HTTPError as e:\n            status_code = e.response.status_code\n            if status_code == 401:\n                raise AuthenticationError(\"Authentication failed (401)\")\n            elif status_code == 404:\n                raise NotFoundError(\"Resource not found (404)\")\n            elif 400 &lt;= status_code &lt; 500:\n                raise ClientError(f\"Client error ({status_code})\")\n            elif 500 &lt;= status_code &lt; 600:\n                raise ServerError(f\"Server error ({status_code})\")\n            else:\n                raise CrowdCentAPIError(f\"HTTP error ({status_code})\")\n        except requests_exceptions.RequestException as e:\n            raise CrowdCentAPIError(f\"Request failed: {e}\")\n\n    # --- Challenge Methods ---\n\n    def get_challenge(self) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for this challenge.\n\n        Returns:\n            A dictionary representing this challenge.\n\n        Raises:\n            NotFoundError: If the challenge with the given slug is not found.\n        \"\"\"\n        response = self._request(\"GET\", f\"/challenges/{self.challenge_slug}/\")\n        return response.json()\n\n    # --- Training Data Methods ---\n\n    def list_training_datasets(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists all training dataset versions for this challenge.\n\n        Returns:\n            A list of dictionaries, each representing a training dataset version.\n\n        Raises:\n            NotFoundError: If the challenge is not found.\n        \"\"\"\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/training_data/\"\n        )\n        return response.json()\n\n    def get_training_dataset(self, version: str) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for a specific training dataset version.\n\n        Args:\n            version: The version string of the training dataset (e.g., '1.0', '2.1')\n                     or the special value ``\"latest\"`` to get the latest version.\n\n        Returns:\n            A dictionary representing the specified training dataset.\n\n        Raises:\n            NotFoundError: If the challenge or the specified training dataset is not found.\n        \"\"\"\n        if version == \"latest\":\n            response = self._request(\n                \"GET\", f\"/challenges/{self.challenge_slug}/training_data/latest/\"\n            )\n            return response.json()\n\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/training_data/{version}/\"\n        )\n        return response.json()\n\n    def download_training_dataset(self, version: str, dest_path: str):\n        \"\"\"Downloads the training data file for a specific dataset version.\n\n        Args:\n            version: The version string of the training dataset (e.g., '1.0', '2.1')\n                    or 'latest' to get the latest version.\n            dest_path: The local file path to save the downloaded dataset.\n\n        Raises:\n            NotFoundError: If the challenge, dataset, or its file is not found.\n        \"\"\"\n        if version == \"latest\":\n            latest_info = self.get_training_dataset(\"latest\")\n            version = latest_info[\"version\"]\n\n        endpoint = (\n            f\"/challenges/{self.challenge_slug}/training_data/{version}/download/\"\n        )\n\n        logger.info(\n            f\"Downloading training data for challenge '{self.challenge_slug}' v{version} to {dest_path}\"\n        )\n        response = self._request(\"GET\", endpoint, stream=True)\n\n        # Get total file size from headers\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        try:\n            from tqdm import tqdm\n\n            with open(dest_path, \"wb\") as f:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(dest_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        f.write(chunk)\n                        pbar.update(len(chunk))\n            logger.info(f\"Successfully downloaded training data to {dest_path}\")\n        except IOError as e:\n            logger.error(f\"Failed to write dataset to {dest_path}: {e}\")\n            raise CrowdCentAPIError(f\"Failed to write dataset file: {e}\") from e\n\n    # --- Inference Data Methods ---\n\n    def list_inference_data(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists all inference data periods for this challenge.\n\n        Returns:\n            A list of dictionaries, each representing an inference data period.\n\n        Raises:\n            NotFoundError: If the challenge is not found.\n        \"\"\"\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/\"\n        )\n        return response.json()\n\n    def get_inference_data(self, release_date: str) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for a specific inference data period by its release date.\n\n        Args:\n            release_date: The release date of the inference data in 'YYYY-MM-DD' format.\n                          You can also pass the special values:\n                          - ``\"current\"`` to fetch the current active inference period\n                          - ``\"latest\"`` to fetch the most recently *available* inference period\n\n        Returns:\n            A dictionary representing the specified inference data period.\n\n        Raises:\n            NotFoundError: If the challenge or the specified inference data is not found.\n            ClientError: If the date format is invalid.\n        \"\"\"\n        if release_date == \"current\":\n            response = self._request(\n                \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/current/\"\n            )\n            return response.json()\n\n        if release_date == \"latest\":\n            # Simply resolve via list_inference_data(); avoid noisy probe.\n            periods = self.list_inference_data()\n            if not periods:\n                raise NotFoundError(\n                    \"No inference data periods found for this challenge.\"\n                )\n\n            latest_period = max(periods, key=lambda p: p[\"release_date\"])\n            release_date_iso = latest_period[\"release_date\"]\n            release_date = release_date_iso.split(\"T\")[0]\n\n        # Validate date format for explicit dates\n        try:\n            datetime.strptime(release_date, \"%Y-%m-%d\")\n        except ValueError:\n            raise ClientError(\n                f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n            )\n\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/\"\n        )\n        return response.json()\n\n    def download_inference_data(\n        self,\n        release_date: str,\n        dest_path: str,\n        poll: bool = True,\n        poll_interval: int = 30,\n        timeout: Optional[int] = 900,\n    ):\n        \"\"\"Downloads the inference features file for a specific period.\n\n        Args:\n            release_date: The release date of the inference data in 'YYYY-MM-DD' format\n                          or the special values ``\"current\"`` or ``\"latest\"``.\n            dest_path: The local file path to save the downloaded features file.\n            poll: Whether to wait for the inference data to be available before downloading.\n            poll_interval: Seconds to wait between retries when polling.\n            timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n                ``None`` waits indefinitely.\n\n        Raises:\n            NotFoundError: If the challenge, inference data, or its file is not found.\n            ClientError: If the date format is invalid.\n        \"\"\"\n        if release_date == \"current\":\n            # If polling is enabled, delegate to wait_for_inference_data which wraps\n            # this method and adds retry logic. Otherwise attempt a single direct\n            # download request.\n            if poll:\n                self.wait_for_inference_data(dest_path, poll_interval, timeout)\n                return\n\n            # Polling disabled \u2192 attempt once and propagate NotFoundError on 404.\n            endpoint = (\n                f\"/challenges/{self.challenge_slug}/inference_data/current/download/\"\n            )\n        else:\n            if release_date == \"latest\":\n                latest_info = self.get_inference_data(\"latest\")\n                release_date_iso = latest_info.get(\"release_date\")\n                release_date = (\n                    release_date_iso.split(\"T\")[0] if release_date_iso else None\n                )\n                if not release_date:\n                    raise CrowdCentAPIError(\n                        \"Malformed response when resolving latest inference period.\"\n                    )\n\n            # Validate date format after any resolution.\n            try:\n                datetime.strptime(release_date, \"%Y-%m-%d\")\n            except ValueError:\n                raise ClientError(\n                    f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n                )\n\n            endpoint = f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/download/\"\n\n        logger.info(\n            f\"Downloading inference data for challenge '{self.challenge_slug}' {release_date} to {dest_path}\"\n        )\n        response = self._request(\"GET\", endpoint, stream=True)\n\n        # Get total file size from headers\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        try:\n            from tqdm import tqdm\n\n            with open(dest_path, \"wb\") as f:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(dest_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        f.write(chunk)\n                        pbar.update(len(chunk))\n            logger.info(f\"Successfully downloaded inference data to {dest_path}\")\n        except IOError as e:\n            logger.error(f\"Failed to write inference data to {dest_path}: {e}\")\n            raise CrowdCentAPIError(f\"Failed to write inference data file: {e}\") from e\n\n    def wait_for_inference_data(\n        self,\n        dest_path: str,\n        poll_interval: int = 30,\n        timeout: Optional[int] = 900,\n    ) -&gt; None:\n        \"\"\"Waits for the *current* inference data release to appear and downloads it.\n\n        The internal data-generation pipeline begins around 14:00 UTC, but the\n        public inference file becomes available only after it passes data-quality\n        checks. This helper repeatedly calls\n        :py:meth:`download_inference_data` with ``release_date=\"current\"`` until\n        the file is ready (HTTP 404s are silently retried).\n\n        Args:\n            dest_path: Local path where the parquet file will be saved once available.\n            poll_interval: Seconds to wait between retries.\n            timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n                ``None`` waits indefinitely.\n\n        Raises:\n            TimeoutError: If *timeout* seconds pass without a successful download.\n            CrowdCentAPIError: For unrecoverable errors returned by the API.\n        \"\"\"\n        start_time = time.time()\n        attempts = 0\n\n        while True:\n            attempts += 1\n            try:\n                # Try to download the *current* period *once*. Pass poll=False to avoid\n                # the mutual recursion between `wait_for_inference_data` and\n                # `download_inference_data` which would otherwise trigger an infinite\n                # loop when the file is not yet available.\n                self.download_inference_data(\"current\", dest_path, poll=False)\n                logger.info(\n                    f\"Successfully downloaded inference data after {attempts} attempt(s) to {dest_path}\"\n                )\n                return  # Success \u2013 exit the loop\n            except NotFoundError:\n                # File not published yet \u2013 check timeout and sleep before retrying.\n                elapsed = time.time() - start_time\n                if timeout is not None and elapsed &gt;= timeout:\n                    raise TimeoutError(\n                        f\"Inference data was not available after waiting {timeout} seconds.\"\n                    )\n                logger.debug(\n                    f\"Inference data not yet available (attempt {attempts}). \"\n                    f\"Sleeping {poll_interval}s before retrying.\"\n                )\n                time.sleep(poll_interval)\n\n    # --- Submission Methods ---\n\n    def list_submissions(self, period: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists the authenticated user's submissions for this challenge.\n\n        Args:\n            period: Optional filter for submissions by period:\n                  - 'current': Only show submissions for the current active period\n                  - 'YYYY-MM-DD': Only show submissions for a specific inference period date\n\n        Returns:\n            A list of dictionaries, each representing a submission.\n        \"\"\"\n        params = {}\n        if period:\n            params[\"period\"] = period\n\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/submissions/\", params=params\n        )\n        return response.json()\n\n    def get_submission(self, submission_id: int) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for a specific submission by its ID.\n\n        Args:\n            submission_id: The ID of the submission to retrieve.\n\n        Returns:\n            A dictionary representing the specified submission.\n\n        Raises:\n            NotFoundError: If the submission with the given ID is not found\n                           or doesn't belong to the user.\n        \"\"\"\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/submissions/{submission_id}/\"\n        )\n        return response.json()\n\n    @nw.narwhalify\n    def submit_predictions(\n        self,\n        file_path: str = \"submission.parquet\",\n        df: Optional[IntoFrameT] = None,\n        slot: int = 1,\n        temp: bool = True,\n        max_retries: int = 3,\n        retry_delay: float = 1.0,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Submits predictions for the current active inference period of this challenge.\n\n        You can provide either a file path to an existing Parquet file or a DataFrame\n        that will be temporarily saved as Parquet for submission.\n\n        The data must contain the required prediction columns specified by the challenge\n        (e.g., id, pred_10d, pred_30d).\n\n        Args:\n            file_path: Optional path to an existing prediction Parquet file.\n            df: Optional DataFrame with the prediction columns. If provided,\n                it will be temporarily saved as Parquet for submission.\n            slot: Submission slot number (1-based).\n            temp: Whether to save the DataFrame to a temporary file.\n            max_retries: Maximum number of retry attempts for connection errors (default: 3).\n            retry_delay: Initial delay between retries in seconds (default: 1.0).\n\n        Returns:\n            A dictionary representing the newly created or updated submission.\n\n        Raises:\n            ValueError: If neither file_path nor df is provided, or if both are provided.\n            FileNotFoundError: If the specified file_path does not exist.\n            ClientError: If the submission is invalid (e.g., wrong format,\n                         outside submission window, already submitted, etc).\n\n        Examples:\n            # Submit from a DataFrame\n            client.submit_predictions(df=predictions_df)\n\n            # Submit from a file\n            client.submit_predictions(file_path=\"predictions.parquet\")\n\n            # Submit with custom retry settings\n            client.submit_predictions(df=predictions_df, max_retries=5, retry_delay=2.0)\n        \"\"\"\n        if df is not None:\n            df.write_parquet(file_path)\n            logger.info(f\"Wrote DataFrame to temporary file: {file_path}\")\n\n        logger.info(\n            f\"Submitting predictions from {file_path} to challenge '{self.challenge_slug}' (Slot: {slot or '1'})\"\n        )\n\n        try:\n            with open(file_path, \"rb\") as f:\n                files = {\n                    \"prediction_file\": (\n                        os.path.basename(file_path),\n                        f,\n                        \"application/octet-stream\",\n                    )\n                }\n                data_payload = {\"slot\": str(slot)}\n                response = self._request(\n                    \"POST\",\n                    f\"/challenges/{self.challenge_slug}/submissions/\",\n                    files=files,\n                    data=data_payload,  # Pass slot in data\n                    max_retries=max_retries,\n                    retry_delay=retry_delay,\n                )\n            logger.info(\n                f\"Successfully submitted predictions to challenge '{self.challenge_slug}'\"\n            )\n            return response.json()\n        except FileNotFoundError as e:\n            logger.error(f\"Prediction file not found at {file_path}\")\n            raise FileNotFoundError(f\"Prediction file not found at {file_path}\") from e\n        except IOError as e:\n            logger.error(f\"Failed to read prediction file {file_path}: {e}\")\n            raise CrowdCentAPIError(f\"Failed to read prediction file: {e}\") from e\n        finally:\n            # Clean up the temporary file if we created one\n            if df is not None and temp:\n                try:\n                    os.unlink(file_path)\n                    logger.debug(f\"Cleaned up temporary file: {file_path}\")\n                except Exception as e:\n                    logger.warning(\n                        f\"Failed to clean up temporary file {file_path}: {e}\"\n                    )\n\n    # --- Challenge Switching ---\n\n    def switch_challenge(self, new_challenge_slug: str) -&gt; None:\n        \"\"\"Switch this client to interact with a different challenge.\n\n        Args:\n            new_challenge_slug: The slug identifier for the new challenge.\n\n        Returns:\n            None. The client is modified in-place.\n        \"\"\"\n        self.challenge_slug = new_challenge_slug\n        logger.info(f\"Client switched to challenge '{new_challenge_slug}'\")\n\n    # --- Meta-Model Download ---\n\n    def download_meta_model(self, dest_path: str):\n        \"\"\"Downloads the consolidated meta-model file for this challenge.\n\n        The meta-model is typically an aggregation (e.g., average) of all valid\n        submissions for past inference periods.\n\n        Args:\n            dest_path: The local file path to save the downloaded meta-model.\n\n        Raises:\n            NotFoundError: If the challenge or its meta-model file is not found.\n            CrowdCentAPIError: For issues during download or file writing.\n            PermissionDenied: If the meta-model is not public and user lacks permission.\n        \"\"\"\n        endpoint = f\"/challenges/{self.challenge_slug}/meta_model/download/\"\n        logger.info(\n            f\"Downloading consolidated meta-model for challenge '{self.challenge_slug}' to {dest_path}\"\n        )\n\n        # The API endpoint redirects to a signed URL, but requests handles the redirect automatically.\n        # We still stream the response from the final URL.\n        response = self._request(\"GET\", endpoint, stream=True)\n\n        # Get total file size from headers\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        try:\n            from tqdm import tqdm\n\n            with open(dest_path, \"wb\") as f:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(dest_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        f.write(chunk)\n                        pbar.update(len(chunk))\n            logger.info(\n                f\"Successfully downloaded consolidated meta-model to {dest_path}\"\n            )\n        except IOError as e:\n            logger.error(f\"Failed to write meta-model to {dest_path}: {e}\")\n            raise CrowdCentAPIError(f\"Failed to write meta-model file: {e}\") from e\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.__init__","title":"<code>__init__(challenge_slug, api_key=None, base_url=None)</code>","text":"<p>Initializes the ChallengeClient for a specific challenge.</p> <p>Parameters:</p> Name Type Description Default <code>challenge_slug</code> <code>str</code> <p>The unique identifier (slug) for the challenge.</p> required <code>api_key</code> <code>Optional[str]</code> <p>Your CrowdCent API key. If not provided, it will attempt      to load from the CROWDCENT_API_KEY environment variable      or a .env file.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL of the CrowdCent API. Defaults to       https://crowdcent.com/api.</p> <code>None</code> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def __init__(\n    self,\n    challenge_slug: str,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the ChallengeClient for a specific challenge.\n\n    Args:\n        challenge_slug: The unique identifier (slug) for the challenge.\n        api_key: Your CrowdCent API key. If not provided, it will attempt\n                 to load from the CROWDCENT_API_KEY environment variable\n                 or a .env file.\n        base_url: The base URL of the CrowdCent API. Defaults to\n                  https://crowdcent.com/api.\n    \"\"\"\n    load_dotenv()  # Load .env file if present\n    self.api_key = api_key or os.getenv(self.API_KEY_ENV_VAR)\n    if not self.api_key:\n        raise AuthenticationError(\n            f\"API key not provided and not found in environment variable \"\n            f\"'{self.API_KEY_ENV_VAR}' or .env file.\"\n        )\n\n    self.challenge_slug = challenge_slug\n    self.base_url = (base_url or self.DEFAULT_BASE_URL).rstrip(\"/\")\n    self.session = requests.Session()\n    self.session.headers.update({\"Authorization\": f\"Api-Key {self.api_key}\"})\n    logger.info(\n        f\"ChallengeClient initialized for '{challenge_slug}' at URL: {self.base_url}\"\n    )\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_all_challenges","title":"<code>list_all_challenges(api_key=None, base_url=None)</code>  <code>classmethod</code>","text":"<p>Lists all active challenges.</p> <p>This is a class method that doesn't require a challenge_slug. Use this to discover available challenges before initializing a ChallengeClient.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Your CrowdCent API key. If not provided, it will attempt      to load from the CROWDCENT_API_KEY environment variable      or a .env file.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL of the CrowdCent API. Defaults to       http://crowdcent.com/api.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing an active challenge.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>@classmethod\ndef list_all_challenges(\n    cls, api_key: Optional[str] = None, base_url: Optional[str] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists all active challenges.\n\n    This is a class method that doesn't require a challenge_slug.\n    Use this to discover available challenges before initializing a ChallengeClient.\n\n    Args:\n        api_key: Your CrowdCent API key. If not provided, it will attempt\n                 to load from the CROWDCENT_API_KEY environment variable\n                 or a .env file.\n        base_url: The base URL of the CrowdCent API. Defaults to\n                  http://crowdcent.com/api.\n\n    Returns:\n        A list of dictionaries, each representing an active challenge.\n    \"\"\"\n    # Create a temporary session for this request\n    load_dotenv()\n    api_key = api_key or os.getenv(cls.API_KEY_ENV_VAR)\n    if not api_key:\n        raise AuthenticationError(\n            f\"API key not provided and not found in environment variable \"\n            f\"'{cls.API_KEY_ENV_VAR}' or .env file.\"\n        )\n\n    base_url = (base_url or cls.DEFAULT_BASE_URL).rstrip(\"/\")\n    session = requests.Session()\n    session.headers.update({\"Authorization\": f\"Api-Key {api_key}\"})\n\n    url = f\"{base_url}/challenges/\"\n    try:\n        response = session.get(url)\n        response.raise_for_status()\n        return response.json()\n    except requests_exceptions.HTTPError as e:\n        status_code = e.response.status_code\n        if status_code == 401:\n            raise AuthenticationError(\"Authentication failed (401)\")\n        elif status_code == 404:\n            raise NotFoundError(\"Resource not found (404)\")\n        elif 400 &lt;= status_code &lt; 500:\n            raise ClientError(f\"Client error ({status_code})\")\n        elif 500 &lt;= status_code &lt; 600:\n            raise ServerError(f\"Server error ({status_code})\")\n        else:\n            raise CrowdCentAPIError(f\"HTTP error ({status_code})\")\n    except requests_exceptions.RequestException as e:\n        raise CrowdCentAPIError(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_challenge","title":"<code>get_challenge()</code>","text":"<p>Gets details for this challenge.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing this challenge.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge with the given slug is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_challenge(self) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for this challenge.\n\n    Returns:\n        A dictionary representing this challenge.\n\n    Raises:\n        NotFoundError: If the challenge with the given slug is not found.\n    \"\"\"\n    response = self._request(\"GET\", f\"/challenges/{self.challenge_slug}/\")\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_training_datasets","title":"<code>list_training_datasets()</code>","text":"<p>Lists all training dataset versions for this challenge.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing a training dataset version.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def list_training_datasets(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists all training dataset versions for this challenge.\n\n    Returns:\n        A list of dictionaries, each representing a training dataset version.\n\n    Raises:\n        NotFoundError: If the challenge is not found.\n    \"\"\"\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/training_data/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_training_dataset","title":"<code>get_training_dataset(version)</code>","text":"<p>Gets details for a specific training dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version string of the training dataset (e.g., '1.0', '2.1')      or the special value <code>\"latest\"</code> to get the latest version.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing the specified training dataset.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge or the specified training dataset is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_training_dataset(self, version: str) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for a specific training dataset version.\n\n    Args:\n        version: The version string of the training dataset (e.g., '1.0', '2.1')\n                 or the special value ``\"latest\"`` to get the latest version.\n\n    Returns:\n        A dictionary representing the specified training dataset.\n\n    Raises:\n        NotFoundError: If the challenge or the specified training dataset is not found.\n    \"\"\"\n    if version == \"latest\":\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/training_data/latest/\"\n        )\n        return response.json()\n\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/training_data/{version}/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.download_training_dataset","title":"<code>download_training_dataset(version, dest_path)</code>","text":"<p>Downloads the training data file for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version string of the training dataset (e.g., '1.0', '2.1')     or 'latest' to get the latest version.</p> required <code>dest_path</code> <code>str</code> <p>The local file path to save the downloaded dataset.</p> required <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge, dataset, or its file is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def download_training_dataset(self, version: str, dest_path: str):\n    \"\"\"Downloads the training data file for a specific dataset version.\n\n    Args:\n        version: The version string of the training dataset (e.g., '1.0', '2.1')\n                or 'latest' to get the latest version.\n        dest_path: The local file path to save the downloaded dataset.\n\n    Raises:\n        NotFoundError: If the challenge, dataset, or its file is not found.\n    \"\"\"\n    if version == \"latest\":\n        latest_info = self.get_training_dataset(\"latest\")\n        version = latest_info[\"version\"]\n\n    endpoint = (\n        f\"/challenges/{self.challenge_slug}/training_data/{version}/download/\"\n    )\n\n    logger.info(\n        f\"Downloading training data for challenge '{self.challenge_slug}' v{version} to {dest_path}\"\n    )\n    response = self._request(\"GET\", endpoint, stream=True)\n\n    # Get total file size from headers\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    try:\n        from tqdm import tqdm\n\n        with open(dest_path, \"wb\") as f:\n            with tqdm(\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                desc=f\"Downloading {os.path.basename(dest_path)}\",\n            ) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n                    pbar.update(len(chunk))\n        logger.info(f\"Successfully downloaded training data to {dest_path}\")\n    except IOError as e:\n        logger.error(f\"Failed to write dataset to {dest_path}: {e}\")\n        raise CrowdCentAPIError(f\"Failed to write dataset file: {e}\") from e\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_inference_data","title":"<code>list_inference_data()</code>","text":"<p>Lists all inference data periods for this challenge.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing an inference data period.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def list_inference_data(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists all inference data periods for this challenge.\n\n    Returns:\n        A list of dictionaries, each representing an inference data period.\n\n    Raises:\n        NotFoundError: If the challenge is not found.\n    \"\"\"\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_inference_data","title":"<code>get_inference_data(release_date)</code>","text":"<p>Gets details for a specific inference data period by its release date.</p> <p>Parameters:</p> Name Type Description Default <code>release_date</code> <code>str</code> <p>The release date of the inference data in 'YYYY-MM-DD' format.           You can also pass the special values:           - <code>\"current\"</code> to fetch the current active inference period           - <code>\"latest\"</code> to fetch the most recently available inference period</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing the specified inference data period.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge or the specified inference data is not found.</p> <code>ClientError</code> <p>If the date format is invalid.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_inference_data(self, release_date: str) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for a specific inference data period by its release date.\n\n    Args:\n        release_date: The release date of the inference data in 'YYYY-MM-DD' format.\n                      You can also pass the special values:\n                      - ``\"current\"`` to fetch the current active inference period\n                      - ``\"latest\"`` to fetch the most recently *available* inference period\n\n    Returns:\n        A dictionary representing the specified inference data period.\n\n    Raises:\n        NotFoundError: If the challenge or the specified inference data is not found.\n        ClientError: If the date format is invalid.\n    \"\"\"\n    if release_date == \"current\":\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/current/\"\n        )\n        return response.json()\n\n    if release_date == \"latest\":\n        # Simply resolve via list_inference_data(); avoid noisy probe.\n        periods = self.list_inference_data()\n        if not periods:\n            raise NotFoundError(\n                \"No inference data periods found for this challenge.\"\n            )\n\n        latest_period = max(periods, key=lambda p: p[\"release_date\"])\n        release_date_iso = latest_period[\"release_date\"]\n        release_date = release_date_iso.split(\"T\")[0]\n\n    # Validate date format for explicit dates\n    try:\n        datetime.strptime(release_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise ClientError(\n            f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n        )\n\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.download_inference_data","title":"<code>download_inference_data(release_date, dest_path, poll=True, poll_interval=30, timeout=900)</code>","text":"<p>Downloads the inference features file for a specific period.</p> <p>Parameters:</p> Name Type Description Default <code>release_date</code> <code>str</code> <p>The release date of the inference data in 'YYYY-MM-DD' format           or the special values <code>\"current\"</code> or <code>\"latest\"</code>.</p> required <code>dest_path</code> <code>str</code> <p>The local file path to save the downloaded features file.</p> required <code>poll</code> <code>bool</code> <p>Whether to wait for the inference data to be available before downloading.</p> <code>True</code> <code>poll_interval</code> <code>int</code> <p>Seconds to wait between retries when polling.</p> <code>30</code> <code>timeout</code> <code>Optional[int]</code> <p>Maximum seconds to wait before raising :class:<code>TimeoutError</code>. <code>None</code> waits indefinitely.</p> <code>900</code> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge, inference data, or its file is not found.</p> <code>ClientError</code> <p>If the date format is invalid.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def download_inference_data(\n    self,\n    release_date: str,\n    dest_path: str,\n    poll: bool = True,\n    poll_interval: int = 30,\n    timeout: Optional[int] = 900,\n):\n    \"\"\"Downloads the inference features file for a specific period.\n\n    Args:\n        release_date: The release date of the inference data in 'YYYY-MM-DD' format\n                      or the special values ``\"current\"`` or ``\"latest\"``.\n        dest_path: The local file path to save the downloaded features file.\n        poll: Whether to wait for the inference data to be available before downloading.\n        poll_interval: Seconds to wait between retries when polling.\n        timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n            ``None`` waits indefinitely.\n\n    Raises:\n        NotFoundError: If the challenge, inference data, or its file is not found.\n        ClientError: If the date format is invalid.\n    \"\"\"\n    if release_date == \"current\":\n        # If polling is enabled, delegate to wait_for_inference_data which wraps\n        # this method and adds retry logic. Otherwise attempt a single direct\n        # download request.\n        if poll:\n            self.wait_for_inference_data(dest_path, poll_interval, timeout)\n            return\n\n        # Polling disabled \u2192 attempt once and propagate NotFoundError on 404.\n        endpoint = (\n            f\"/challenges/{self.challenge_slug}/inference_data/current/download/\"\n        )\n    else:\n        if release_date == \"latest\":\n            latest_info = self.get_inference_data(\"latest\")\n            release_date_iso = latest_info.get(\"release_date\")\n            release_date = (\n                release_date_iso.split(\"T\")[0] if release_date_iso else None\n            )\n            if not release_date:\n                raise CrowdCentAPIError(\n                    \"Malformed response when resolving latest inference period.\"\n                )\n\n        # Validate date format after any resolution.\n        try:\n            datetime.strptime(release_date, \"%Y-%m-%d\")\n        except ValueError:\n            raise ClientError(\n                f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n            )\n\n        endpoint = f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/download/\"\n\n    logger.info(\n        f\"Downloading inference data for challenge '{self.challenge_slug}' {release_date} to {dest_path}\"\n    )\n    response = self._request(\"GET\", endpoint, stream=True)\n\n    # Get total file size from headers\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    try:\n        from tqdm import tqdm\n\n        with open(dest_path, \"wb\") as f:\n            with tqdm(\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                desc=f\"Downloading {os.path.basename(dest_path)}\",\n            ) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n                    pbar.update(len(chunk))\n        logger.info(f\"Successfully downloaded inference data to {dest_path}\")\n    except IOError as e:\n        logger.error(f\"Failed to write inference data to {dest_path}: {e}\")\n        raise CrowdCentAPIError(f\"Failed to write inference data file: {e}\") from e\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.wait_for_inference_data","title":"<code>wait_for_inference_data(dest_path, poll_interval=30, timeout=900)</code>","text":"<p>Waits for the current inference data release to appear and downloads it.</p> <p>The internal data-generation pipeline begins around 14:00 UTC, but the public inference file becomes available only after it passes data-quality checks. This helper repeatedly calls meth:<code>download_inference_data</code> with <code>release_date=\"current\"</code> until the file is ready (HTTP 404s are silently retried).</p> <p>Parameters:</p> Name Type Description Default <code>dest_path</code> <code>str</code> <p>Local path where the parquet file will be saved once available.</p> required <code>poll_interval</code> <code>int</code> <p>Seconds to wait between retries.</p> <code>30</code> <code>timeout</code> <code>Optional[int]</code> <p>Maximum seconds to wait before raising :class:<code>TimeoutError</code>. <code>None</code> waits indefinitely.</p> <code>900</code> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If timeout seconds pass without a successful download.</p> <code>CrowdCentAPIError</code> <p>For unrecoverable errors returned by the API.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def wait_for_inference_data(\n    self,\n    dest_path: str,\n    poll_interval: int = 30,\n    timeout: Optional[int] = 900,\n) -&gt; None:\n    \"\"\"Waits for the *current* inference data release to appear and downloads it.\n\n    The internal data-generation pipeline begins around 14:00 UTC, but the\n    public inference file becomes available only after it passes data-quality\n    checks. This helper repeatedly calls\n    :py:meth:`download_inference_data` with ``release_date=\"current\"`` until\n    the file is ready (HTTP 404s are silently retried).\n\n    Args:\n        dest_path: Local path where the parquet file will be saved once available.\n        poll_interval: Seconds to wait between retries.\n        timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n            ``None`` waits indefinitely.\n\n    Raises:\n        TimeoutError: If *timeout* seconds pass without a successful download.\n        CrowdCentAPIError: For unrecoverable errors returned by the API.\n    \"\"\"\n    start_time = time.time()\n    attempts = 0\n\n    while True:\n        attempts += 1\n        try:\n            # Try to download the *current* period *once*. Pass poll=False to avoid\n            # the mutual recursion between `wait_for_inference_data` and\n            # `download_inference_data` which would otherwise trigger an infinite\n            # loop when the file is not yet available.\n            self.download_inference_data(\"current\", dest_path, poll=False)\n            logger.info(\n                f\"Successfully downloaded inference data after {attempts} attempt(s) to {dest_path}\"\n            )\n            return  # Success \u2013 exit the loop\n        except NotFoundError:\n            # File not published yet \u2013 check timeout and sleep before retrying.\n            elapsed = time.time() - start_time\n            if timeout is not None and elapsed &gt;= timeout:\n                raise TimeoutError(\n                    f\"Inference data was not available after waiting {timeout} seconds.\"\n                )\n            logger.debug(\n                f\"Inference data not yet available (attempt {attempts}). \"\n                f\"Sleeping {poll_interval}s before retrying.\"\n            )\n            time.sleep(poll_interval)\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_submissions","title":"<code>list_submissions(period=None)</code>","text":"<p>Lists the authenticated user's submissions for this challenge.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>Optional[str]</code> <p>Optional filter for submissions by period:   - 'current': Only show submissions for the current active period   - 'YYYY-MM-DD': Only show submissions for a specific inference period date</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing a submission.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def list_submissions(self, period: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists the authenticated user's submissions for this challenge.\n\n    Args:\n        period: Optional filter for submissions by period:\n              - 'current': Only show submissions for the current active period\n              - 'YYYY-MM-DD': Only show submissions for a specific inference period date\n\n    Returns:\n        A list of dictionaries, each representing a submission.\n    \"\"\"\n    params = {}\n    if period:\n        params[\"period\"] = period\n\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/submissions/\", params=params\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_submission","title":"<code>get_submission(submission_id)</code>","text":"<p>Gets details for a specific submission by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>submission_id</code> <code>int</code> <p>The ID of the submission to retrieve.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing the specified submission.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the submission with the given ID is not found            or doesn't belong to the user.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_submission(self, submission_id: int) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for a specific submission by its ID.\n\n    Args:\n        submission_id: The ID of the submission to retrieve.\n\n    Returns:\n        A dictionary representing the specified submission.\n\n    Raises:\n        NotFoundError: If the submission with the given ID is not found\n                       or doesn't belong to the user.\n    \"\"\"\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/submissions/{submission_id}/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions","title":"<code>submit_predictions(file_path='submission.parquet', df=None, slot=1, temp=True, max_retries=3, retry_delay=1.0)</code>","text":"<p>Submits predictions for the current active inference period of this challenge.</p> <p>You can provide either a file path to an existing Parquet file or a DataFrame that will be temporarily saved as Parquet for submission.</p> <p>The data must contain the required prediction columns specified by the challenge (e.g., id, pred_10d, pred_30d).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Optional path to an existing prediction Parquet file.</p> <code>'submission.parquet'</code> <code>df</code> <code>Optional[IntoFrameT]</code> <p>Optional DataFrame with the prediction columns. If provided, it will be temporarily saved as Parquet for submission.</p> <code>None</code> <code>slot</code> <code>int</code> <p>Submission slot number (1-based).</p> <code>1</code> <code>temp</code> <code>bool</code> <p>Whether to save the DataFrame to a temporary file.</p> <code>True</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for connection errors (default: 3).</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds (default: 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing the newly created or updated submission.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither file_path nor df is provided, or if both are provided.</p> <code>FileNotFoundError</code> <p>If the specified file_path does not exist.</p> <code>ClientError</code> <p>If the submission is invalid (e.g., wrong format,          outside submission window, already submitted, etc).</p> <p>Examples:</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions--submit-from-a-dataframe","title":"Submit from a DataFrame","text":"<p>client.submit_predictions(df=predictions_df)</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions--submit-from-a-file","title":"Submit from a file","text":"<p>client.submit_predictions(file_path=\"predictions.parquet\")</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions--submit-with-custom-retry-settings","title":"Submit with custom retry settings","text":"<p>client.submit_predictions(df=predictions_df, max_retries=5, retry_delay=2.0)</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>@nw.narwhalify\ndef submit_predictions(\n    self,\n    file_path: str = \"submission.parquet\",\n    df: Optional[IntoFrameT] = None,\n    slot: int = 1,\n    temp: bool = True,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n) -&gt; Dict[str, Any]:\n    \"\"\"Submits predictions for the current active inference period of this challenge.\n\n    You can provide either a file path to an existing Parquet file or a DataFrame\n    that will be temporarily saved as Parquet for submission.\n\n    The data must contain the required prediction columns specified by the challenge\n    (e.g., id, pred_10d, pred_30d).\n\n    Args:\n        file_path: Optional path to an existing prediction Parquet file.\n        df: Optional DataFrame with the prediction columns. If provided,\n            it will be temporarily saved as Parquet for submission.\n        slot: Submission slot number (1-based).\n        temp: Whether to save the DataFrame to a temporary file.\n        max_retries: Maximum number of retry attempts for connection errors (default: 3).\n        retry_delay: Initial delay between retries in seconds (default: 1.0).\n\n    Returns:\n        A dictionary representing the newly created or updated submission.\n\n    Raises:\n        ValueError: If neither file_path nor df is provided, or if both are provided.\n        FileNotFoundError: If the specified file_path does not exist.\n        ClientError: If the submission is invalid (e.g., wrong format,\n                     outside submission window, already submitted, etc).\n\n    Examples:\n        # Submit from a DataFrame\n        client.submit_predictions(df=predictions_df)\n\n        # Submit from a file\n        client.submit_predictions(file_path=\"predictions.parquet\")\n\n        # Submit with custom retry settings\n        client.submit_predictions(df=predictions_df, max_retries=5, retry_delay=2.0)\n    \"\"\"\n    if df is not None:\n        df.write_parquet(file_path)\n        logger.info(f\"Wrote DataFrame to temporary file: {file_path}\")\n\n    logger.info(\n        f\"Submitting predictions from {file_path} to challenge '{self.challenge_slug}' (Slot: {slot or '1'})\"\n    )\n\n    try:\n        with open(file_path, \"rb\") as f:\n            files = {\n                \"prediction_file\": (\n                    os.path.basename(file_path),\n                    f,\n                    \"application/octet-stream\",\n                )\n            }\n            data_payload = {\"slot\": str(slot)}\n            response = self._request(\n                \"POST\",\n                f\"/challenges/{self.challenge_slug}/submissions/\",\n                files=files,\n                data=data_payload,  # Pass slot in data\n                max_retries=max_retries,\n                retry_delay=retry_delay,\n            )\n        logger.info(\n            f\"Successfully submitted predictions to challenge '{self.challenge_slug}'\"\n        )\n        return response.json()\n    except FileNotFoundError as e:\n        logger.error(f\"Prediction file not found at {file_path}\")\n        raise FileNotFoundError(f\"Prediction file not found at {file_path}\") from e\n    except IOError as e:\n        logger.error(f\"Failed to read prediction file {file_path}: {e}\")\n        raise CrowdCentAPIError(f\"Failed to read prediction file: {e}\") from e\n    finally:\n        # Clean up the temporary file if we created one\n        if df is not None and temp:\n            try:\n                os.unlink(file_path)\n                logger.debug(f\"Cleaned up temporary file: {file_path}\")\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to clean up temporary file {file_path}: {e}\"\n                )\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.switch_challenge","title":"<code>switch_challenge(new_challenge_slug)</code>","text":"<p>Switch this client to interact with a different challenge.</p> <p>Parameters:</p> Name Type Description Default <code>new_challenge_slug</code> <code>str</code> <p>The slug identifier for the new challenge.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None. The client is modified in-place.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def switch_challenge(self, new_challenge_slug: str) -&gt; None:\n    \"\"\"Switch this client to interact with a different challenge.\n\n    Args:\n        new_challenge_slug: The slug identifier for the new challenge.\n\n    Returns:\n        None. The client is modified in-place.\n    \"\"\"\n    self.challenge_slug = new_challenge_slug\n    logger.info(f\"Client switched to challenge '{new_challenge_slug}'\")\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.download_meta_model","title":"<code>download_meta_model(dest_path)</code>","text":"<p>Downloads the consolidated meta-model file for this challenge.</p> <p>The meta-model is typically an aggregation (e.g., average) of all valid submissions for past inference periods.</p> <p>Parameters:</p> Name Type Description Default <code>dest_path</code> <code>str</code> <p>The local file path to save the downloaded meta-model.</p> required <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge or its meta-model file is not found.</p> <code>CrowdCentAPIError</code> <p>For issues during download or file writing.</p> <code>PermissionDenied</code> <p>If the meta-model is not public and user lacks permission.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def download_meta_model(self, dest_path: str):\n    \"\"\"Downloads the consolidated meta-model file for this challenge.\n\n    The meta-model is typically an aggregation (e.g., average) of all valid\n    submissions for past inference periods.\n\n    Args:\n        dest_path: The local file path to save the downloaded meta-model.\n\n    Raises:\n        NotFoundError: If the challenge or its meta-model file is not found.\n        CrowdCentAPIError: For issues during download or file writing.\n        PermissionDenied: If the meta-model is not public and user lacks permission.\n    \"\"\"\n    endpoint = f\"/challenges/{self.challenge_slug}/meta_model/download/\"\n    logger.info(\n        f\"Downloading consolidated meta-model for challenge '{self.challenge_slug}' to {dest_path}\"\n    )\n\n    # The API endpoint redirects to a signed URL, but requests handles the redirect automatically.\n    # We still stream the response from the final URL.\n    response = self._request(\"GET\", endpoint, stream=True)\n\n    # Get total file size from headers\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    try:\n        from tqdm import tqdm\n\n        with open(dest_path, \"wb\") as f:\n            with tqdm(\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                desc=f\"Downloading {os.path.basename(dest_path)}\",\n            ) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n                    pbar.update(len(chunk))\n        logger.info(\n            f\"Successfully downloaded consolidated meta-model to {dest_path}\"\n        )\n    except IOError as e:\n        logger.error(f\"Failed to write meta-model to {dest_path}: {e}\")\n        raise CrowdCentAPIError(f\"Failed to write meta-model file: {e}\") from e\n</code></pre>"},{"location":"tutorials/advanced-custom-dataset-eodhd/","title":"Build Your Own Custom Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install crowdcent-challenge numerblox eod centimators polars altair vegafusion vl-convert-python\n</pre> !pip install crowdcent-challenge numerblox eod centimators polars altair vegafusion vl-convert-python In\u00a0[\u00a0]: Copied! <pre>import os\nimport polars as pl\nfrom datetime import datetime\nimport requests\n\n# Set your EODHD API key\nEOD_API_KEY = \"YOUR_EODHD_API_KEY_HERE\"  # Get from https://eodhd.com/cp/dashboard\n\nif EOD_API_KEY == \"YOUR_EODHD_API_KEY_HERE\":\n    print(\"\u26a0\ufe0f Please set your EODHD API key above\")\n    print(\"Get one free at: https://eodhd.com/pricing-special-10?via=crowdcent\")\n</pre> import os import polars as pl from datetime import datetime import requests  # Set your EODHD API key EOD_API_KEY = \"YOUR_EODHD_API_KEY_HERE\"  # Get from https://eodhd.com/cp/dashboard  if EOD_API_KEY == \"YOUR_EODHD_API_KEY_HERE\":     print(\"\u26a0\ufe0f Please set your EODHD API key above\")     print(\"Get one free at: https://eodhd.com/pricing-special-10?via=crowdcent\") In\u00a0[3]: Copied! <pre>def get_hyperliquid_perpetuals(include_delisted: bool = False):\n    \"\"\"Get perpetual IDs as a polars DataFrame\n\n    Args:\n        include_delisted: If True, include delisted perpetuals in the output\n    \"\"\"\n\n    url = \"https://api.hyperliquid.xyz/info\"\n    payload = {\"type\": \"meta\"}\n    special_map_dict = {\n        \"POPCAT-USD.CC\": \"POPCAT28782-USD.CC\",\n        \"VVV-USD.CC\": \"VVV.CC\",\n        \"BRETT-USD.CC\": \"BRETT29743-USD.CC\",\n        \"UNIBOT-USD.CC\": \"UNIBOT27009-USD.CC\",\n        \"ZRO-USD.CC\": \"ZRO26997-USD.CC\",\n        \"MOVE-USD.CC\": \"MOVE32452-USD.CC\",\n        \"STG-USD.CC\": \"STG18934-USD.CC\",\n        \"GOAT-USD.CC\": \"GOAT33440-USD.CC\",\n        \"PEPE-USD.CC\": \"PEPE24478-USD.CC\",\n        \"PROMPT-USD.CC\": \"PROMPT-USD.CC\",\n        \"NIL-USD.CC\": \"NIL35702-USD.CC\",\n        \"MNT-USD.CC\": \"MNT27075-USD.CC\",\n        \"ACE-USD.CC\": \"ACE28674-USD.CC\",\n        \"HYPE-USD.CC\": \"HYPE32196-USD.CC\",\n        \"IMX-USD.CC\": \"IMX10603-USD.CC\",\n        \"INIT-USD.CC\": \"INIT-USD.CC\",\n        \"PURR-USD.CC\": \"PURR34332-USD.CC\",\n        \"MOODENG-USD.CC\": \"MOODENG33093-USD.CC\",\n        \"CHILLGUY-USD.CC\": \"CHILLGUY-USD.CC\",\n        \"FARTCOIN-USD.CC\": \"FARTCOIN-USD.CC\",\n        \"GRASS-USD.CC\": \"GRASS32956-USD.CC\",\n        \"GRIFFAIN-USD.CC\": \"GRIFFAIN-USD.CC\",\n        \"MELANIA-USD.CC\": \"MELANIA35347-USD.CC\",\n        \"KAITO-USD.CC\": \"KAITO-USD.CC\",\n        \"SUI-USD.CC\": \"SUI20947-USD.CC\",\n        \"BERA-USD.CC\": \"BERA-USD.CC\",\n        \"MEW-USD.CC\": \"MEW30126-USD.CC\",\n        \"ANIME-USD.CC\": \"ANIME35319-USD.CC\",\n        \"NEIRO-USD.CC\": \"NEIRO32521-USD.CC\",\n        \"DOGS-USD.CC\": \"DOGS32698-USD.CC\",\n        \"STX-USD.CC\": \"STX4847-USD.CC\",\n        \"S-USD.CC\": \"S32684-USD.CC\",\n        \"COMP-USD.CC\": \"COMP5692-USD.CC\",\n        \"TRUMP-USD.CC\": \"TRUMP-OFFICIAL-USD.CC\",\n        \"BLAST-USD.CC\": \"BLAST28480-USD.CC\",\n        \"TAO-USD.CC\": \"TAO22974-USD.CC\",\n        \"SAGA-USD.CC\": \"SAGA30372-USD.CC\",\n        \"TON-USD.CC\": \"TON11419-USD.CC\",\n        \"BIO-USD.CC\": \"BIO.CC\",\n        \"GMX-USD.CC\": \"GMX11857-USD.CC\",\n        \"NTRN-USD.CC\": \"NTRN26680-USD.CC\",\n        \"SUPER-USD.CC\": \"SUPER8290-USD.CC\",\n        \"SCR-USD.CC\": \"SCR26998-USD.CC\",\n        \"BANANA-USD.CC\": \"BANANA28066-USD.CC\",\n        \"ME-USD.CC\": \"ME32197-USD.CC\",\n        \"GMT-USD.CC\": \"GMT18069-USD.CC\",\n        \"IO-USD.CC\": \"IO29835-USD.CC\",\n        \"ZK-USD.CC\": \"ZKSYNC.CC\",\n        \"ALT-USD.CC\": \"ALT29073-USD.CC\",\n        \"POL-USD.CC\": \"POL28321-USD.CC\",\n        \"WCT-USD.CC\": \"WCT33152-USD.CC\",\n        \"XAI-USD.CC\": \"XAI28933-USD.CC\",\n        \"JUP-USD.CC\": \"JUP29210-USD.CC\",\n        \"APE-USD.CC\": \"APE3-USD.CC\",\n        \"SPX-USD.CC\": \"SPX28081-USD.CC\",\n        \"HYPER-USD.CC\": \"HYPER36281-USD.CC\",\n        \"IP-USD.CC\": \"IP-USD.CC\",\n        \"ZORA-USD.CC\": \"ZORA35931-USD.CC\",\n        \"PEOPLE-USD.CC\": \"PEOPLE-USD.CC\",\n        \"BABY-USD.CC\": \"BABY32198-USD.CC\",\n        \"ARB-USD.CC\": \"ARB11841-USD.CC\",\n        \"UNI-USD.CC\": \"UNI7083-USD.CC\",\n        \"OMNI-USD.CC\": \"OMNI30315-USD.CC\",\n        \"SOPH-USD.CC\": \"SOPHON-USD.CC\",\n        \"NEIROETH-USD.CC\": \"NEIRO-USD.CC\",\n        \"APT-USD.CC\": \"APT21794-USD.CC\",\n        \"STRK-USD.CC\": \"STRK22691-USD.CC\",\n        \"RESOLV-USD.CC\": \"RESOLV-USD.CC\",\n        \"TST-USD.CC\": \"TST35647-USD.CC\",\n        \"PUMP-USD.CC\": \"PUMP29601-USD.CC\",\n        \"WLFI-USD.CC\": \"WLFI33251-USD.CC\",\n        \"ASTER-USD.CC\": \"ASTER36341-USD.CC\",\n        \"SKY-USD.CC\": \"SKY33038-USD.CC\",\n    }\n    response = requests.post(url, json=payload)\n    response.raise_for_status()\n\n    data = response.json()\n    universe = data.get(\"universe\", [])\n\n    # Extract names of perpetuals based on delisted status\n    perpetual_ids = [\n        perp[\"name\"]\n        for perp in universe\n        if include_delisted or not perp.get(\"isDelisted\", False)\n    ]\n\n    # Create DataFrame with just id column\n    df = pl.DataFrame({\"id\": perpetual_ids})\n    df = df.with_columns(\n        (pl.col(\"id\") + \"-USD.CC\")\n        .str.replace(\"k\", \"\")\n        .replace(special_map_dict)\n        .alias(\"eodhd_id\")\n    )\n\n\n    return df\n        \n\n# Get the Hyperliquid universe\nperpetuals_df = get_hyperliquid_perpetuals(include_delisted=True)\nprint(perpetuals_df)\n# Extract ticker lists\neodhd_tickers = perpetuals_df[\"eodhd_id\"].to_list()\nid_mapping = dict(perpetuals_df.select(\"eodhd_id\", \"id\").iter_rows())\n\nprint(f\"\\n\ud83d\udcca Dataset will include {len(eodhd_tickers)} cryptocurrencies\")\nprint(f\"Sample tickers: {eodhd_tickers[:10]}\")\nprint(f\"Sample IDs: {list(id_mapping.values())[:10]}\")\n</pre> def get_hyperliquid_perpetuals(include_delisted: bool = False):     \"\"\"Get perpetual IDs as a polars DataFrame      Args:         include_delisted: If True, include delisted perpetuals in the output     \"\"\"      url = \"https://api.hyperliquid.xyz/info\"     payload = {\"type\": \"meta\"}     special_map_dict = {         \"POPCAT-USD.CC\": \"POPCAT28782-USD.CC\",         \"VVV-USD.CC\": \"VVV.CC\",         \"BRETT-USD.CC\": \"BRETT29743-USD.CC\",         \"UNIBOT-USD.CC\": \"UNIBOT27009-USD.CC\",         \"ZRO-USD.CC\": \"ZRO26997-USD.CC\",         \"MOVE-USD.CC\": \"MOVE32452-USD.CC\",         \"STG-USD.CC\": \"STG18934-USD.CC\",         \"GOAT-USD.CC\": \"GOAT33440-USD.CC\",         \"PEPE-USD.CC\": \"PEPE24478-USD.CC\",         \"PROMPT-USD.CC\": \"PROMPT-USD.CC\",         \"NIL-USD.CC\": \"NIL35702-USD.CC\",         \"MNT-USD.CC\": \"MNT27075-USD.CC\",         \"ACE-USD.CC\": \"ACE28674-USD.CC\",         \"HYPE-USD.CC\": \"HYPE32196-USD.CC\",         \"IMX-USD.CC\": \"IMX10603-USD.CC\",         \"INIT-USD.CC\": \"INIT-USD.CC\",         \"PURR-USD.CC\": \"PURR34332-USD.CC\",         \"MOODENG-USD.CC\": \"MOODENG33093-USD.CC\",         \"CHILLGUY-USD.CC\": \"CHILLGUY-USD.CC\",         \"FARTCOIN-USD.CC\": \"FARTCOIN-USD.CC\",         \"GRASS-USD.CC\": \"GRASS32956-USD.CC\",         \"GRIFFAIN-USD.CC\": \"GRIFFAIN-USD.CC\",         \"MELANIA-USD.CC\": \"MELANIA35347-USD.CC\",         \"KAITO-USD.CC\": \"KAITO-USD.CC\",         \"SUI-USD.CC\": \"SUI20947-USD.CC\",         \"BERA-USD.CC\": \"BERA-USD.CC\",         \"MEW-USD.CC\": \"MEW30126-USD.CC\",         \"ANIME-USD.CC\": \"ANIME35319-USD.CC\",         \"NEIRO-USD.CC\": \"NEIRO32521-USD.CC\",         \"DOGS-USD.CC\": \"DOGS32698-USD.CC\",         \"STX-USD.CC\": \"STX4847-USD.CC\",         \"S-USD.CC\": \"S32684-USD.CC\",         \"COMP-USD.CC\": \"COMP5692-USD.CC\",         \"TRUMP-USD.CC\": \"TRUMP-OFFICIAL-USD.CC\",         \"BLAST-USD.CC\": \"BLAST28480-USD.CC\",         \"TAO-USD.CC\": \"TAO22974-USD.CC\",         \"SAGA-USD.CC\": \"SAGA30372-USD.CC\",         \"TON-USD.CC\": \"TON11419-USD.CC\",         \"BIO-USD.CC\": \"BIO.CC\",         \"GMX-USD.CC\": \"GMX11857-USD.CC\",         \"NTRN-USD.CC\": \"NTRN26680-USD.CC\",         \"SUPER-USD.CC\": \"SUPER8290-USD.CC\",         \"SCR-USD.CC\": \"SCR26998-USD.CC\",         \"BANANA-USD.CC\": \"BANANA28066-USD.CC\",         \"ME-USD.CC\": \"ME32197-USD.CC\",         \"GMT-USD.CC\": \"GMT18069-USD.CC\",         \"IO-USD.CC\": \"IO29835-USD.CC\",         \"ZK-USD.CC\": \"ZKSYNC.CC\",         \"ALT-USD.CC\": \"ALT29073-USD.CC\",         \"POL-USD.CC\": \"POL28321-USD.CC\",         \"WCT-USD.CC\": \"WCT33152-USD.CC\",         \"XAI-USD.CC\": \"XAI28933-USD.CC\",         \"JUP-USD.CC\": \"JUP29210-USD.CC\",         \"APE-USD.CC\": \"APE3-USD.CC\",         \"SPX-USD.CC\": \"SPX28081-USD.CC\",         \"HYPER-USD.CC\": \"HYPER36281-USD.CC\",         \"IP-USD.CC\": \"IP-USD.CC\",         \"ZORA-USD.CC\": \"ZORA35931-USD.CC\",         \"PEOPLE-USD.CC\": \"PEOPLE-USD.CC\",         \"BABY-USD.CC\": \"BABY32198-USD.CC\",         \"ARB-USD.CC\": \"ARB11841-USD.CC\",         \"UNI-USD.CC\": \"UNI7083-USD.CC\",         \"OMNI-USD.CC\": \"OMNI30315-USD.CC\",         \"SOPH-USD.CC\": \"SOPHON-USD.CC\",         \"NEIROETH-USD.CC\": \"NEIRO-USD.CC\",         \"APT-USD.CC\": \"APT21794-USD.CC\",         \"STRK-USD.CC\": \"STRK22691-USD.CC\",         \"RESOLV-USD.CC\": \"RESOLV-USD.CC\",         \"TST-USD.CC\": \"TST35647-USD.CC\",         \"PUMP-USD.CC\": \"PUMP29601-USD.CC\",         \"WLFI-USD.CC\": \"WLFI33251-USD.CC\",         \"ASTER-USD.CC\": \"ASTER36341-USD.CC\",         \"SKY-USD.CC\": \"SKY33038-USD.CC\",     }     response = requests.post(url, json=payload)     response.raise_for_status()      data = response.json()     universe = data.get(\"universe\", [])      # Extract names of perpetuals based on delisted status     perpetual_ids = [         perp[\"name\"]         for perp in universe         if include_delisted or not perp.get(\"isDelisted\", False)     ]      # Create DataFrame with just id column     df = pl.DataFrame({\"id\": perpetual_ids})     df = df.with_columns(         (pl.col(\"id\") + \"-USD.CC\")         .str.replace(\"k\", \"\")         .replace(special_map_dict)         .alias(\"eodhd_id\")     )       return df           # Get the Hyperliquid universe perpetuals_df = get_hyperliquid_perpetuals(include_delisted=True) print(perpetuals_df) # Extract ticker lists eodhd_tickers = perpetuals_df[\"eodhd_id\"].to_list() id_mapping = dict(perpetuals_df.select(\"eodhd_id\", \"id\").iter_rows())  print(f\"\\n\ud83d\udcca Dataset will include {len(eodhd_tickers)} cryptocurrencies\") print(f\"Sample tickers: {eodhd_tickers[:10]}\") print(f\"Sample IDs: {list(id_mapping.values())[:10]}\") <pre>shape: (216, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id    \u2506 eodhd_id     \u2502\n\u2502 ---   \u2506 ---          \u2502\n\u2502 str   \u2506 str          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 BTC   \u2506 BTC-USD.CC   \u2502\n\u2502 ETH   \u2506 ETH-USD.CC   \u2502\n\u2502 ATOM  \u2506 ATOM-USD.CC  \u2502\n\u2502 MATIC \u2506 MATIC-USD.CC \u2502\n\u2502 DYDX  \u2506 DYDX-USD.CC  \u2502\n\u2502 \u2026     \u2506 \u2026            \u2502\n\u2502 HEMI  \u2506 HEMI-USD.CC  \u2502\n\u2502 APEX  \u2506 APEX-USD.CC  \u2502\n\u2502 2Z    \u2506 2Z-USD.CC    \u2502\n\u2502 ZEC   \u2506 ZEC-USD.CC   \u2502\n\u2502 MON   \u2506 MON-USD.CC   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udcca Dataset will include 216 cryptocurrencies\nSample tickers: ['BTC-USD.CC', 'ETH-USD.CC', 'ATOM-USD.CC', 'MATIC-USD.CC', 'DYDX-USD.CC', 'SOL-USD.CC', 'AVAX-USD.CC', 'BNB-USD.CC', 'APE3-USD.CC', 'OP-USD.CC']\nSample IDs: ['BTC', 'ETH', 'ATOM', 'MATIC', 'DYDX', 'SOL', 'AVAX', 'BNB', 'APE', 'OP']\n</pre> In\u00a0[4]: Copied! <pre>from numerblox.download import EODDownloader\n\n# Set date range\nstart_date = \"20200101\"\nend_date = datetime.now()\n\n# Initialize EOD downloader\neod = EODDownloader(\n    directory_path=\"data\",\n    key=EOD_API_KEY,\n    tickers=eodhd_tickers\n)\neod.end_date = end_date\n\nprint(\"Downloading historical data...\")\nprint(\"This may take a few minutes depending on number of tickers\")\n\n# Download data\neod.download_training_data(start=start_date)\n\n# Load the downloaded data\nfilename = f\"data/eod_{start_date}_{end_date.strftime('%Y%m%d')}.parquet\"\neod_df = pl.read_parquet(filename)\neod_df = eod_df.with_columns(pl.col(\"date\").str.to_datetime())\n\n# Add clean ID column using Hyperliquid naming\neod_df = eod_df.with_columns(pl.col(\"ticker\").replace(id_mapping).alias(\"id\"))\n\n# Check coverage\nrequested_tickers = len(eodhd_tickers)\ndownloaded_tickers = eod_df[\"ticker\"].n_unique()\ncoverage_pct = (downloaded_tickers / requested_tickers) * 100\n\nprint(f\"\u2705 Downloaded {len(eod_df)} rows for {downloaded_tickers} tickers\")\nprint(f\"\ud83d\udcca Coverage: {downloaded_tickers}/{requested_tickers} tickers ({coverage_pct:.1f}%)\")\nprint(f\"\ud83d\udcc5 Date range: {eod_df['date'].min()} to {eod_df['date'].max()}\")\n\neod_df.head()\n</pre> from numerblox.download import EODDownloader  # Set date range start_date = \"20200101\" end_date = datetime.now()  # Initialize EOD downloader eod = EODDownloader(     directory_path=\"data\",     key=EOD_API_KEY,     tickers=eodhd_tickers ) eod.end_date = end_date  print(\"Downloading historical data...\") print(\"This may take a few minutes depending on number of tickers\")  # Download data eod.download_training_data(start=start_date)  # Load the downloaded data filename = f\"data/eod_{start_date}_{end_date.strftime('%Y%m%d')}.parquet\" eod_df = pl.read_parquet(filename) eod_df = eod_df.with_columns(pl.col(\"date\").str.to_datetime())  # Add clean ID column using Hyperliquid naming eod_df = eod_df.with_columns(pl.col(\"ticker\").replace(id_mapping).alias(\"id\"))  # Check coverage requested_tickers = len(eodhd_tickers) downloaded_tickers = eod_df[\"ticker\"].n_unique() coverage_pct = (downloaded_tickers / requested_tickers) * 100  print(f\"\u2705 Downloaded {len(eod_df)} rows for {downloaded_tickers} tickers\") print(f\"\ud83d\udcca Coverage: {downloaded_tickers}/{requested_tickers} tickers ({coverage_pct:.1f}%)\") print(f\"\ud83d\udcc5 Date range: {eod_df['date'].min()} to {eod_df['date'].max()}\")  eod_df.head() <pre>Downloading historical data...\nThis may take a few minutes depending on number of tickers\n</pre> <pre>EOD price data extraction:   0%|          | 0/216 [00:00&lt;?, ?it/s]</pre> <pre>WARNING: Date pull failed on ticker: 'HPOS-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/HPOS-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'FRIEND-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/FRIEND-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'OX-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/OX-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'SHIA-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/SHIA-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'CANTO-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/CANTO-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'NFTI-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/NFTI-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'ALT29073-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/PANDORA-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101WARNING: Date pull failed on ticker: 'PANDORA-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/PANDORA-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\n\nWARNING: Date pull failed on ticker: 'AI-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/AI-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'TST35647-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/JELLY-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: 'APEX-USD.CC'. Exception: \"None of ['date'] are in the columns\"\nWARNING: Date pull failed on ticker: 'HEMI-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/HEMI-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\nWARNING: Date pull failed on ticker: '2Z-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/2Z-USD.CC?period=d&amp;to=2025-10-08+18%3A02%3A25.499812&amp;fmt=json&amp;api_token=YOUR_API_KEY&amp;from=20200101\n\u2705 Downloaded 219879 rows for 203 tickers\n\ud83d\udcca Coverage: 203/216 tickers (94.0%)\n\ud83d\udcc5 Date range: 2020-01-01 00:00:00 to 2025-10-08 00:00:00\n</pre> Out[4]: shape: (5, 9)openhighlowcloseadjusted_closevolumetickerdateidf64f64f64f64f64i64strdatetime[\u03bcs]str1.7969911.7969911.1044771.3310821.3310822314949442\"ARB11841-USD.CC\"2023-03-23 00:00:00\"ARB\"1.3253961.5558721.186061.2724921.2724922537709581\"ARB11841-USD.CC\"2023-03-24 00:00:00\"ARB\"1.2723931.3072321.192971.2247051.2247051294894243\"ARB11841-USD.CC\"2023-03-25 00:00:00\"ARB\"1.2241171.3418531.2080921.2833151.2833151059587959\"ARB11841-USD.CC\"2023-03-26 00:00:00\"ARB\"1.2825211.3202751.1244951.1627051.1627051014240603\"ARB11841-USD.CC\"2023-03-27 00:00:00\"ARB\" In\u00a0[5]: Copied! <pre>import altair as alt\nalt.data_transformers.enable('vegafusion')\n\n# Select top tickers by data coverage for visualization\ntop_tickers = (\n    eod_df.group_by(\"id\")\n    .agg(pl.col(\"date\").count().alias(\"count\"))\n    .sort(\"count\", descending=True)\n    .head(20)[\"id\"]\n    .to_list()\n)\n\n# Create visualization of raw prices\nviz_df = eod_df.filter(pl.col(\"id\").is_in(top_tickers)).to_pandas()\n\nchart = (\n    alt.Chart(viz_df)\n    .mark_line(opacity=0.6)\n    .encode(\n        x=alt.X(\"date:T\", title=\"Date\"),\n        y=alt.Y(\"close:Q\", title=\"Close Price (USD)\", scale=alt.Scale(type=\"log\")),\n        color=alt.Color(\"id:N\", title=\"Ticker\", legend=alt.Legend(columns=2)),\n        tooltip=[\"id:N\", \"date:T\", \"close:Q\"]\n    )\n    .properties(\n        width=700,\n        height=300,\n        title=\"Input: Raw Stock Prices Over Time (Log Scale)\"\n    )\n)\n\nchart\n</pre> import altair as alt alt.data_transformers.enable('vegafusion')  # Select top tickers by data coverage for visualization top_tickers = (     eod_df.group_by(\"id\")     .agg(pl.col(\"date\").count().alias(\"count\"))     .sort(\"count\", descending=True)     .head(20)[\"id\"]     .to_list() )  # Create visualization of raw prices viz_df = eod_df.filter(pl.col(\"id\").is_in(top_tickers)).to_pandas()  chart = (     alt.Chart(viz_df)     .mark_line(opacity=0.6)     .encode(         x=alt.X(\"date:T\", title=\"Date\"),         y=alt.Y(\"close:Q\", title=\"Close Price (USD)\", scale=alt.Scale(type=\"log\")),         color=alt.Color(\"id:N\", title=\"Ticker\", legend=alt.Legend(columns=2)),         tooltip=[\"id:N\", \"date:T\", \"close:Q\"]     )     .properties(         width=700,         height=300,         title=\"Input: Raw Stock Prices Over Time (Log Scale)\"     ) )  chart  Out[5]: In\u00a0[6]: Copied! <pre>from crowdcent_challenge.scoring import create_ranking_targets\n\n# Create standard 10d and 30d ranking targets using CrowdCent's official function\nprint(\"\ud83c\udfaf Creating ranking targets using CrowdCent's methodology...\")\n\ndf = create_ranking_targets(\n    eod_df,\n    horizons=[10, 30],  # Standard CrowdCent horizons\n    price_col=\"close\",\n    date_col=\"date\", \n    ticker_col=\"ticker\",\n    return_raw_returns=False,  # We only need the normalized targets\n    drop_incomplete=True       # Drop rows without complete targets\n)\n\nprint(f\"\u2705 Created targets: target_10d, target_30d\")\nprint(f\"\ud83d\udcca Rows with complete targets: {len(df):,}\")\n\n# Show sample targets\nprint(\"\\nSample targets:\")\ndf.select([\"date\", \"ticker\", \"id\", \"target_10d\", \"target_30d\"]).tail(10)\n</pre> from crowdcent_challenge.scoring import create_ranking_targets  # Create standard 10d and 30d ranking targets using CrowdCent's official function print(\"\ud83c\udfaf Creating ranking targets using CrowdCent's methodology...\")  df = create_ranking_targets(     eod_df,     horizons=[10, 30],  # Standard CrowdCent horizons     price_col=\"close\",     date_col=\"date\",      ticker_col=\"ticker\",     return_raw_returns=False,  # We only need the normalized targets     drop_incomplete=True       # Drop rows without complete targets )  print(f\"\u2705 Created targets: target_10d, target_30d\") print(f\"\ud83d\udcca Rows with complete targets: {len(df):,}\")  # Show sample targets print(\"\\nSample targets:\") df.select([\"date\", \"ticker\", \"id\", \"target_10d\", \"target_30d\"]).tail(10) <pre>\ud83c\udfaf Creating ranking targets using CrowdCent's methodology...\n\u2705 Created targets: target_10d, target_30d\n\ud83d\udcca Rows with complete targets: 213,841\n\nSample targets:\n</pre> Out[6]: shape: (10, 5)datetickeridtarget_10dtarget_30ddatetime[\u03bcs]strstrf64f642025-08-30 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.9740930.9637312025-08-31 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.9585490.9948192025-09-01 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.922280.9896372025-09-02 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.7823831.02025-09-03 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.8497411.02025-09-04 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.922281.02025-09-05 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.9067361.02025-09-06 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.9533681.02025-09-07 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.6113991.02025-09-08 00:00:00\"ZEC-USD.CC\"\"ZEC\"0.3782381.0 In\u00a0[7]: Copied! <pre># Sample recent data for visualization\nsample_dates = df[\"date\"].unique().sort().tail(30).to_list()\ntarget_viz_df = df.filter(pl.col(\"date\").is_in(sample_dates)).to_pandas()\n\n# Create histograms for both targets\nhist_10d = (\n    alt.Chart(target_viz_df)\n    .mark_bar(opacity=0.7)\n    .encode(\n        x=alt.X(\"target_10d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 10d Value\"),\n        y=alt.Y(\"count()\", title=\"Count\"),\n        tooltip=[\"count()\"]\n    )\n    .properties(\n        width=350,\n        height=200,\n        title=\"10-Day Target Distribution (Last 30 Days)\"\n    )\n)\n\nhist_30d = (\n    alt.Chart(target_viz_df)\n    .mark_bar(opacity=0.7, color=\"orange\")\n    .encode(\n        x=alt.X(\"target_30d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 30d Value\"),\n        y=alt.Y(\"count()\", title=\"Count\"),\n        tooltip=[\"count()\"]\n    )\n    .properties(\n        width=350,\n        height=200,\n        title=\"30-Day Target Distribution (Last 30 Days)\"\n    )\n)\n\nalt.hconcat(hist_10d, hist_30d)\n</pre> # Sample recent data for visualization sample_dates = df[\"date\"].unique().sort().tail(30).to_list() target_viz_df = df.filter(pl.col(\"date\").is_in(sample_dates)).to_pandas()  # Create histograms for both targets hist_10d = (     alt.Chart(target_viz_df)     .mark_bar(opacity=0.7)     .encode(         x=alt.X(\"target_10d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 10d Value\"),         y=alt.Y(\"count()\", title=\"Count\"),         tooltip=[\"count()\"]     )     .properties(         width=350,         height=200,         title=\"10-Day Target Distribution (Last 30 Days)\"     ) )  hist_30d = (     alt.Chart(target_viz_df)     .mark_bar(opacity=0.7, color=\"orange\")     .encode(         x=alt.X(\"target_30d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 30d Value\"),         y=alt.Y(\"count()\", title=\"Count\"),         tooltip=[\"count()\"]     )     .properties(         width=350,         height=200,         title=\"30-Day Target Distribution (Last 30 Days)\"     ) )  alt.hconcat(hist_10d, hist_30d)  Out[7]: In\u00a0[8]: Copied! <pre>from sklearn import set_config\nfrom sklearn.pipeline import make_pipeline\nfrom centimators.feature_transformers import (\n    LogReturnTransformer,\n    RankTransformer,\n    LagTransformer,\n    MovingAverageTransformer,\n)\n\n# Enable metadata routing for sklearn\nset_config(enable_metadata_routing=True)\n\nprint(\"\ud83d\udd27 Building feature engineering pipeline...\")\n\n# Define transformers with custom parameters\nlog_return_transformer = LogReturnTransformer().set_transform_request(ticker_series=True)\nranker = RankTransformer().set_transform_request(date_series=True)\nma_transformer = MovingAverageTransformer(\n    windows=[2, 10]  # Custom moving average windows\n).set_transform_request(ticker_series=True)\nlagger = LagTransformer(windows=[0, 5, 10, 15, 20]).set_transform_request(ticker_series=True)  # Custom lag windows\n\n# Create feature pipeline\nfeature_pipeline = make_pipeline(\n    log_return_transformer,\n    ranker,\n    ma_transformer,\n    lagger,\n    verbose=True\n)\n\nfeature_pipeline\n</pre> from sklearn import set_config from sklearn.pipeline import make_pipeline from centimators.feature_transformers import (     LogReturnTransformer,     RankTransformer,     LagTransformer,     MovingAverageTransformer, )  # Enable metadata routing for sklearn set_config(enable_metadata_routing=True)  print(\"\ud83d\udd27 Building feature engineering pipeline...\")  # Define transformers with custom parameters log_return_transformer = LogReturnTransformer().set_transform_request(ticker_series=True) ranker = RankTransformer().set_transform_request(date_series=True) ma_transformer = MovingAverageTransformer(     windows=[2, 10]  # Custom moving average windows ).set_transform_request(ticker_series=True) lagger = LagTransformer(windows=[0, 5, 10, 15, 20]).set_transform_request(ticker_series=True)  # Custom lag windows  # Create feature pipeline feature_pipeline = make_pipeline(     log_return_transformer,     ranker,     ma_transformer,     lagger,     verbose=True )  feature_pipeline <pre>\ud83d\udd27 Building feature engineering pipeline...\n</pre> Out[8]: <pre>Pipeline(steps=[('logreturntransformer', LogReturnTransformer()),\n                ('ranktransformer', RankTransformer()),\n                ('movingaveragetransformer',\n                 MovingAverageTransformer(windows=[2, 10])),\n                ('lagtransformer', LagTransformer(windows=[20, 15, 10, 5, 0]))],\n         verbose=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted Parameters steps\u00a0 [('logreturntransformer', ...), ('ranktransformer', ...), ...] transform_input\u00a0 None memory\u00a0 None verbose\u00a0 True LogReturnTransformer Parameters feature_names\u00a0 None RankTransformer Parameters feature_names\u00a0 None MovingAverageTransformer Parameters windows\u00a0 [2, 10] feature_names\u00a0 None LagTransformer Parameters windows\u00a0 [20, 15, ...] feature_names\u00a0 None In\u00a0[9]: Copied! <pre># Apply feature engineering\ninput_features = [\"close\", \"open\", \"high\", \"low\", \"volume\"]\n\nprint(f\"Transforming {input_features} into features...\")\n\nfeature_df = feature_pipeline.fit_transform(\n    df[input_features],\n    date_series=df[\"date\"],\n    ticker_series=df[\"ticker\"]\n)\n\n# Get feature names and add to dataframe\nfeature_names = feature_pipeline.get_feature_names_out()\ndf = df.with_columns(feature_df).drop_nulls(subset=feature_names)\n\nprint(f\"\\n\u2705 Created {len(feature_names)} features\")\nprint(f\"Final dataset shape: {df.shape}\")\nprint(f\"\\nFeature names:\")\nfor i, name in enumerate(feature_names[:10]):\n    print(f\"  {name}\")\nif len(feature_names) &gt; 10:\n    print(f\"  ... and {len(feature_names) - 10} more\")\n\ndf.head()\n</pre> # Apply feature engineering input_features = [\"close\", \"open\", \"high\", \"low\", \"volume\"]  print(f\"Transforming {input_features} into features...\")  feature_df = feature_pipeline.fit_transform(     df[input_features],     date_series=df[\"date\"],     ticker_series=df[\"ticker\"] )  # Get feature names and add to dataframe feature_names = feature_pipeline.get_feature_names_out() df = df.with_columns(feature_df).drop_nulls(subset=feature_names)  print(f\"\\n\u2705 Created {len(feature_names)} features\") print(f\"Final dataset shape: {df.shape}\") print(f\"\\nFeature names:\") for i, name in enumerate(feature_names[:10]):     print(f\"  {name}\") if len(feature_names) &gt; 10:     print(f\"  ... and {len(feature_names) - 10} more\")  df.head() <pre>Transforming ['close', 'open', 'high', 'low', 'volume'] into features...\n[Pipeline]  (step 1 of 4) Processing logreturntransformer, total=   0.0s\n[Pipeline] ... (step 2 of 4) Processing ranktransformer, total=   0.0s\n[Pipeline]  (step 3 of 4) Processing movingaveragetransformer, total=   0.0s\n[Pipeline] .... (step 4 of 4) Processing lagtransformer, total=   0.0s\n\n\u2705 Created 50 features\nFinal dataset shape: (207972, 61)\n\nFeature names:\n  close_logreturn_rank_ma2_lag20\n  close_logreturn_rank_ma10_lag20\n  open_logreturn_rank_ma2_lag20\n  open_logreturn_rank_ma10_lag20\n  high_logreturn_rank_ma2_lag20\n  high_logreturn_rank_ma10_lag20\n  low_logreturn_rank_ma2_lag20\n  low_logreturn_rank_ma10_lag20\n  volume_logreturn_rank_ma2_lag20\n  volume_logreturn_rank_ma10_lag20\n  ... and 40 more\n</pre> Out[9]: shape: (5, 61)openhighlowcloseadjusted_closevolumetickerdateidtarget_10dtarget_30dclose_logreturn_rank_ma2_lag20close_logreturn_rank_ma10_lag20open_logreturn_rank_ma2_lag20open_logreturn_rank_ma10_lag20high_logreturn_rank_ma2_lag20high_logreturn_rank_ma10_lag20low_logreturn_rank_ma2_lag20low_logreturn_rank_ma10_lag20volume_logreturn_rank_ma2_lag20volume_logreturn_rank_ma10_lag20close_logreturn_rank_ma2_lag15close_logreturn_rank_ma10_lag15open_logreturn_rank_ma2_lag15open_logreturn_rank_ma10_lag15high_logreturn_rank_ma2_lag15high_logreturn_rank_ma10_lag15low_logreturn_rank_ma2_lag15low_logreturn_rank_ma10_lag15volume_logreturn_rank_ma2_lag15volume_logreturn_rank_ma10_lag15close_logreturn_rank_ma2_lag10close_logreturn_rank_ma10_lag10open_logreturn_rank_ma2_lag10open_logreturn_rank_ma10_lag10high_logreturn_rank_ma2_lag10high_logreturn_rank_ma10_lag10low_logreturn_rank_ma2_lag10low_logreturn_rank_ma10_lag10volume_logreturn_rank_ma2_lag10volume_logreturn_rank_ma10_lag10close_logreturn_rank_ma2_lag5close_logreturn_rank_ma10_lag5open_logreturn_rank_ma2_lag5open_logreturn_rank_ma10_lag5high_logreturn_rank_ma2_lag5high_logreturn_rank_ma10_lag5low_logreturn_rank_ma2_lag5low_logreturn_rank_ma10_lag5volume_logreturn_rank_ma2_lag5volume_logreturn_rank_ma10_lag5close_logreturn_rank_ma2_lag0close_logreturn_rank_ma10_lag0open_logreturn_rank_ma2_lag0open_logreturn_rank_ma10_lag0high_logreturn_rank_ma2_lag0high_logreturn_rank_ma10_lag0low_logreturn_rank_ma2_lag0low_logreturn_rank_ma10_lag0volume_logreturn_rank_ma2_lag0volume_logreturn_rank_ma10_lag0f64f64f64f64f64i64strdatetime[\u03bcs]strf64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f641.4637751.4638931.3160341.3392421.339242624706911\"ARB11841-USD.CC\"2023-04-23 00:00:00\"ARB\"0.144330.1855670.0631580.3515790.0263160.3915790.0526320.3505260.1105260.4831580.5368420.3642110.2684210.3610530.3421050.3589470.4736840.4336840.4210530.4663160.4263160.4157890.7263160.540.2526320.4315790.6105260.5157890.3473680.540.8052630.4726320.4896910.6137280.432990.5171240.7525770.6010420.5721650.5881170.5463920.561910.0463920.4430820.5257730.538340.4845360.5254260.4587630.5568310.4175260.4690181.3391981.3616571.2937391.3383441.338344645855049\"ARB11841-USD.CC\"2023-04-24 00:00:00\"ARB\"0.6082470.4742270.4684210.4168420.0578950.3494740.4157890.4263160.50.480.2578950.3926320.4473680.3421050.2578950.3568420.2473680.3705260.3263160.4515790.4052630.3694740.9947370.5547370.6368420.5210530.9947370.5378950.7315790.560.9736840.5284210.5206190.5512750.4896910.5952790.4793810.5715030.3505150.5591540.5154640.5227670.2938140.3997830.0463920.4425070.0463920.4306020.1030930.4743570.2835050.4072491.3382971.3810481.3004331.3795541.379554636337759\"ARB11841-USD.CC\"2023-04-25 00:00:00\"ARB\"0.5876290.3608250.6684210.380.4684210.4147370.6368420.3884210.8315790.4873680.3421050.3915790.80.3905260.4473680.3389470.5842110.3831580.5789470.4136840.6315790.4410530.5578950.5178950.9894740.5357890.8684210.5621050.9947370.5726320.6473680.5336840.2422680.5021810.5257730.5328050.1855670.5213130.2525770.5228430.3762890.5108520.6855670.4686160.2938140.3992080.4381440.4393710.5309280.4640690.3814430.4158331.3791771.4946461.3412121.3846581.3846581232822646\"ARB11841-USD.CC\"2023-04-26 00:00:00\"ARB\"0.5463920.3505150.3736840.4021050.6684210.3778950.3842110.3936840.5421050.5021050.2105260.3915790.5105260.4073680.8052630.3863160.5631580.4042110.7157890.4926320.4789470.4189470.4473680.5694740.5578950.4989470.4210530.5452630.5315790.5578950.2578950.5378950.6494850.5790670.2474230.4837110.4587630.5506240.5927840.5345520.5979380.5465650.8402060.4783510.6804120.467010.8814430.522680.8917530.5463920.6340210.4824741.3847431.4488561.3823421.4229341.422934747265158\"ARB11841-USD.CC\"2023-04-27 00:00:00\"ARB\"0.2164950.422680.3473680.3621050.3736840.40.5210530.4684210.4789470.5431580.3315790.4273680.3368420.4494740.5105260.4052630.2526320.4221050.5263160.5136840.40.3852630.441020.5366250.4393380.548920.3360280.52510.3873580.5543140.1667930.5007270.5257730.5399670.6494850.5605970.7474230.6202710.9123710.6000540.5721650.5452850.7835050.5371130.8350520.4783510.6237110.4969070.6237110.511340.5103090.484536 In\u00a0[10]: Copied! <pre># Select a few representative features to visualize\nsample_features = [\n    \"close_logreturn_rank_ma10_lag0\",  # Most recent smoothed close feature\n    \"volume_logreturn_rank_ma10_lag0\",  # Most recent smoothed volume feature\n]\n\n# Filter to top tickers and recent dates for cleaner visualization\nrecent_dates = df[\"date\"].unique().sort().tail(100).to_list()\nfeature_viz_df = (\n    df.filter(pl.col(\"id\").is_in(top_tickers[:10]))\n    .filter(pl.col(\"date\").is_in(recent_dates))\n    .select([\"id\", \"date\"] + sample_features)\n    .to_pandas()\n)\n\n# Melt for plotting multiple features\nfeature_viz_melted = feature_viz_df.melt(\n    id_vars=[\"id\", \"date\"],\n    value_vars=sample_features,\n    var_name=\"feature\",\n    value_name=\"value\"\n)\n\n# Create feature visualization\nfeature_chart = (\n    alt.Chart(feature_viz_melted)\n    .mark_line(opacity=0.6)\n    .encode(\n        x=alt.X(\"date:T\", title=\"Date\"),\n        y=alt.Y(\"value:Q\", title=\"Normalized Feature Value [0, 1]\", scale=alt.Scale(domain=[0, 1])),\n        color=alt.Color(\"id:N\", title=\"Ticker\"),\n        strokeDash=alt.StrokeDash(\"feature:N\", title=\"Feature Type\"),\n        tooltip=[\"id:N\", \"date:T\", \"feature:N\", \"value:Q\"]\n    )\n    .properties(\n        width=700,\n        height=300,\n        title=\"Pipeline Output: Normalized/Smoothed Features (Recent 100 Days)\"\n    )\n)\n\nfeature_chart\n</pre> # Select a few representative features to visualize sample_features = [     \"close_logreturn_rank_ma10_lag0\",  # Most recent smoothed close feature     \"volume_logreturn_rank_ma10_lag0\",  # Most recent smoothed volume feature ]  # Filter to top tickers and recent dates for cleaner visualization recent_dates = df[\"date\"].unique().sort().tail(100).to_list() feature_viz_df = (     df.filter(pl.col(\"id\").is_in(top_tickers[:10]))     .filter(pl.col(\"date\").is_in(recent_dates))     .select([\"id\", \"date\"] + sample_features)     .to_pandas() )  # Melt for plotting multiple features feature_viz_melted = feature_viz_df.melt(     id_vars=[\"id\", \"date\"],     value_vars=sample_features,     var_name=\"feature\",     value_name=\"value\" )  # Create feature visualization feature_chart = (     alt.Chart(feature_viz_melted)     .mark_line(opacity=0.6)     .encode(         x=alt.X(\"date:T\", title=\"Date\"),         y=alt.Y(\"value:Q\", title=\"Normalized Feature Value [0, 1]\", scale=alt.Scale(domain=[0, 1])),         color=alt.Color(\"id:N\", title=\"Ticker\"),         strokeDash=alt.StrokeDash(\"feature:N\", title=\"Feature Type\"),         tooltip=[\"id:N\", \"date:T\", \"feature:N\", \"value:Q\"]     )     .properties(         width=700,         height=300,         title=\"Pipeline Output: Normalized/Smoothed Features (Recent 100 Days)\"     ) )  feature_chart  Out[10]: <p>What changed after the pipeline:</p> <ul> <li>All features now range from 0 to 1 (normalized through ranking)</li> <li>Different price levels no longer matter - we're comparing relative performance</li> <li>Features are smoothed (moving averages reduce noise)</li> <li>The data is now ready for machine learning models to find patterns</li> </ul> <p>This transformation is crucial for cross-sectional ranking models where we predict which assets will outperform others, not their absolute price movements.</p> In\u00a0[11]: Copied! <pre># Create final dataset structure\nfinal_df = (\n    df.rename({\"ticker\": \"eodhd_id\"})\n    .select([\n        \"id\",\n        \"eodhd_id\", \n        \"date\"\n    ] + list(feature_names) + [\n        \"target_10d\",\n        \"target_30d\"\n    ])\n)\n\nprint(f\"\ud83d\udcca Final Dataset Summary:\")\nprint(f\"Shape: {final_df.shape}\")\nprint(f\"Date range: {final_df['date'].min()} to {final_df['date'].max()}\")\nprint(f\"Cryptocurrencies: {final_df['id'].n_unique()}\")\nprint(f\"Features: {len(feature_names)}\")\nprint(f\"Targets: 2 (target_10d, target_30d)\")\n\n# Show sample with first few features\nsample_cols = [\"id\", \"date\"] + list(feature_names)[:3] + [\"target_10d\", \"target_30d\"]\nprint(f\"\\nSample data:\")\nfinal_df.select(sample_cols).head()\n</pre> # Create final dataset structure final_df = (     df.rename({\"ticker\": \"eodhd_id\"})     .select([         \"id\",         \"eodhd_id\",          \"date\"     ] + list(feature_names) + [         \"target_10d\",         \"target_30d\"     ]) )  print(f\"\ud83d\udcca Final Dataset Summary:\") print(f\"Shape: {final_df.shape}\") print(f\"Date range: {final_df['date'].min()} to {final_df['date'].max()}\") print(f\"Cryptocurrencies: {final_df['id'].n_unique()}\") print(f\"Features: {len(feature_names)}\") print(f\"Targets: 2 (target_10d, target_30d)\")  # Show sample with first few features sample_cols = [\"id\", \"date\"] + list(feature_names)[:3] + [\"target_10d\", \"target_30d\"] print(f\"\\nSample data:\") final_df.select(sample_cols).head() <pre>\ud83d\udcca Final Dataset Summary:\nShape: (207972, 55)\nDate range: 2020-02-01 00:00:00 to 2025-09-09 00:00:00\nCryptocurrencies: 195\nFeatures: 50\nTargets: 2 (target_10d, target_30d)\n\nSample data:\n</pre> Out[11]: shape: (5, 7)iddateclose_logreturn_rank_ma2_lag20close_logreturn_rank_ma10_lag20open_logreturn_rank_ma2_lag20target_10dtarget_30dstrdatetime[\u03bcs]f64f64f64f64f64\"ARB\"2023-04-23 00:00:000.0631580.3515790.0263160.144330.185567\"ARB\"2023-04-24 00:00:000.4684210.4168420.0578950.6082470.474227\"ARB\"2023-04-25 00:00:000.6684210.380.4684210.5876290.360825\"ARB\"2023-04-26 00:00:000.3736840.4021050.6684210.5463920.350515\"ARB\"2023-04-27 00:00:000.3473680.3621050.3736840.2164950.42268 In\u00a0[12]: Copied! <pre># Save to parquet\noutput_path = \"custom_crypto_dataset.parquet\"\nfinal_df.write_parquet(output_path)\n\n# Get file size, why not?\nfile_size_mb = os.path.getsize(output_path) / 1024 / 1024\n\nprint(f\"\u2705 Dataset saved to: {output_path}\")\nprint(f\"File size: {file_size_mb:.2f} MB\")\nprint(f\"Rows: {len(final_df):,}\")\nprint(f\"\\nYou can now use this dataset for ML experiments!\")\n</pre> # Save to parquet output_path = \"custom_crypto_dataset.parquet\" final_df.write_parquet(output_path)  # Get file size, why not? file_size_mb = os.path.getsize(output_path) / 1024 / 1024  print(f\"\u2705 Dataset saved to: {output_path}\") print(f\"File size: {file_size_mb:.2f} MB\") print(f\"Rows: {len(final_df):,}\") print(f\"\\nYou can now use this dataset for ML experiments!\") <pre>\u2705 Dataset saved to: custom_crypto_dataset.parquet\nFile size: 49.20 MB\nRows: 207,972\n\nYou can now use this dataset for ML experiments!\n</pre> In\u00a0[13]: Copied! <pre># Prepare features\nfeature_cols = list(feature_names)\nprint(f\"Feature columns: {len(feature_cols)} features\")\n\n# Time-based split with embargo period\nembargo_days = 30\nsorted_dates = final_df[\"date\"].unique().sort()\nsplit_idx = int(len(sorted_dates) * 0.8)\nsplit_date = sorted_dates[split_idx]\nembargo_end = split_date + pl.duration(days=embargo_days)\n\nprint(f\"Split date: {split_date}\")\nprint(f\"Embargo: {embargo_days} days\")\n\ntrain_df = final_df.filter(pl.col(\"date\") &lt; split_date)\ntest_df = final_df.filter(pl.col(\"date\") &gt; embargo_end)\n\nprint(f\"Train period: {train_df['date'].min()} to {train_df['date'].max()} ({len(train_df)} rows)\")\nprint(f\"Test period: {test_df['date'].min()} to {test_df['date'].max()} ({len(test_df)} rows)\")\n</pre> # Prepare features feature_cols = list(feature_names) print(f\"Feature columns: {len(feature_cols)} features\")  # Time-based split with embargo period embargo_days = 30 sorted_dates = final_df[\"date\"].unique().sort() split_idx = int(len(sorted_dates) * 0.8) split_date = sorted_dates[split_idx] embargo_end = split_date + pl.duration(days=embargo_days)  print(f\"Split date: {split_date}\") print(f\"Embargo: {embargo_days} days\")  train_df = final_df.filter(pl.col(\"date\") &lt; split_date) test_df = final_df.filter(pl.col(\"date\") &gt; embargo_end)  print(f\"Train period: {train_df['date'].min()} to {train_df['date'].max()} ({len(train_df)} rows)\") print(f\"Test period: {test_df['date'].min()} to {test_df['date'].max()} ({len(test_df)} rows)\") <pre>Feature columns: 50 features\nSplit date: 2024-07-27 00:00:00\nEmbargo: 30 days\nTrain period: 2020-02-01 00:00:00 to 2024-07-26 00:00:00 (138679 rows)\nTest period: 2024-08-27 00:00:00 to 2025-09-09 00:00:00 (64829 rows)\n</pre> In\u00a0[14]: Copied! <pre>from xgboost import XGBRegressor\nfrom crowdcent_challenge.scoring import evaluate_hyperliquid_submission\n\n# Train simple, untuned multi-output XGBoost model for both 10d and 30d targets\nmodel = XGBRegressor(n_estimators=500, random_state=42, verbosity=0, device=\"cuda\")\n\nX_train = train_df[feature_cols].to_pandas()\ny_train = train_df[[\"target_10d\", \"target_30d\"]].to_pandas()\n\nprint(f\"Training on {X_train.shape} features, {y_train.shape} targets\")\nprint(\"This may take a few minutes depending on number of features, model parameters, GPU, etc...\")\nmodel.fit(X_train, y_train)\n\n# Make predictions for both horizons\nX_test = test_df[feature_cols].to_pandas()\ntest_preds = model.predict(X_test)\n\n# Extract predictions for both horizons and create predictions dataframe\ntest_results = test_df.select([\"date\", \"id\", \"target_10d\", \"target_30d\"]).with_columns([\n    pl.Series(\"pred_10d\", test_preds[:, 0]),\n    pl.Series(\"pred_30d\", test_preds[:, 1])\n])\n\n# Evaluate per date (cross-sectionally) then average\ndaily_scores = (\n    test_results\n    .group_by(\"date\")\n    .agg([\n        pl.col(\"target_10d\"),\n        pl.col(\"pred_10d\"),\n        pl.col(\"target_30d\"),\n        pl.col(\"pred_30d\")\n    ])\n    .with_columns(\n        pl.struct([\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"])\n        .map_elements(\n            lambda x: evaluate_hyperliquid_submission(\n                y_true_10d=x[\"target_10d\"],\n                y_pred_10d=x[\"pred_10d\"],\n                y_true_30d=x[\"target_30d\"],\n                y_pred_30d=x[\"pred_30d\"],\n            ),\n            return_dtype=pl.Struct,\n        )\n        .alias(\"metrics\")\n    )\n    .unnest(\"metrics\")\n    .sort(\"date\")\n)\n\n\n# Average scores across all test dates\nmetrics = [\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"]\navg_scores = daily_scores.select([pl.col(metric).mean() for metric in metrics]).to_dicts()[0]\n\nprint(f\"\\n\ud83d\udcc8 Model Validation Results (CrowdCent Official Metrics):\")\nprint(f\"Spearman Correlation 10d: {avg_scores['spearman_10d']:.4f}\")\nprint(f\"Spearman Correlation 30d: {avg_scores['spearman_30d']:.4f}\")\nprint(f\"NDCG@40 10d: {avg_scores['ndcg@40_10d']:.4f}\")\nprint(f\"NDCG@40 30d: {avg_scores['ndcg@40_30d']:.4f}\")\n\nprint(f\"\\n\ud83c\udfaf Your custom dataset is ready for advanced modeling!\")\n</pre> from xgboost import XGBRegressor from crowdcent_challenge.scoring import evaluate_hyperliquid_submission  # Train simple, untuned multi-output XGBoost model for both 10d and 30d targets model = XGBRegressor(n_estimators=500, random_state=42, verbosity=0, device=\"cuda\")  X_train = train_df[feature_cols].to_pandas() y_train = train_df[[\"target_10d\", \"target_30d\"]].to_pandas()  print(f\"Training on {X_train.shape} features, {y_train.shape} targets\") print(\"This may take a few minutes depending on number of features, model parameters, GPU, etc...\") model.fit(X_train, y_train)  # Make predictions for both horizons X_test = test_df[feature_cols].to_pandas() test_preds = model.predict(X_test)  # Extract predictions for both horizons and create predictions dataframe test_results = test_df.select([\"date\", \"id\", \"target_10d\", \"target_30d\"]).with_columns([     pl.Series(\"pred_10d\", test_preds[:, 0]),     pl.Series(\"pred_30d\", test_preds[:, 1]) ])  # Evaluate per date (cross-sectionally) then average daily_scores = (     test_results     .group_by(\"date\")     .agg([         pl.col(\"target_10d\"),         pl.col(\"pred_10d\"),         pl.col(\"target_30d\"),         pl.col(\"pred_30d\")     ])     .with_columns(         pl.struct([\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"])         .map_elements(             lambda x: evaluate_hyperliquid_submission(                 y_true_10d=x[\"target_10d\"],                 y_pred_10d=x[\"pred_10d\"],                 y_true_30d=x[\"target_30d\"],                 y_pred_30d=x[\"pred_30d\"],             ),             return_dtype=pl.Struct,         )         .alias(\"metrics\")     )     .unnest(\"metrics\")     .sort(\"date\") )   # Average scores across all test dates metrics = [\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"] avg_scores = daily_scores.select([pl.col(metric).mean() for metric in metrics]).to_dicts()[0]  print(f\"\\n\ud83d\udcc8 Model Validation Results (CrowdCent Official Metrics):\") print(f\"Spearman Correlation 10d: {avg_scores['spearman_10d']:.4f}\") print(f\"Spearman Correlation 30d: {avg_scores['spearman_30d']:.4f}\") print(f\"NDCG@40 10d: {avg_scores['ndcg@40_10d']:.4f}\") print(f\"NDCG@40 30d: {avg_scores['ndcg@40_30d']:.4f}\")  print(f\"\\n\ud83c\udfaf Your custom dataset is ready for advanced modeling!\") <pre>Training on (138679, 50) features, (138679, 2) targets\nThis may take a few minutes depending on number of features, model parameters, GPU, etc...\n\n\ud83d\udcc8 Model Validation Results (CrowdCent Official Metrics):\nSpearman Correlation 10d: 0.1805\nSpearman Correlation 30d: 0.1027\nNDCG@40 10d: 0.6406\nNDCG@40 30d: 0.6017\n\n\ud83c\udfaf Your custom dataset is ready for advanced modeling!\n</pre>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#create-a-custom-training-dataset-with-eodhd","title":"Create a custom training dataset with EODHD\u00b6","text":"<p>Learn how to create custom crypto training datasets (features and targets) from scratch. This tutorial demonstrates the full pipeline from data download to ML-ready features and modeling.</p> <p>What you'll learn:</p> <ul> <li>Download historical crypto data with numerblox and eodhd</li> <li>Engineer cross-sectional features with centimators</li> <li>Create ranking targets for prediction</li> <li>Validate data quality</li> <li>Structure datasets for ML</li> </ul> <p>Why build custom datasets? While CrowdCent provides training data, building your own allows you to:</p> <ul> <li>Experiment with different features and time windows</li> <li>Include additional data sources</li> <li>Test hypotheses on historical data</li> <li>Develop a unique edge in predictions</li> </ul>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#get-eodhd-api-access","title":"Get EODHD API Access\u00b6","text":"<p>Special offer for CrowdCent users: Get 10% off all EODHD plans \ud83d\udc49 https://eodhd.com/pricing-special-10?via=crowdcent</p> <p>Recommended plan: EOD Historical Data - All World ($17.99/mo) which provides 100k calls/day - sufficient for most use cases</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-1-fetch-live-hyperliquid-universe","title":"Step 1: Fetch Live Hyperliquid Universe\u00b6","text":"<p>We'll get the current cryptocurrency universe directly from the Hyperliquid API. This includes both active and delisted perpetuals, giving us maximum historical data coverage.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-2-download-historical-data-with-numerblox","title":"Step 2: Download Historical Data with numerblox\u00b6","text":""},{"location":"tutorials/advanced-custom-dataset-eodhd/#visualize-raw-price-data","title":"Visualize Raw Price Data\u00b6","text":"<p>Let's look at what the raw downloaded data looks like - this is the \"Input\" to our feature engineering pipeline.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-3-create-ranking-targets-with-crowdcent-library","title":"Step 3: Create Ranking Targets with CrowdCent Library\u00b6","text":"<p>We'll use the <code>create_ranking_targets</code> function from crowdcent-challenge to create standard 10-day and 30-day ranking targets. This matches exactly what CrowdCent uses in the hyperliquid-ranking challenge.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#visualize-target-distributions","title":"Visualize Target Distributions\u00b6","text":"<p>The ranking targets are normalized to [0, 1] where 0 means worst performer and 1 means best performer on that day. This cross-sectional ranking approach means our model predicts relative performance, not absolute returns.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-4-engineer-features-with-centimators","title":"Step 4: Engineer features with centimators\u00b6","text":"<p>This is the step where you can and should put most of your energy. Try different feature engineering strategies, different moving average windows, lag windows, and anything else you can think of.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#visualize-pipeline-output-engineered-features","title":"Visualize Pipeline Output: Engineered Features\u00b6","text":"<p>After feature engineering, all features are normalized and cross-sectionally ranked. This makes them comparable across different tickers and time periods. Let's visualize a few features for our top tickers.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-5-structure-dataset-for-ml","title":"Step 5: Structure Dataset for ML\u00b6","text":""},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-6-save-dataset","title":"Step 6: Save Dataset\u00b6","text":""},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-7-quick-model-validation","title":"Step 7: Quick Model Validation\u00b6","text":"<p>Let's verify our dataset works with a simple train-test split and xgboost model to ensure data quality.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#next-steps","title":"Next Steps\u00b6","text":"<p>\ud83c\udf89 Congratulations! You've built a complete crypto prediction dataset. Here's what to try next, although the list is endless:</p> <p>Ready to submit predictions with your new dataset? Check out the Hyperliquid End-to-End Tutorial where you'll learn how to download inference data, make predictions, and submit to CrowdCent competitions using your custom model. You will need to replace the download data steps with your own production pipeline using your new custom dataset!</p> <p>Immediate experiments and next steps:</p> <ol> <li>Try different models: Random Forest, Neural Networks, and other model estimators from Centimators like LSTMRegressor</li> <li>Adjust time horizons: Create 3d, 14d, 50d targets</li> <li>Additional data cleaning: Check the raw data for anomolies and remove/adjust samples.</li> <li>Feature engineering: Add lags, differences, and other combinatorial feature combinations. Centimators provides additional <code>feature transformers</code> like GroupStatsTransformer and will always be adding more!</li> <li>Cross-validation: Use walk-forward, time-series CV for robust validation like in this advanced tutorial notebook.</li> <li>More data sources: Add social sentiment, onchain data, or macro indicators</li> <li>Ensemble methods: Combine multiple models and time horizons</li> <li>Real-time pipeline: Set up automated data updates and retraining</li> <li>Automate submissions: Use techniques from the submission automation guide to set it and forget it.</li> </ol>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#other-resources","title":"Other Resources\u00b6","text":"<ul> <li>EODHD API: https://eodhd.com/pricing-special-10?via=crowdcent</li> <li>Centimators GitHub: https://github.com/crowdcent/centimators</li> <li>CrowdCent Docs: https://docs.crowdcent.com</li> <li>Join our Discord: Join here to get help and share your results with the community</li> </ul> <p>Happy modeling! \ud83e\uddea</p>"},{"location":"tutorials/advanced-cv-lstm/","title":"XGBoost vs LSTM with Time-Series Cross-Validation","text":"<p>Learn how to leverage the temporal structure in CrowdCent's training data to dramatically improve prediction performance using an LSTM model compared to a traditional XGBoost approach. We'll use the <code>sklego.model_selection.TimeGapSplit</code> to set up proper time-series cross-validation with gap periods to prevent leakage and then visualize the performance on various metrics over time with moving averages.</p> In\u00a0[1]: Copied! <pre>import crowdcent_challenge as cc\nimport polars as pl\nfrom xgboost import XGBRegressor\nfrom datetime import timedelta\nfrom sklego.model_selection import TimeGapSplit\nimport altair as alt\nimport os\nfrom crowdcent_challenge.scoring import evaluate_hyperliquid_submission\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nfrom centimators.model_estimators import LSTMRegressor\n</pre> import crowdcent_challenge as cc import polars as pl from xgboost import XGBRegressor from datetime import timedelta from sklego.model_selection import TimeGapSplit import altair as alt import os from crowdcent_challenge.scoring import evaluate_hyperliquid_submission  os.environ[\"KERAS_BACKEND\"] = \"jax\"  from centimators.model_estimators import LSTMRegressor In\u00a0[2]: Copied! <pre>client = cc.ChallengeClient(\n    challenge_slug=\"hyperliquid-ranking\",\n)\n</pre> client = cc.ChallengeClient(     challenge_slug=\"hyperliquid-ranking\", ) <pre>2025-06-16 20:28:18,994 - INFO - ChallengeClient initialized for 'hyperliquid-ranking' at URL: https://crowdcent.com/api\n</pre> In\u00a0[3]: Copied! <pre>client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")\n\ndata = pl.read_parquet(\"training_data.parquet\")\ndata.head()\n</pre> client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")  data = pl.read_parquet(\"training_data.parquet\") data.head() <pre>2025-06-16 20:28:19,256 - INFO - Downloading training data for challenge 'hyperliquid-ranking' v1.0 to training_data.parquet\nDownloading training_data.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 85.1M/85.1M [00:01&lt;00:00, 66.6MB/s]\n2025-06-16 20:28:20,974 - INFO - Successfully downloaded training data to training_data.parquet\n</pre> Out[3]: shape: (5, 85)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_5_lag5feature_6_lag5feature_7_lag5feature_12_lag5feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0target_10dtarget_30dstrstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\"BRETT\"\"BRETT29743-USD.CC\"2024-05-06 00:00:000.557210.5400860.5127540.5083860.6924260.552880.5491310.5338970.7367480.6172630.6039530.6239570.4502950.4792220.5193030.5281810.5273120.5258780.4974280.5185640.2394160.3983130.4512050.4873230.1182480.4053370.4489270.4920240.0861310.411440.4857980.5503510.2364960.343396\u20260.1773720.3651260.4936380.3518250.2189780.4181210.5460890.2686130.2525550.3658880.4952710.4204380.5510950.5384860.5429480.2333330.2495130.3239130.4479190.2898550.2631760.3342560.4351020.4289860.3904050.4009220.5086260.2724640.2705380.3069670.4318480.4014490.4109440.5077380.5074710.5072460.985507\"BRETT\"\"BRETT29743-USD.CC\"2024-05-07 00:00:000.4563390.4711330.5145620.5088860.547090.5194710.5154270.5329920.6133310.5474060.6066550.6273920.5634070.4617030.5124520.5486030.5179680.5189840.4913520.5146580.2335770.3449580.4464840.486310.1167880.3319390.4026210.4923320.0569340.3351330.4500720.550140.1109490.337178\u20260.1751610.3473160.4709130.3518140.2043740.375890.5276650.2306460.1707980.316250.4708390.3742090.6024330.5607090.539460.2724640.2697930.3073750.4289260.4072460.320390.3261650.4264550.4840580.4179360.3765340.4912640.3652170.2979320.3175550.4207230.2811590.3276840.5009990.4871540.8695650.985507\"BRETT\"\"BRETT29743-USD.CC\"2024-05-08 00:00:000.5664880.4536140.5077110.5293080.3837590.4341020.4716310.5182370.4484970.4916560.5639880.6157960.378070.3912570.4650070.5248810.468420.554210.4801550.5218110.1065690.3365290.4074770.4902670.2875910.3356750.4176910.5148160.2306570.3395770.453080.5749430.2919710.33502\u20260.2620280.3480650.4715070.3329740.2818150.3867360.5256980.2394050.2656880.3284730.4720020.4984980.6164020.5853060.5385370.3652170.2979160.3172220.4178010.2550720.2457690.2907220.4076340.3144930.3237330.3316550.4684770.1710140.205210.2701150.3968290.2057970.3521470.4767550.476010.9130430.985507\"BRETT\"\"BRETT29743-USD.CC\"2024-05-09 00:00:000.3825780.3831570.461010.5057680.4012340.425040.4411070.4961570.3218550.4137050.5247510.5937160.407170.4102030.4729390.5051730.5909940.5799140.4971070.5136470.2861310.3343550.4274680.5131960.2686130.3349240.3913430.4909490.3372260.3295410.4400730.5661840.2540150.330593\u20260.2619430.3434920.4503750.4030470.3701360.3919210.526860.2422720.2481430.3291730.4490710.5549880.5818730.5808940.5248280.1710140.2051940.2697740.3939070.2681160.2616950.2983090.3875450.2768120.3399290.3347350.4641480.3217390.2820060.3062990.3921480.2652170.4101030.504990.4683360.9347830.992754\"BRETT\"\"BRETT29743-USD.CC\"2024-05-10 00:00:000.4072990.3984320.4674910.4857090.2686130.4143990.4460950.4893010.2321170.4136730.4886280.57320.2408760.3967510.4566530.5067860.602920.5985950.5356170.5305950.2525550.3299270.3905030.4891340.2350360.2518250.3666990.4909230.3489050.2905110.4407290.5463160.2671530.254015\u20260.2683330.3413660.435410.4203220.3846130.3991430.5300580.2333650.2502590.3235050.4512190.389940.4986190.5486070.5101920.3217390.281990.3059580.3892260.2492750.2754520.2636390.3854330.3449280.3826250.3365680.4527840.3434780.2884220.2712180.3945390.2739130.3319260.4685180.4689640.9492750.985507 In\u00a0[4]: Copied! <pre>cv_kwargs = {\n    \"val_days\": 100,\n    \"gap_days\": 30,\n    \"n_splits\": 3,\n    \"cv_window_type\": \"rolling\",\n}\n\ncv = TimeGapSplit(\n    date_serie=data[\"date\"],\n    valid_duration=timedelta(days=cv_kwargs[\"val_days\"]),\n    gap_duration=timedelta(days=cv_kwargs[\"gap_days\"]),\n    n_splits=cv_kwargs[\"n_splits\"],\n    window=cv_kwargs[\"cv_window_type\"],\n)\n\ncv.summary(data)\n</pre> cv_kwargs = {     \"val_days\": 100,     \"gap_days\": 30,     \"n_splits\": 3,     \"cv_window_type\": \"rolling\", }  cv = TimeGapSplit(     date_serie=data[\"date\"],     valid_duration=timedelta(days=cv_kwargs[\"val_days\"]),     gap_duration=timedelta(days=cv_kwargs[\"gap_days\"]),     n_splits=cv_kwargs[\"n_splits\"],     window=cv_kwargs[\"cv_window_type\"], )  cv.summary(data) Out[4]: shape: (6, 7)Start dateEnd datePeriodUnique daysnbr samplespartfolddatetime[\u03bcs]datetime[\u03bcs]duration[\u03bcs]i64i64stri642020-02-26 00:00:002024-06-09 00:00:001565d1566128580\"train\"02024-07-10 00:00:002024-10-17 00:00:0099d10014170\"valid\"02020-06-05 00:00:002024-09-17 00:00:001565d1566138210\"train\"12024-10-18 00:00:002025-01-25 00:00:0099d10015160\"valid\"12020-09-13 00:00:002024-12-26 00:00:001565d1566148300\"train\"22025-01-26 00:00:002025-05-05 00:00:0099d10016958\"valid\"2 In\u00a0[5]: Copied! <pre>feature_cols = [\n    col\n    for col in data.columns\n    if col not in [\"id\", \"eodhd_id\", \"date\", \"target_10d\", \"target_30d\"]\n]\ntarget_cols = [\"target_10d\", \"target_30d\"]\nlag_windows = [0, 5, 10, 15]\nn_features_per_timestep = len(feature_cols) // len(lag_windows)\n</pre> feature_cols = [     col     for col in data.columns     if col not in [\"id\", \"eodhd_id\", \"date\", \"target_10d\", \"target_30d\"] ] target_cols = [\"target_10d\", \"target_30d\"] lag_windows = [0, 5, 10, 15] n_features_per_timestep = len(feature_cols) // len(lag_windows) In\u00a0[\u00a0]: Copied! <pre>print(\"Training and evaluating XGBoost model with detailed scoring...\")\n\nfold_data = []\nmodels = [\n    XGBRegressor(n_estimators=2000),\n    LSTMRegressor(\n        output_units=2,\n        lag_windows=lag_windows,\n        n_features_per_timestep=n_features_per_timestep,\n    ),\n]\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(data)):\n    print(f\"\\nFold {fold + 1}/{cv.n_splits}\")\n\n    # Get train and validation data\n    train_data = data[train_idx]\n    val_data = data[val_idx]\n\n    print(f\"  Train dates: {train_data['date'].min()} to {train_data['date'].max()}\")\n    print(f\"  Val dates: {val_data['date'].min()} to {val_data['date'].max()}\")\n    print(f\"  Train samples: {len(train_data)}, Val samples: {len(val_data)}\")\n\n    # Train model\n    for model in models:\n        fit_kwargs = {}\n        if isinstance(model, LSTMRegressor):\n            fit_kwargs[\"epochs\"] = 5\n            fit_kwargs[\"validation_data\"] = (\n                val_data[feature_cols].to_pandas(),\n                val_data[target_cols].to_pandas(),\n            )\n        model.fit(\n            train_data[feature_cols].to_pandas(),\n            train_data[target_cols].to_pandas(),\n            **fit_kwargs,\n        )\n\n        # Make predictions\n        preds = pl.from_numpy(\n            model.predict(val_data[feature_cols].to_pandas()),\n            {\"pred_10d\": pl.Float64, \"pred_30d\": pl.Float64},\n        )\n        preds = preds.with_columns(\n            pl.lit(fold).alias(\"fold\"), pl.lit(model.__class__.__name__).alias(\"model\")\n        )\n\n        fold_data.append(val_data.with_columns(preds))\n</pre> print(\"Training and evaluating XGBoost model with detailed scoring...\")  fold_data = [] models = [     XGBRegressor(n_estimators=2000),     LSTMRegressor(         output_units=2,         lag_windows=lag_windows,         n_features_per_timestep=n_features_per_timestep,     ), ]  for fold, (train_idx, val_idx) in enumerate(cv.split(data)):     print(f\"\\nFold {fold + 1}/{cv.n_splits}\")      # Get train and validation data     train_data = data[train_idx]     val_data = data[val_idx]      print(f\"  Train dates: {train_data['date'].min()} to {train_data['date'].max()}\")     print(f\"  Val dates: {val_data['date'].min()} to {val_data['date'].max()}\")     print(f\"  Train samples: {len(train_data)}, Val samples: {len(val_data)}\")      # Train model     for model in models:         fit_kwargs = {}         if isinstance(model, LSTMRegressor):             fit_kwargs[\"epochs\"] = 5             fit_kwargs[\"validation_data\"] = (                 val_data[feature_cols].to_pandas(),                 val_data[target_cols].to_pandas(),             )         model.fit(             train_data[feature_cols].to_pandas(),             train_data[target_cols].to_pandas(),             **fit_kwargs,         )          # Make predictions         preds = pl.from_numpy(             model.predict(val_data[feature_cols].to_pandas()),             {\"pred_10d\": pl.Float64, \"pred_30d\": pl.Float64},         )         preds = preds.with_columns(             pl.lit(fold).alias(\"fold\"), pl.lit(model.__class__.__name__).alias(\"model\")         )          fold_data.append(val_data.with_columns(preds)) <pre>Training and evaluating XGBoost model with detailed scoring...\n\nFold 1/3\n  Train dates: 2020-02-26 00:00:00 to 2024-06-09 00:00:00\n  Val dates: 2024-07-10 00:00:00 to 2024-10-17 00:00:00\n  Train samples: 128580, Val samples: 14170\n</pre> <pre>Epoch 1/5\n4019/4019 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 5ms/step - loss: 0.0817 - mse: 0.0817 - val_loss: 0.0784 - val_mse: 0.0784\nEpoch 2/5\n4019/4019 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21s 5ms/step - loss: 0.0784 - mse: 0.0784 - val_loss: 0.0781 - val_mse: 0.0781\nEpoch 3/5\n4019/4019 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21s 5ms/step - loss: 0.0780 - mse: 0.0780 - val_loss: 0.0781 - val_mse: 0.0781\nEpoch 4/5\n4019/4019 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21s 5ms/step - loss: 0.0783 - mse: 0.0783 - val_loss: 0.0781 - val_mse: 0.0781\nEpoch 5/5\n4019/4019 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21s 5ms/step - loss: 0.0781 - mse: 0.0781 - val_loss: 0.0778 - val_mse: 0.0778\n28/28 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 10ms/step\n\nFold 2/3\n  Train dates: 2020-06-05 00:00:00 to 2024-09-17 00:00:00\n  Val dates: 2024-10-18 00:00:00 to 2025-01-25 00:00:00\n  Train samples: 138210, Val samples: 15160\nEpoch 1/5\n4320/4320 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24s 5ms/step - loss: 0.0773 - mse: 0.0773 - val_loss: 0.0734 - val_mse: 0.0734\nEpoch 2/5\n4320/4320 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 5ms/step - loss: 0.0775 - mse: 0.0775 - val_loss: 0.0729 - val_mse: 0.0729\nEpoch 3/5\n4320/4320 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 5ms/step - loss: 0.0773 - mse: 0.0773 - val_loss: 0.0728 - val_mse: 0.0728\nEpoch 4/5\n4320/4320 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 5ms/step - loss: 0.0772 - mse: 0.0772 - val_loss: 0.0729 - val_mse: 0.0729\nEpoch 5/5\n4320/4320 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 5ms/step - loss: 0.0773 - mse: 0.0773 - val_loss: 0.0731 - val_mse: 0.0731\n30/30 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 11ms/step\n\nFold 3/3\n  Train dates: 2020-09-13 00:00:00 to 2024-12-26 00:00:00\n  Val dates: 2025-01-26 00:00:00 to 2025-05-05 00:00:00\n  Train samples: 148300, Val samples: 16958\nEpoch 1/5\n4635/4635 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 5ms/step - loss: 0.0769 - mse: 0.0769 - val_loss: 0.0767 - val_mse: 0.0767\nEpoch 2/5\n4635/4635 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24s 5ms/step - loss: 0.0767 - mse: 0.0767 - val_loss: 0.0783 - val_mse: 0.0783\nEpoch 3/5\n4635/4635 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24s 5ms/step - loss: 0.0769 - mse: 0.0769 - val_loss: 0.0770 - val_mse: 0.0770\nEpoch 4/5\n4635/4635 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24s 5ms/step - loss: 0.0764 - mse: 0.0764 - val_loss: 0.0767 - val_mse: 0.0767\nEpoch 5/5\n4635/4635 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24s 5ms/step - loss: 0.0768 - mse: 0.0768 - val_loss: 0.0787 - val_mse: 0.0787\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 11ms/step\n</pre> In\u00a0[31]: Copied! <pre>combined_preds = pl.concat(fold_data)\ncombined_preds.head()\n</pre> combined_preds = pl.concat(fold_data) combined_preds.head() Out[31]: shape: (5, 89)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0target_10dtarget_30dpred_10dpred_30dfoldmodelstrstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64i32str\"BRETT\"\"BRETT29743-USD.CC\"2024-07-10 00:00:000.5826360.631960.5676330.6533650.5670620.5308950.5087040.566440.5058350.6227840.6450150.6915820.610060.6612890.5683150.6404920.5048290.4696910.4715150.4860040.5916670.5871520.5169990.6294830.3867230.4768930.4361820.5516730.5810110.5434230.5467890.6647510.4539450.532003\u20260.4861670.5544750.6197810.5081450.4810450.5711670.5837080.4316660.3972320.4334620.4433890.40.4600270.5235890.5572020.3671330.4855610.4812270.4960030.4279720.4096470.4765350.5698280.3888110.4484780.490240.5470420.5440560.4878610.4608370.4673840.3286710.4055940.2626040.4367910\"XGBRegressor\"\"TIA\"\"TIA-USD.CC\"2024-07-10 00:00:000.5273240.4625470.4245170.4790830.6677060.5960060.5017830.5027820.6641250.4899140.4337120.4527010.5473240.4645560.4345250.4675870.5326960.5492620.5005880.4894460.3334880.4304060.425280.4503030.267350.4675280.4573880.4748880.3851080.5246160.4365110.4390010.3513540.449339\u20260.414040.4519770.4465310.440490.3959220.4302390.4383680.5484980.5214320.5353470.4896050.7580420.5690090.4997070.4530810.7524480.5472820.5074050.4841740.8839160.6634440.594030.4719790.6307690.535630.4924840.4472630.5916080.5700530.5417920.5169220.0139860.1748250.1899650.131810\"XGBRegressor\"\"UNIBOT\"\"UNIBOT27009-USD.CC\"2024-07-10 00:00:000.1973040.4924130.4559580.4342610.2173840.4279730.4150990.4298360.1451710.4387110.402830.4109170.2043460.4411760.4398640.434010.5924750.4953480.5193820.525080.4502910.3237970.4696180.4236260.399350.3083670.4366760.4063460.3642570.2547140.4101980.3888120.4347580.319552\u20260.5330150.4858630.4252740.5396930.4872250.4642010.422670.3796020.4352060.4652770.5032090.3818180.4350980.3794480.4252860.510490.5043680.4063680.4176180.3762240.5389980.3968560.4030730.5118880.525790.4226710.4379710.6041960.4918990.5167710.5110460.9720280.8111890.3121270.4598490\"XGBRegressor\"\"GMX\"\"GMX11857-USD.CC\"2024-07-10 00:00:000.6553520.5473520.525050.514110.4935410.4786260.4530180.5022330.5827770.4888750.4570430.4808310.5775860.5163620.4896270.5182910.439940.5128940.4576280.512470.5116320.5834920.4899140.5253450.6199050.5567230.4786470.5181880.577770.5802730.4873550.4888070.5974490.587517\u20260.5829020.5358890.5056570.5989070.5981780.557270.561890.4661480.4910620.5019780.5032810.4881120.5645920.5740420.5357430.4251750.5220230.5393730.5076920.4923080.540170.5602220.4772710.4615380.5302230.558870.5236080.5776220.5218850.4999210.4897240.2657340.4475520.3625850.3911940\"XGBRegressor\"\"MERL\"\"MERL-USD.CC\"2024-07-10 00:00:000.5265390.404210.4059030.4546940.6573040.4996280.4010890.4810430.5238630.4326920.4313680.5175780.5235810.481960.3955420.4464140.6504230.5132890.5088110.5125390.5161130.5213260.4385540.4814550.4915790.5744410.4594910.5048310.5690240.5464440.4671020.5489910.4818970.502739\u20260.5487840.4907380.5175770.3645720.4232350.4525970.4443080.4177680.438880.4760850.4945370.5846150.5358020.5285640.4530470.5356640.4464390.510440.443710.6055940.5670690.5567560.4914420.5846150.4745940.4886660.4295550.5440560.4809120.518060.4897410.0559440.5174830.2771850.2272540\"XGBRegressor\" In\u00a0[32]: Copied! <pre>def calculate_daily_scores(predictions_df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Calculate daily scores for predictions using hyperliquid evaluation metrics.\n\n    Args:\n        predictions_df: DataFrame containing predictions and targets\n\n    Returns:\n        DataFrame with daily evaluation metrics\n    \"\"\"\n    columns = [\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"]\n\n    return (\n        predictions_df.group_by([\"date\", \"model\"])\n        .agg([pl.col(col) for col in columns])\n        .with_columns(\n            pl.struct(columns)\n            .alias(\"daily_scores\")\n            .map_elements(\n                lambda x: evaluate_hyperliquid_submission(\n                    y_true_10d=x[\"target_10d\"],\n                    y_pred_10d=x[\"pred_10d\"],\n                    y_true_30d=x[\"target_30d\"],\n                    y_pred_30d=x[\"pred_30d\"],\n                ),\n                return_dtype=pl.Struct,\n            )\n        )\n    ).unnest(\"daily_scores\")\n\n\ndaily_scores = calculate_daily_scores(combined_preds)\ndaily_scores\n</pre> def calculate_daily_scores(predictions_df: pl.DataFrame) -&gt; pl.DataFrame:     \"\"\"Calculate daily scores for predictions using hyperliquid evaluation metrics.      Args:         predictions_df: DataFrame containing predictions and targets      Returns:         DataFrame with daily evaluation metrics     \"\"\"     columns = [\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"]      return (         predictions_df.group_by([\"date\", \"model\"])         .agg([pl.col(col) for col in columns])         .with_columns(             pl.struct(columns)             .alias(\"daily_scores\")             .map_elements(                 lambda x: evaluate_hyperliquid_submission(                     y_true_10d=x[\"target_10d\"],                     y_pred_10d=x[\"pred_10d\"],                     y_true_30d=x[\"target_30d\"],                     y_pred_30d=x[\"pred_30d\"],                 ),                 return_dtype=pl.Struct,             )         )     ).unnest(\"daily_scores\")   daily_scores = calculate_daily_scores(combined_preds) daily_scores Out[32]: shape: (600, 10)datemodelpred_10dtarget_10dpred_30dtarget_30dspearman_10dspearman_30dndcg@40_10dndcg@40_30ddatetime[\u03bcs]strlist[f64]list[f64]list[f64]list[f64]f64f64f64f642025-03-19 00:00:00\"LSTMRegressor\"[0.510342, 0.589169, \u2026 0.516715][0.611429, 0.831429, \u2026 0.177143][0.520879, 0.534651, \u2026 0.528912][0.845714, 0.231429, \u2026 0.977143]0.4071220.2846060.7498860.6904132025-03-05 00:00:00\"XGBRegressor\"[0.438884, 0.374294, \u2026 0.463834][0.212644, 0.066092, \u2026 0.833333][0.33935, 0.566031, \u2026 0.488783][0.45977, 0.175287, \u2026 0.511494]0.1523090.0549230.6539850.5504782024-10-13 00:00:00\"LSTMRegressor\"[0.614008, 0.540451, \u2026 0.517269][0.655629, 0.443709, \u2026 0.907285][0.592994, 0.529025, \u2026 0.515621][0.741722, 0.125828, \u2026 0.695364]0.1275050.0730990.6037020.6080252025-04-08 00:00:00\"XGBRegressor\"[0.509997, 0.595915, \u2026 0.702079][0.938202, 0.176966, \u2026 0.983146][0.625393, 0.684723, \u2026 0.639499][0.955056, 0.69382, \u2026 0.713483]0.2877630.043480.6390290.5859652024-10-24 00:00:00\"XGBRegressor\"[0.532243, 0.478276, \u2026 0.520768][0.188312, 0.168831, \u2026 0.305195][0.66408, 0.632372, \u2026 0.574681][0.883117, 0.103896, \u2026 0.350649]0.2284290.0944410.6556640.615117\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u20262025-03-10 00:00:00\"LSTMRegressor\"[0.416265, 0.472912, \u2026 0.559355][0.554286, 0.597143, \u2026 0.748571][0.460617, 0.44555, \u2026 0.547587][0.542857, 0.408571, \u2026 0.925714]0.1789970.1257980.5662580.6310342024-08-06 00:00:00\"XGBRegressor\"[0.913959, 0.477772, \u2026 0.412959][0.041379, 0.641379, \u2026 0.682759][0.760317, 0.657521, \u2026 0.529122][0.034483, 0.02069, \u2026 0.462069]0.239257-0.0010980.6961980.5992272024-07-18 00:00:00\"XGBRegressor\"[0.68644, 0.285066, \u2026 0.467805][0.482517, 0.083916, \u2026 0.951049][0.57571, 0.447701, \u2026 0.350697][0.160839, 0.286713, \u2026 0.825175]0.16556-0.1286050.6496760.5241512024-10-14 00:00:00\"XGBRegressor\"[0.428081, 0.506207, \u2026 0.358323][0.112583, 0.298013, \u2026 0.827815][0.41764, 0.546764, \u2026 0.229562][0.344371, 0.192053, \u2026 0.549669]0.2694870.1799180.6977520.6582822024-09-12 00:00:00\"LSTMRegressor\"[0.458443, 0.373957, \u2026 0.47088][0.136986, 0.047945, \u2026 0.979452][0.488663, 0.425867, \u2026 0.500952][0.60274, 0.205479, \u2026 0.876712]0.201720.0567490.6758720.585648 In\u00a0[33]: Copied! <pre>from centimators.feature_transformers import MovingAverageTransformer\n\ndaily_scores = daily_scores.sort(\"date\")\nma_transformer = MovingAverageTransformer(\n    feature_names=[\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"],\n    windows=[7, 30],\n)\nma_columns = ma_transformer.fit_transform(\n    daily_scores, ticker_series=daily_scores[\"model\"]\n)\ndaily_scores_df = daily_scores.with_columns(ma_columns)\n</pre> from centimators.feature_transformers import MovingAverageTransformer  daily_scores = daily_scores.sort(\"date\") ma_transformer = MovingAverageTransformer(     feature_names=[\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"],     windows=[7, 30], ) ma_columns = ma_transformer.fit_transform(     daily_scores, ticker_series=daily_scores[\"model\"] ) daily_scores_df = daily_scores.with_columns(ma_columns) In\u00a0[37]: Copied! <pre>def plot_metric_comparison(df, metric_name, anchor_ref=0, width=400, height=200):\n    \"\"\"Plot a single metric across 10d and 30d timeframes side by side with shared y-axis.\n\n    Args:\n        df: Polars DataFrame with daily scores\n        metric_name: Base metric name ('spearman' or 'ndcg@40')\n        width: Chart width in pixels per timeframe\n        height: Chart height\n    \"\"\"\n    # Convert to pandas once\n    pdf = df.to_pandas()\n\n    # Shared selection for zooming\n    brush = alt.selection_interval(bind=\"scales\", encodings=[\"x\"])\n\n    # Define timeframes and colors\n    timeframes = [\"10d\", \"30d\"]\n\n    # Get column names for this metric\n    col_10d = f\"{metric_name}_10d\"\n    col_30d = f\"{metric_name}_30d\"\n\n    # Calculate shared y-axis domain\n    min_val = min(df[col_10d].min(), df[col_30d].min())\n    max_val = max(df[col_10d].max(), df[col_30d].max())\n    y_domain = [min_val * 0.95, max_val * 1.05]\n\n    charts = []\n    for timeframe in timeframes:\n        col_name = f\"{metric_name}_{timeframe}\"\n\n        # Calculate per-model statistics\n        model_stats = (\n            df.group_by(\"model\")\n            .agg(\n                [\n                    pl.col(col_name).mean().alias(\"mean\"),\n                    pl.col(col_name).std().alias(\"std\"),\n                ]\n            )\n            .with_columns((pl.col(\"mean\") / pl.col(\"std\")).alias(\"sharpe\"))\n        )\n\n        # Create concise title with mean values\n        mean_values = model_stats.to_pandas()\n        mean_text = \" | \".join(\n            [f\"{row['model']}: {row['mean']:.3f}\" for _, row in mean_values.iterrows()]\n        )\n        title_text = f\"{metric_name.upper()} {timeframe}\\nMeans: {mean_text}\"\n\n        chart = (\n            alt.Chart(pdf)\n            .add_params(brush)\n            .mark_point(opacity=0.6)\n            .encode(\n                x=alt.X(\"date:T\", title=\"Date\"),\n                y=alt.Y(\n                    f\"{col_name}:Q\",\n                    title=f\"{metric_name.upper()} {timeframe}\",\n                    scale=alt.Scale(domain=y_domain),\n                ),\n                color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),\n                tooltip=[\n                    \"date:T\",\n                    \"model:N\",\n                    f\"{col_name}:Q\",\n                    alt.Tooltip(\"model:N\", title=\"Model\"),\n                ],\n            )\n            .properties(\n                width=width,\n                height=height,\n                title=alt.TitleParams(\n                    text=title_text,\n                    fontSize=10,\n                    anchor=\"start\",\n                ),\n            )\n        )\n\n        moving_average_chart = (\n            alt.Chart(pdf)\n            .mark_line(strokeWidth=2, opacity=1)\n            .encode(\n                x=alt.X(\"date:T\"),\n                y=alt.Y(f\"{col_name}_ma30:Q\", scale=alt.Scale(domain=y_domain)),\n                color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),\n                tooltip=[\n                    \"date:T\",\n                    \"model:N\",\n                    f\"{col_name}_ma30:Q\",\n                    alt.Tooltip(\"model:N\", title=\"Model\"),\n                ],\n            )\n        )\n\n        # Add per-model mean reference lines\n        mean_lines = []\n        for row in model_stats.iter_rows(named=True):\n            model = row[\"model\"]\n            mean_val = row[\"mean\"]\n\n            mean_line = (\n                alt.Chart(pdf[pdf[\"model\"] == model])\n                .mark_rule(strokeDash=[5, 5], opacity=1, strokeWidth=2)\n                .encode(\n                    y=alt.datum(mean_val),\n                    color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=0.7)),\n                )\n            )\n            mean_lines.append(mean_line)\n\n        # Add anchor reference line\n        anchor_line = (\n            alt.Chart(pdf)\n            .mark_rule(strokeDash=[2, 2], opacity=0.7, color=\"gray\", strokeWidth=1)\n            .encode(y=alt.datum(anchor_ref))\n        )\n\n        combined_chart = chart + moving_average_chart + anchor_line\n        for mean_line in mean_lines:\n            combined_chart += mean_line\n\n        charts.append(combined_chart)\n\n    return alt.hconcat(*charts, spacing=10).resolve_scale(x=\"shared\")\n\n\n# Plot both metrics\nspearman_chart = plot_metric_comparison(daily_scores_df, \"spearman\", anchor_ref=0)\nndcg_chart = plot_metric_comparison(daily_scores_df, \"ndcg@40\", anchor_ref=0.5)\n\n# Combine vertically\nalt.vconcat(spearman_chart, ndcg_chart, spacing=20)\n</pre> def plot_metric_comparison(df, metric_name, anchor_ref=0, width=400, height=200):     \"\"\"Plot a single metric across 10d and 30d timeframes side by side with shared y-axis.      Args:         df: Polars DataFrame with daily scores         metric_name: Base metric name ('spearman' or 'ndcg@40')         width: Chart width in pixels per timeframe         height: Chart height     \"\"\"     # Convert to pandas once     pdf = df.to_pandas()      # Shared selection for zooming     brush = alt.selection_interval(bind=\"scales\", encodings=[\"x\"])      # Define timeframes and colors     timeframes = [\"10d\", \"30d\"]      # Get column names for this metric     col_10d = f\"{metric_name}_10d\"     col_30d = f\"{metric_name}_30d\"      # Calculate shared y-axis domain     min_val = min(df[col_10d].min(), df[col_30d].min())     max_val = max(df[col_10d].max(), df[col_30d].max())     y_domain = [min_val * 0.95, max_val * 1.05]      charts = []     for timeframe in timeframes:         col_name = f\"{metric_name}_{timeframe}\"          # Calculate per-model statistics         model_stats = (             df.group_by(\"model\")             .agg(                 [                     pl.col(col_name).mean().alias(\"mean\"),                     pl.col(col_name).std().alias(\"std\"),                 ]             )             .with_columns((pl.col(\"mean\") / pl.col(\"std\")).alias(\"sharpe\"))         )          # Create concise title with mean values         mean_values = model_stats.to_pandas()         mean_text = \" | \".join(             [f\"{row['model']}: {row['mean']:.3f}\" for _, row in mean_values.iterrows()]         )         title_text = f\"{metric_name.upper()} {timeframe}\\nMeans: {mean_text}\"          chart = (             alt.Chart(pdf)             .add_params(brush)             .mark_point(opacity=0.6)             .encode(                 x=alt.X(\"date:T\", title=\"Date\"),                 y=alt.Y(                     f\"{col_name}:Q\",                     title=f\"{metric_name.upper()} {timeframe}\",                     scale=alt.Scale(domain=y_domain),                 ),                 color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),                 tooltip=[                     \"date:T\",                     \"model:N\",                     f\"{col_name}:Q\",                     alt.Tooltip(\"model:N\", title=\"Model\"),                 ],             )             .properties(                 width=width,                 height=height,                 title=alt.TitleParams(                     text=title_text,                     fontSize=10,                     anchor=\"start\",                 ),             )         )          moving_average_chart = (             alt.Chart(pdf)             .mark_line(strokeWidth=2, opacity=1)             .encode(                 x=alt.X(\"date:T\"),                 y=alt.Y(f\"{col_name}_ma30:Q\", scale=alt.Scale(domain=y_domain)),                 color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),                 tooltip=[                     \"date:T\",                     \"model:N\",                     f\"{col_name}_ma30:Q\",                     alt.Tooltip(\"model:N\", title=\"Model\"),                 ],             )         )          # Add per-model mean reference lines         mean_lines = []         for row in model_stats.iter_rows(named=True):             model = row[\"model\"]             mean_val = row[\"mean\"]              mean_line = (                 alt.Chart(pdf[pdf[\"model\"] == model])                 .mark_rule(strokeDash=[5, 5], opacity=1, strokeWidth=2)                 .encode(                     y=alt.datum(mean_val),                     color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=0.7)),                 )             )             mean_lines.append(mean_line)          # Add anchor reference line         anchor_line = (             alt.Chart(pdf)             .mark_rule(strokeDash=[2, 2], opacity=0.7, color=\"gray\", strokeWidth=1)             .encode(y=alt.datum(anchor_ref))         )          combined_chart = chart + moving_average_chart + anchor_line         for mean_line in mean_lines:             combined_chart += mean_line          charts.append(combined_chart)      return alt.hconcat(*charts, spacing=10).resolve_scale(x=\"shared\")   # Plot both metrics spearman_chart = plot_metric_comparison(daily_scores_df, \"spearman\", anchor_ref=0) ndcg_chart = plot_metric_comparison(daily_scores_df, \"ndcg@40\", anchor_ref=0.5)  # Combine vertically alt.vconcat(spearman_chart, ndcg_chart, spacing=20) Out[37]:"},{"location":"tutorials/advanced-cv-lstm/#key-insight-sequential-feature-processing","title":"Key Insight: Sequential Feature Processing\u00b6","text":"<p>CrowdCent's training and inference data contains features with a defined temporal sequence through lag windows (e.g., <code>feature_1_lag15</code>, <code>feature_1_lag10</code>, <code>feature_1_lag5</code>, <code>feature_1_lag0</code>). While traditional models like XGBoost treat these as independent features, an LSTM can process them sequentially, capturing temporal dependencies.</p>"},{"location":"tutorials/advanced-cv-lstm/#model-comparison","title":"Model Comparison\u00b6","text":"<ul> <li>XGBoost: Treats all features as independent inputs, achieving ~0.06 average 30-day Spearman correlation</li> <li>LSTM (from Centimators): Processes features sequentially by reshaping them along the lag axis, achieving ~0.19 average 30-day Spearman correlation - over 3x improvement!</li> </ul>"},{"location":"tutorials/advanced-cv-lstm/#feature-reshaping-example","title":"Feature Reshaping Example\u00b6","text":"<p>The LSTM transforms the flat feature vector:</p> <pre><code>[feature_1_lag10, feature_1_lag5, feature_1_lag0, feature_2_lag10, feature_2_lag5, feature_2_lag0, ...]\n</code></pre> <p>into a sequential 2D tensor:</p> <pre><code>[[feature_1_lag10, feature_2_lag10],\n [feature_1_lag5, feature_2_lag5],\n [feature_1_lag0, feature_2_lag0]]\n</code></pre>"},{"location":"tutorials/advanced-cv-lstm/#what-youll-learn","title":"What You'll Learn\u00b6","text":"<ol> <li>How to set up proper time-series cross-validation with gap periods to prevent leakage</li> <li>How to train both XGBoost and LSTM models on the same features for fair comparison</li> <li>How to evaluate models using CrowdCent's official scoring metrics (Spearman correlation and NDCG@40)</li> <li>How to visualize performance over time with moving averages</li> </ol>"},{"location":"tutorials/advanced-cv-lstm/#key-findings","title":"Key Findings\u00b6","text":"<ul> <li>10-day predictions achieve higher raw scores than 30-day predictions (e.g., 0.255 vs 0.19 spearman corr for LSTM)</li> <li>The LSTM significantly outperforms XGBoost without any hyperparameter tuning (experimental results)</li> <li>All results shown are out-of-sample using proper time-series cross-validation (TimeGapSplit from sklego)</li> </ul> <p>Note: This comparison uses identical features for both models and no hyperparameter optimization, demonstrating the power of sequential processing for this dataset.</p>"},{"location":"tutorials/advanced-cv-lstm/#initialize-the-client","title":"Initialize the client\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#get-crowdcents-training-data","title":"Get CrowdCent's training data\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#create-cross-validation-folds","title":"Create cross validation folds\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#define-features-targets-and-lag-windows","title":"Define features, targets, and lag windows\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#train-models-on-each-train-fold-and-predict-validation","title":"Train models on each train fold and predict validation\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#calculate-scores","title":"Calculate scores\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#plot-validation-metrics-combining-all-folds","title":"Plot validation metrics (combining all folds)\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/","title":"Hyperliquid End-to-End","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install crowdcent-challenge\nimport crowdcent_challenge as cc\nimport polars as pl\nfrom xgboost import XGBRegressor\n</pre> !pip install crowdcent-challenge import crowdcent_challenge as cc import polars as pl from xgboost import XGBRegressor <p>For this tutorial, you will need:</p> <ol> <li>CrowdCent account: register for free</li> <li>CrowdCent API Key: generate an API key from your user profile</li> </ol> In\u00a0[\u00a0]: Copied! <pre>CROWDCENT_API_KEY = \"API_KEY_HERE\"\n</pre> CROWDCENT_API_KEY = \"API_KEY_HERE\" In\u00a0[\u00a0]: Copied! <pre>client = cc.ChallengeClient(\n    challenge_slug=\"hyperliquid-ranking\",\n    api_key=CROWDCENT_API_KEY,\n)\n</pre> client = cc.ChallengeClient(     challenge_slug=\"hyperliquid-ranking\",     api_key=CROWDCENT_API_KEY, ) <pre>2025-07-25 16:47:55,298 - INFO - ChallengeClient initialized for 'hyperliquid-ranking' at URL: https://crowdcent.com/api\n</pre> In\u00a0[4]: Copied! <pre>client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")\n\ntraining_data = pl.read_parquet(\"training_data.parquet\")\ntraining_data.head()\n</pre> client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")  training_data = pl.read_parquet(\"training_data.parquet\") training_data.head() <pre>2025-07-25 16:47:57,090 - INFO - Downloading training data for challenge 'hyperliquid-ranking' v1.0 to training_data.parquet\nDownloading training_data.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 85.1M/85.1M [00:04&lt;00:00, 18.3MB/s]\n2025-07-25 16:48:02,297 - INFO - Successfully downloaded training data to training_data.parquet\n</pre> Out[4]: shape: (5, 85)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_5_lag5feature_6_lag5feature_7_lag5feature_12_lag5feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0target_10dtarget_30dstrstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\"BRETT\"\"BRETT29743-USD.CC\"2024-05-06 00:00:000.557210.5400860.5127540.5083860.6924260.552880.5491310.5338970.7367480.6172630.6039530.6239570.4502950.4792220.5193030.5281810.5273120.5258780.4974280.5185640.2394160.3983130.4512050.4873230.1182480.4053370.4489270.4920240.0861310.411440.4857980.5503510.2364960.343396\u20260.1773720.3651260.4936380.3518250.2189780.4181210.5460890.2686130.2525550.3658880.4952710.4204380.5510950.5384860.5429480.2333330.2495130.3239130.4479190.2898550.2631760.3342560.4351020.4289860.3904050.4009220.5086260.2724640.2705380.3069670.4318480.4014490.4109440.5077380.5074710.5072460.985507\"BRETT\"\"BRETT29743-USD.CC\"2024-05-07 00:00:000.4563390.4711330.5145620.5088860.547090.5194710.5154270.5329920.6133310.5474060.6066550.6273920.5634070.4617030.5124520.5486030.5179680.5189840.4913520.5146580.2335770.3449580.4464840.486310.1167880.3319390.4026210.4923320.0569340.3351330.4500720.550140.1109490.337178\u20260.1751610.3473160.4709130.3518140.2043740.375890.5276650.2306460.1707980.316250.4708390.3742090.6024330.5607090.539460.2724640.2697930.3073750.4289260.4072460.320390.3261650.4264550.4840580.4179360.3765340.4912640.3652170.2979320.3175550.4207230.2811590.3276840.5009990.4871540.8695650.985507\"BRETT\"\"BRETT29743-USD.CC\"2024-05-08 00:00:000.5664880.4536140.5077110.5293080.3837590.4341020.4716310.5182370.4484970.4916560.5639880.6157960.378070.3912570.4650070.5248810.468420.554210.4801550.5218110.1065690.3365290.4074770.4902670.2875910.3356750.4176910.5148160.2306570.3395770.453080.5749430.2919710.33502\u20260.2620280.3480650.4715070.3329740.2818150.3867360.5256980.2394050.2656880.3284730.4720020.4984980.6164020.5853060.5385370.3652170.2979160.3172220.4178010.2550720.2457690.2907220.4076340.3144930.3237330.3316550.4684770.1710140.205210.2701150.3968290.2057970.3521470.4767550.476010.9130430.985507\"BRETT\"\"BRETT29743-USD.CC\"2024-05-09 00:00:000.3825780.3831570.461010.5057680.4012340.425040.4411070.4961570.3218550.4137050.5247510.5937160.407170.4102030.4729390.5051730.5909940.5799140.4971070.5136470.2861310.3343550.4274680.5131960.2686130.3349240.3913430.4909490.3372260.3295410.4400730.5661840.2540150.330593\u20260.2619430.3434920.4503750.4030470.3701360.3919210.526860.2422720.2481430.3291730.4490710.5549880.5818730.5808940.5248280.1710140.2051940.2697740.3939070.2681160.2616950.2983090.3875450.2768120.3399290.3347350.4641480.3217390.2820060.3062990.3921480.2652170.4101030.504990.4683360.9347830.992754\"BRETT\"\"BRETT29743-USD.CC\"2024-05-10 00:00:000.4072990.3984320.4674910.4857090.2686130.4143990.4460950.4893010.2321170.4136730.4886280.57320.2408760.3967510.4566530.5067860.602920.5985950.5356170.5305950.2525550.3299270.3905030.4891340.2350360.2518250.3666990.4909230.3489050.2905110.4407290.5463160.2671530.254015\u20260.2683330.3413660.435410.4203220.3846130.3991430.5300580.2333650.2502590.3235050.4512190.389940.4986190.5486070.5101920.3217390.281990.3059580.3892260.2492750.2754520.2636390.3854330.3449280.3826250.3365680.4527840.3434780.2884220.2712180.3945390.2739130.3319260.4685180.4689640.9492750.985507 In\u00a0[5]: Copied! <pre>xgb_regressor = XGBRegressor(n_estimators=2000)\nfeature_names = [col for col in training_data.columns if col.startswith(\"feature\")]\n\nxgb_regressor.fit(\n    training_data[feature_names],\n    training_data[[\"target_10d\", \"target_30d\"]],\n)\n</pre> xgb_regressor = XGBRegressor(n_estimators=2000) feature_names = [col for col in training_data.columns if col.startswith(\"feature\")]  xgb_regressor.fit(     training_data[feature_names],     training_data[[\"target_10d\", \"target_30d\"]], ) Out[5]: <pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=2000,\n             n_jobs=None, num_parallel_tree=None, ...)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor?Documentation for XGBRegressoriFitted Parameters objective\u00a0 'reg:squarederror' base_score\u00a0 None booster\u00a0 None callbacks\u00a0 None colsample_bylevel\u00a0 None colsample_bynode\u00a0 None colsample_bytree\u00a0 None device\u00a0 None early_stopping_rounds\u00a0 None enable_categorical\u00a0 False eval_metric\u00a0 None feature_types\u00a0 None feature_weights\u00a0 None gamma\u00a0 None grow_policy\u00a0 None importance_type\u00a0 None interaction_constraints\u00a0 None learning_rate\u00a0 None max_bin\u00a0 None max_cat_threshold\u00a0 None max_cat_to_onehot\u00a0 None max_delta_step\u00a0 None max_depth\u00a0 None max_leaves\u00a0 None min_child_weight\u00a0 None missing\u00a0 nan monotone_constraints\u00a0 None multi_strategy\u00a0 None n_estimators\u00a0 2000 n_jobs\u00a0 None num_parallel_tree\u00a0 None random_state\u00a0 None reg_alpha\u00a0 None reg_lambda\u00a0 None sampling_method\u00a0 None scale_pos_weight\u00a0 None subsample\u00a0 None tree_method\u00a0 None validate_parameters\u00a0 None verbosity\u00a0 None In\u00a0[6]: Copied! <pre>client.download_inference_data(\"current\", \"inference_data.parquet\")\n\ninference_data = pl.read_parquet(\"inference_data.parquet\")\ninference_data.head()\n</pre> client.download_inference_data(\"current\", \"inference_data.parquet\")  inference_data = pl.read_parquet(\"inference_data.parquet\") inference_data.head() <pre>2025-07-25 16:48:34,787 - INFO - Downloading inference data for challenge 'hyperliquid-ranking' current to inference_data.parquet\nDownloading inference_data.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125k/125k [00:00&lt;00:00, 3.39MB/s]\n2025-07-25 16:48:35,220 - INFO - Successfully downloaded inference data to inference_data.parquet\n2025-07-25 16:48:35,221 - INFO - Successfully downloaded inference data after 1 attempt(s) to inference_data.parquet\n</pre> Out[6]: shape: (5, 83)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_15_lag5feature_8_lag5feature_5_lag5feature_6_lag5feature_7_lag5feature_12_lag5feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0strstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\"ENS\"\"ENS-USD.CC\"2025-07-24 00:00:000.5637430.5532160.5105260.5295790.5883040.5514620.507310.5426090.513450.4941520.4611110.4926970.5906430.5771930.5160820.5273470.5953220.5450290.5210530.4995380.5695910.5666670.5447370.5394540.5520470.5701750.5423980.5320560.60.5567250.5114040.5012260.6035090.597076\u20260.5750470.8187130.685380.6184210.5723370.7216370.6608190.5774850.5451070.7520470.6777780.6274850.5794380.5497080.5514620.5482460.5147570.541520.6672510.6169590.5728070.5859650.7023390.6362570.5757310.4643270.5929820.5748540.5247080.6035090.6777780.6374270.5790940.5181290.5339180.5540940.517544\"DOOD\"\"DOOD-USD.CC\"2025-07-24 00:00:000.5672510.60.470760.4820890.2912280.3567250.3847950.4233680.6935670.6760230.5233920.563110.4187130.5175440.4564330.4791550.2935670.4333330.4871350.5030570.4198830.4935670.5403510.4564860.5625730.4269010.4587720.4298660.5508770.6222220.6307020.5297640.4093570.414035\u20260.435140.4105260.486550.4216370.4448580.5684210.5596490.6178360.5418460.3964910.4029240.4602340.4517970.4245610.4847950.4590640.4964620.8198830.5251460.5093570.5017540.4222220.4163740.4216370.4505850.7497080.6590640.6406430.591520.7976610.5970760.5055560.506140.5181290.4713450.4453220.50614\"ETH\"\"ETH-USD.CC\"2025-07-24 00:00:000.6982460.6479530.5862570.582370.6783630.6491230.5807020.6015090.6514620.6286550.5502920.5575040.6538010.6847950.5789470.573740.5614040.5713450.5385960.5418270.5052630.6017540.5830410.5844990.4374270.5578950.5666670.5810410.4807020.5660820.5602340.5518570.543860.59883\u20260.6156830.80.6187130.6339180.6107460.7918130.6362570.6324560.5962750.7064330.6251460.6549710.611350.571930.5222220.5467840.5238530.5368420.6631580.6324560.5961990.5871350.6935670.6257310.5973680.5614040.6766080.6213450.5793860.6549710.6807020.6397660.6064330.5192980.5456140.5312870.527485\"TAO\"\"TAO22974-USD.CC\"2025-07-24 00:00:000.3929820.4485380.4383040.4855390.5473680.4871350.4625730.48960.3847950.4391810.4447370.4997790.4175440.4754390.4722220.4896140.5614040.5111110.5233920.4996330.6245610.5087720.4795320.5174330.6807020.6140350.5122810.5251290.6631580.5239770.4909360.5251740.6350880.526316\u20260.5000150.4046780.542690.5149120.4924790.5543860.6087720.5239770.5143250.3590640.4970760.4862570.4857950.4678360.4964910.5038010.4958140.5450290.5169590.5128650.4975150.6257310.5152050.564620.5163740.5005850.5274850.5257310.514620.5742690.4666670.4964910.50.486550.4771930.5102340.508626\"PNUT\"\"PNUT-USD.CC\"2025-07-24 00:00:000.343860.4795320.4871350.491550.371930.4830410.4593570.4687050.5333330.5783630.5535090.54570.3461990.4152050.4678360.4960820.5637430.5520470.5152050.4911960.5356730.4397660.491520.4920380.5695910.470760.4885960.4793150.5894740.5614040.5739770.5612960.5099420.42807\u20260.4849840.314620.4421050.4625730.4520250.4152050.5023390.5403510.5314320.4842110.4970760.456140.4756450.4093570.4912280.5216370.4935620.6175440.5058480.4728070.5159360.5298250.4222220.4464910.4790940.5356730.4754390.5184210.5483920.514620.4994150.4637430.5046780.3976610.4035090.4859650.488304 In\u00a0[7]: Copied! <pre>preds = xgb_regressor.predict(inference_data[feature_names])\npred_df = pl.from_numpy(preds, [\"pred_10d\", \"pred_30d\"])\n</pre> preds = xgb_regressor.predict(inference_data[feature_names]) pred_df = pl.from_numpy(preds, [\"pred_10d\", \"pred_30d\"]) In\u00a0[8]: Copied! <pre>pred_df = pred_df.with_columns(inference_data[\"id\"]).select(\n    [\"id\", \"pred_10d\", \"pred_30d\"]\n)\n\n# ensure predictions are between 0 and 1\npred_df = pred_df.with_columns(pl.col([\"pred_10d\", \"pred_30d\"]).clip(0, 1))\n\n\nwith pl.Config(tbl_rows=20):\n    display(pred_df.sort(\"pred_30d\", descending=True))\n</pre> pred_df = pred_df.with_columns(inference_data[\"id\"]).select(     [\"id\", \"pred_10d\", \"pred_30d\"] )  # ensure predictions are between 0 and 1 pred_df = pred_df.with_columns(pl.col([\"pred_10d\", \"pred_30d\"]).clip(0, 1))   with pl.Config(tbl_rows=20):     display(pred_df.sort(\"pred_30d\", descending=True)) shape: (171, 3)idpred_10dpred_30dstrf32f32\"ENS\"0.640090.766982\"PEOPLE\"0.6550820.759009\"APE\"0.5530190.754386\"TRUMP\"0.5406690.751174\"FXS\"0.7462140.745177\"LDO\"0.4490980.717584\"LAYER\"0.5133930.710931\"TURBO\"0.5861660.700337\"BERA\"0.7088640.698633\"RENDER\"0.4407730.692121\u2026\u2026\u2026\"HYPER\"0.2799130.25558\"USUAL\"0.6078790.239557\"TRB\"0.4814520.223786\"FTT\"0.256450.202582\"LAUNCHCOIN\"0.6407410.19282\"IMX\"0.3221570.177864\"HMSTR\"0.5257980.175566\"kNEIRO\"0.1784020.085106\"INIT\"0.3973680.06886\"JTO\"0.5924580.0 In\u00a0[\u00a0]: Copied! <pre># directly submit a dataframe to slot 1\nclient.submit_predictions(df=pred_df, slot=1)\n</pre> # directly submit a dataframe to slot 1 client.submit_predictions(df=pred_df, slot=1) <pre>2025-07-25 16:48:45,241 - INFO - Wrote DataFrame to temporary file: submission.parquet\n2025-07-25 16:48:45,242 - INFO - Submitting predictions from submission.parquet to challenge 'hyperliquid-ranking' (Slot: 1)\n2025-07-25 16:48:45,839 - INFO - Successfully submitted predictions to challenge 'hyperliquid-ranking'\n</pre> Out[\u00a0]: <pre>{'match_info': {'matched_ids': 171,\n  'unmatched_ids': 0,\n  'message': '171 IDs matched inference data'},\n 'id': 814,\n 'username': 'jrai',\n 'challenge_slug': 'hyperliquid-ranking',\n 'inference_data_release_date': '2025-07-25T14:00:29.177700Z',\n 'submitted_at': '2025-07-25T16:48:45.671926Z',\n 'status': 'pending',\n 'slot': 1,\n 'score_details': None,\n 'percentile_details': None,\n 'prediction_file': 'https://cc-challenge-storage.s3.amazonaws.com/challenges/hyperliquid-ranking/submissions/4/57/submission_9QQtZah.parquet?AWSAccessKeyId=AKIATBWNDLY2W4VNIT4P&amp;Signature=Tlvu7YNhkONh2c52JQLjlQ1%2FQjQ%3D&amp;Expires=1753465725'}</pre>"},{"location":"tutorials/hyperliquid-end-to-end/#build-a-model-on-crowdcents-training-data-and-submit","title":"Build a model on CrowdCent's training data and submit\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#load-api-key","title":"Load API key\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#initialize-the-client","title":"Initialize the client\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#get-crowdcents-training-data","title":"Get CrowdCent's training data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#train-a-model-on-the-training-data","title":"Train a model on the training data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#get-crowdcents-latest-inference-data","title":"Get CrowdCent's latest inference data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#make-predictions-on-the-inference-data","title":"Make predictions on the inference data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#submit-to-the-hyperliquid-ranking-challenge-on-crowdcent","title":"Submit to the <code>hyperliquid-ranking</code> challenge on CrowdCent\u00b6","text":""},{"location":"tutorials/submission-automation/","title":"Submission Automation","text":""},{"location":"tutorials/submission-automation/#scheduling-a-kaggle-notebook","title":"Scheduling a Kaggle notebook","text":"<p>If you're just starting out, we recommend using Kaggle Notebooks to schedule your submissions.</p> <ol> <li>Settings (\u2699) \u2192 Schedule a notebook run \u2192 On </li> <li>Choose Frequency (daily / weekly / monthly), Start date, Runs \u2264 10 \u2192 Save </li> <li>A clock icon appears; each run writes a new Version with full logs &amp; outputs  </li> <li>Limits: CPU-only \u2022 \u2264 9 h per run \u2022 1 private / 5 public schedules active </li> <li>Pause or delete the job anytime from the same Settings card  </li> </ol> <p><sub>Need GPUs? Trigger notebook commits with the Kaggle API from cron/GitHub Actions.</sub></p>"},{"location":"tutorials/submission-automation/#scheduling-a-google-colab-vertex-ai-notebook","title":"Scheduling a Google Colab (Vertex AI) notebook","text":"<p>https://www.youtube.com/watch?v=ypGah2gRYck</p> <ol> <li>Create a Google Cloud account if you don't have one already</li> <li>Go to Google Colab Notebooks in Vertex AI</li> <li>Set up a schedule:</li> <li>Open your notebook in Colab</li> <li>Click Runtime \u2192 Manage sessions</li> <li>Select Recurring and configure your schedule</li> <li>Set frequency (daily/weekly/monthly) and duration</li> <li>Click Save</li> <li>Authentication options:</li> <li>Use service account keys stored securely</li> <li>Set up environment variables in the Vertex AI console</li> <li>Use Google Cloud's Secret Manager for API keys</li> </ol> <p><sub>Note: Scheduled Colab notebooks run on Google Cloud and may incur charges based on your usage.</sub></p>"}]}