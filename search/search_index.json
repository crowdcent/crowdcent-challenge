{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CrowdCent Challenge Documentation","text":"<p>Welcome to the documentation for the <code>crowdcent-challenge</code>. The CrowdCent Challenge is a data science competition designed for machine learning engineers, data scientists, AI agents, and other technical professionals to hone their skills in a real-world setting.</p> <p>Get started, learn about how we score submissions, explore the python client, or dive into the API reference.</p> <ul> <li> <p> Getting Started</p> <p>Make your first submission to the CrowdCent Challenge.</p> <p> Get Started</p> </li> <li> <p> Scoring System</p> <p>Learn how submissions are evaluated and how leaderboard rankings work.</p> <p> View Scoring Details</p> </li> <li> <p> Python Client Quick Start</p> <p>Install the client library and make your first submission programmatically.</p> <p> Quick Start Guide</p> </li> <li> <p> FAQ</p> <p>Frequently asked questions about the challenge, API Key, and more.</p> <p> View FAQ</p> </li> </ul>"},{"location":"#have-questions","title":"Have Questions?","text":"<p>If you need immediate assistance, please email us at info@crowdcent.com or join our Discord server.</p>"},{"location":"#want-to-contribute","title":"Want to Contribute?","text":"<p>These docs and the python client are open source! Anyone can submit to this documentation or improve the python client and CLI for the <code>crowdcent-challenge</code> API. Please see the contributing guidelines for more information. If you'd like to join the conversation, please join our Discord server.</p>"},{"location":"about/","title":"About CrowdCent","text":"<p>CrowdCent is on a mission to decentralize investment management by changing the way investment funds make decisions and allocate capital. We are the machine learning and coordination layer for online investment communities looking to turn their data into actionable, investable portfolios.</p> <p>More information about CrowdCent can be found on crowdcent.com. </p>"},{"location":"ai-agents-mcp/","title":"Using AI Agents","text":"<p>CrowdCent provides a Model Context Protocol (MCP) server that enables direct interaction with the CrowdCent Challenge API from AI agents and assistants like Cursor or Claude Desktop. This allows you to use natural language to perform challenge-related-tasks such as downloading data, training models, and submitting predictions.</p> <p>The MCP server is a separate, open-source project.</p> <ul> <li>Repository: crowdcent/crowdcent-mcp</li> </ul> <p></p>"},{"location":"ai-agents-mcp/#example-prompts","title":"Example Prompts","text":"<p>Once the server is running and your agent is configured, you can use natural language prompts.</p> <p>Example Prompts:</p> <pre><code>\"Download the latest CrowdCent training data and show me the first 5 rows.\"\n</code></pre> <pre><code>\"Submit predictions from 'predictions.parquet' to the CrowdCent challenge.\"\n</code></pre>"},{"location":"ai-agents-mcp/#installation","title":"Installation","text":"<ol> <li> <p>Clone the server repository:     <pre><code>git clone https://github.com/crowdcent/crowdcent-mcp.git\ncd crowdcent-mcp\n</code></pre></p> </li> <li> <p>Install dependencies:     The project uses <code>uv</code> for package management.     <pre><code>uv pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"ai-agents-mcp/#configuration","title":"Configuration","text":""},{"location":"ai-agents-mcp/#api-key","title":"API Key","text":"<p>The MCP server requires your CrowdCent API key.</p> <ol> <li>Create a <code>.env</code> file in the <code>crowdcent-mcp</code> directory.</li> <li>Add your key to the file:     <pre><code>CROWDCENT_API_KEY=your_api_key_here\n</code></pre></li> </ol> <p>You can get an API key from your CrowdCent Account Page.</p>"},{"location":"ai-agents-mcp/#agent-setup","title":"Agent Setup","text":"<p>You'll need to point your AI agent to the local MCP server.</p> CursorClaude Desktop <p>To integrate the MCP server with Cursor:</p> <ol> <li>Open your Cursor settings (<code>~/.cursor/mcp.json</code> or through the UI).</li> <li> <p>Add the following server configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"crowdcent-mcp\": {\n      \"command\": \"/path/to/your/uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\", \"/path/to/crowdcent-mcp\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n</code></pre> </li> </ol> <p>For Claude Desktop, add the following to your configuration file:</p> <ul> <li>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></li> <li>Linux: <code>~/.config/Claude/claude_desktop_config.json</code></li> </ul> <pre><code>{\n  \"mcpServers\": {\n    \"crowdcent-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\", \"/path/to/crowdcent-mcp\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Use Absolute Paths</p> <p>In all configurations, replace <code>/path/to/your/uv</code> and <code>/path/to/crowdcent-mcp</code> with the absolute paths on your system. For example, <code>uv</code> is often located at <code>~/.cargo/bin/uv</code>.</p> <p>For detailed troubleshooting, please refer to the crowdcent-mcp repository.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! The <code>crowdcent-challenge</code> API client and documentation are open-source projects and contributions can be as simple as a fact check or as complex as a new feature.</p> <p>Here's a breakdown of the standard GitHub workflow:</p> <ol> <li>Fork the repository</li> <li>Clone your fork:    <pre><code>git clone https://github.com/&lt;your-username&gt;/crowdcent-challenge.git\ncd crowdcent-challenge\nuv pip install -e .[dev]\n</code></pre></li> <li>Branch off:    <pre><code>git checkout -b my-feature\n</code></pre></li> <li>Make changes &amp; test</li> <li>Commit with clear messages:    <pre><code>git commit -m \"feat: Add new feature\"\n</code></pre></li> <li>Push &amp; open a PR from your fork     <pre><code>git push origin my-feature\n</code></pre></li> </ol> <p>Include tests where appropriate. Keep PRs focused - one feature/fix per PR.</p>"},{"location":"disclaimer/","title":"Disclaimer","text":"<p>Under no circumstances should any information provided in this software \u2014 or on associated distribution outlets \u2014 be construed as an offer soliciting the purchase or sale of any security or interest in any pooled investment vehicle sponsored, discussed, or mentioned by CrowdCent LLC or affiliates. Nor should it be construed as an offer to provide investment advisory services; an offer to invest in a CrowdCent investment vehicle will be made separately and only by means of the confidential offering documents of the specific pooled investment vehicles \u2014 which should be read in their entirety, and only to those who, among other requirements, meet certain qualifications under federal securities laws. Such investors, defined as accredited investors and qualified purchasers, are generally deemed capable of evaluating the merits and risks of prospective investments and financial matters. There can be no assurances that CrowdCent's investment objectives will be achieved or investment strategies will be successful. Any investment in a vehicle managed by CrowdCent involves a high degree of risk including the risk that the entire amount invested is lost. Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by CrowdCent and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results.</p>"},{"location":"equity-nlp/","title":"Equity NLP &#128683;","text":"<p>Coming soon!</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#general-getting-started","title":"General &amp; Getting Started","text":""},{"location":"faq/#what-is-the-crowdcent-challenge","title":"What is the CrowdCent Challenge?","text":"<p>The CrowdCent Challenge is a series of open data science competitions (challenges) focused on predicting investment/market outcomes. Participants use various datasets to build machine learning models that predict future returns over various time horizons. Submissions are used to create meta-models that can be turned into investable portfolios.</p>"},{"location":"faq/#how-do-i-get-started","title":"How do I get started?","text":"<p>Refer to the Installation &amp; Quick Start Guide for detailed steps.</p> <ol> <li>Install the client library: We recommend using uv: <code>uv pip install crowdcent-challenge</code>. Alternatively, use <code>pip install crowdcent-challenge</code>.</li> <li>Get an API Key: Visit your CrowdCent profile page and generate a new key. Save it securely.</li> <li>Set up Authentication: Provide your API key either when initializing the Python <code>ChallengeClient</code>, setting the <code>CROWDCENT_API_KEY</code> environment variable, or placing it in a <code>.env</code> file (<code>CROWDCENT_API_KEY=your_key_here</code>) in your project directory.</li> <li>Explore: Use the Python client or the <code>crowdcent</code> CLI to list available challenges (<code>crowdcent list-challenges</code>).</li> <li>Download Data: Choose a challenge and download the training and inference data using the client or CLI (e.g., <code>crowdcent download-training-data &lt;challenge_slug&gt; latest</code>).</li> <li>Build &amp; Submit: Train your model and submit your predictions in the required format.</li> </ol>"},{"location":"faq/#who-can-participate","title":"Who can participate?","text":"<p>The challenge is open to anyone interested in data science, machine learning, and finance. Check the terms of service for more details.</p>"},{"location":"faq/#api-key-authentication","title":"API Key &amp; Authentication","text":""},{"location":"faq/#how-do-i-get-an-api-key","title":"How do I get an API Key?","text":"<p>Go to your profile page on the CrowdCent website after logging in. Click the \"Generate New Key\" button.</p>"},{"location":"faq/#how-do-i-use-my-api-key","title":"How do I use my API Key?","text":"<p>The <code>crowdcent-challenge</code> library (both Python client and CLI) automatically looks for your API key in the following order:</p> <ol> <li>Passed directly to the <code>ChallengeClient</code> initializer (<code>api_key=...</code>).</li> <li>The <code>CROWDCENT_API_KEY</code> environment variable.</li> <li>A <code>.env</code> file in your current working directory containing <code>CROWDCENT_API_KEY=your_key_here</code>.</li> </ol>"},{"location":"faq/#what-if-my-api-key-doesnt-work","title":"What if my API key doesn't work?","text":"<p>Ensure you copied the key correctly and included the <code>ApiKey</code> prefix if using tools like Swagger UI directly (the client library handles this automatically). Verify it hasn't been revoked on your profile page. If issues persist, generate a new key or contact support.</p>"},{"location":"faq/#python-client-vs-cli","title":"Python Client vs. CLI","text":""},{"location":"faq/#whats-the-difference-between-the-challengeclient-python-and-the-crowdcent-cli","title":"What's the difference between the <code>ChallengeClient</code> (Python) and the <code>crowdcent</code> CLI?","text":"<p>They both interact with the same CrowdCent API.</p> <ul> <li><code>ChallengeClient</code> (Python): Designed for programmatic use within your modeling scripts or notebooks. Ideal for automating data downloads, processing, and prediction uploads.</li> <li><code>crowdcent</code> (CLI): A command-line tool for manual operations like listing challenges, downloading specific data files, checking submission status, etc., directly from your terminal.</li> </ul>"},{"location":"faq/#which-one-should-i-use","title":"Which one should I use?","text":"<p>Use the <code>ChallengeClient</code> within your Python code for automation and integration with your modeling workflow. Use the <code>crowdcent</code> CLI for quick checks, manual downloads, or exploring the available challenges and data without writing Python code.</p>"},{"location":"faq/#data","title":"Data","text":""},{"location":"faq/#what-format-is-the-data-provided-in","title":"What format is the data provided in?","text":"<p>All datasets (training data, inference features, meta-models) and submission files are in the Apache Parquet (<code>.parquet</code>) format. This columnar format is efficient for the type of data used in the challenge.</p>"},{"location":"faq/#what-kind-of-features-are-included","title":"What kind of features are included?","text":"<ul> <li>Important Note: Some challenges or training datasets might only provide target labels and identifiers (<code>id</code>, <code>date</code>, <code>target_10d</code>, <code>target_30d</code>). In these cases, participants are expected to source or engineer their own relevant features.</li> <li>Features are often renamed to simple names like <code>feature_1</code>, <code>feature_2</code>, etc. Refer to the specific challenge rules for details on the data provided for each competition.</li> </ul>"},{"location":"faq/#what-am-i-predicting","title":"What am I predicting?","text":"<p>Your goal is to to predict relative performance metrics (e.g., returns) for different future time horizons based on the provided features. The required prediction columns vary by challenge, but may look like <code>pred_10d</code>, <code>pred_30d</code>. Always check the specific challenge rules for the exact requirements.</p>"},{"location":"faq/#whats-the-difference-between-training-and-inference-data","title":"What's the difference between Training and Inference data?","text":"<ul> <li>Training Data: Used to train your models. Contains historical data, including target variables (e.g., <code>target_10d</code>, <code>target_30d</code>, ...) and identifiers (<code>id</code>, dates). It may also contain pre-computed features, but sometimes you will need to generate your own features based on the provided IDs and timestamps. Training datasets are versioned (e.g. v1.0, v1.1, etc.).</li> <li>Inference Data: Contains features (if provided by the challenge) and identifiers (<code>id</code>) for a new period but without the target labels. This is the data you use (along with any features you generate) to make predictions for submission. Inference data is released periodically.</li> </ul>"},{"location":"faq/#what-is-the-meta-model","title":"What is the 'Meta-Model'?","text":"<p>The meta-model typically represents an aggregation (e.g., an average or ensemble) of all valid user submissions for past inference periods within a specific challenge. It can serve as a benchmark or potentially as an additional feature for your own models. You can download it via the client or CLI.</p>"},{"location":"faq/#submissions","title":"Submissions","text":""},{"location":"faq/#what-format-does-my-submission-file-need-to-be","title":"What format does my submission file need to be?","text":"<p>Your submission must be a Parquet file containing an <code>id</code> column that matches the IDs from the corresponding inference dataset, and all the required prediction columns (e.g., <code>pred_1M</code>, <code>pred_3M</code>, etc.). All prediction columns must contain numeric values, and no missing values are allowed.</p>"},{"location":"faq/#how-do-i-submit-my-predictions","title":"How do I submit my predictions?","text":"<ul> <li>Python: Use <code>client.submit_predictions(\"path/to/your/predictions.parquet\")</code>.</li> <li>CLI: Use <code>crowdcent submit path/to/your/predictions.parquet</code>.</li> </ul> <p>If a submission window is currently open, your prediction is submitted immediately. If no window is open, your prediction is queued and will be automatically submitted when the next window opens.</p> <p>By default, submissions are also queued for the following period (auto-rollover). Use <code>queue_next=False</code> (Python) or <code>--no-queue-next</code> (CLI) to opt out.</p>"},{"location":"faq/#how-often-can-i-submit","title":"How often can I submit?","text":"<p>You can submit multiple times for an active inference period. Your latest valid submission before the deadline is the one that counts for scoring. Check the specific challenge rules.</p>"},{"location":"faq/#how-do-i-check-my-submission-status","title":"How do I check my submission status?","text":"<ul> <li>Python: Use <code>client.list_submissions()</code> (optionally filter by <code>period='current'</code> or <code>period='YYYY-MM-DD'</code>) and <code>client.get_submission(&lt;submission_id&gt;)</code>.</li> <li>CLI: Use <code>crowdcent list-submissions &lt;challenge_slug&gt;</code> (optionally filter with <code>--period current</code> or <code>--period YYYY-MM-DD</code>) and <code>crowdcent get-submission &lt;challenge_slug&gt; &lt;submission_id&gt;</code>. Statuses include \"pending\", \"processing\", \"evaluated\" (or \"scored\"), and \"error\" (or \"failed\").</li> </ul>"},{"location":"faq/#environment-troubleshooting","title":"Environment &amp; Troubleshooting","text":""},{"location":"faq/#what-version-of-python-should-i-use","title":"What version of Python should I use?","text":"<p>The client library requires Python 3.10 or higher (as specified in <code>pyproject.toml</code>).</p>"},{"location":"faq/#uv-or-pip","title":"<code>uv</code> or <code>pip</code>?","text":"<p>We recommend using <code>uv</code> for faster dependency management (<code>uv pip install ...</code>). However, standard <code>pip install ...</code> also works.</p>"},{"location":"faq/#where-can-i-get-help","title":"Where can I get help?","text":"<ul> <li>Discord: Join the CrowdCent Discord server.</li> <li>Email: Contact info@crowdcent.com.</li> </ul>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute","title":"How can I contribute?","text":"<p>Contributions to the <code>crowdcent-challenge</code> client library and its documentation are welcome! Please see the Contributing Guidelines for details on the standard GitHub workflow (fork, branch, commit, PR). </p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#register-for-a-crowdcent-account","title":"Register for a CrowdCent account","text":"<p>Sign up for a CrowdCent account here or sign in with your GitHub account. We require an email verification step to ensure the account is real. If you'd like to work with the challenge programmatically, you'll need to generate an API key from your user profile. See the client quickstart for more details.</p>"},{"location":"getting-started/#explore-challenges","title":"Explore Challenges","text":"<p>Once logged in, you'll land on the Challenge List page. Browse through the available challenges to find one that interests you. Each challenge card will give you a brief overview. Click on a challenge to see more details.</p> <p></p>"},{"location":"getting-started/#download-data","title":"Download Data","text":"<p>On the detail page for your chosen challenge (e.g. hyperliquid-ranking), you will find:</p> <ul> <li>A section to download the latest Training Data. You'll need this to train your model.</li> <li>Information about the current or most recent Inference Data period. If a period is active, you can download the inference features here.</li> </ul>"},{"location":"getting-started/#build-a-model","title":"Build a Model","text":"<p>Using the downloaded training data, build a model to predict the challenge target(s). You can refer to our tutorial notebooks (if available in the challenge description or docs) for examples.</p>"},{"location":"getting-started/#submit-predictions-during-an-inference-period","title":"Submit predictions during an Inference Period","text":"<ul> <li>The Challenge Detail page will display information about the current Inference Data period, including its release date and submission deadline.</li> <li>You have a specific number of submission slots (e.g., up to 5) for each inference period. You can choose which slot to use for each submission.</li> </ul> <p> The submission panel on the Challenge Detail page shows inference periods and your submission slots. In this submission panel image, we see that slots 1, 2, and 3 have successful submissions for the current inference period (lower-right check marks). However, only slots 1 and 3 have queued submissions for the next inference period (upper-right squares)</p> <p>There are two main ways to submit your predictions:</p>"},{"location":"getting-started/#1-via-the-website-ui","title":"1. Via the Website (UI)","text":"<ul> <li>Go to the Challenge Detail page.</li> <li>In the submission panel, select an available slot.</li> <li>Upload your prediction file (typically a Parquet file).</li> <li>Submissions are now flexible:<ul> <li>If the window is open, your file is submitted immediately. By default, it is also queued for the next period (auto-rollover).</li> <li>If the window is closed, your file is queued and will be automatically submitted when the next period opens.</li> </ul> </li> </ul>"},{"location":"getting-started/#2-programmatically-via-api","title":"2. Programmatically (via API)","text":"<ul> <li>Go to your User Profile page (accessible from the top navigation bar when logged in).</li> <li>In the \"API Keys\" section, you can generate a new API key. Give it a descriptive name. Store this key securely as it will not be shown again.</li> <li>Use this API key with the <code>crowdcent-challenge</code> Python package to submit your predictions. </li> <li>The client supports the same flexible behavior: submitting during an open window will also queue for the next period by default (<code>queue_next=True</code>). Submitting during a closed window will automatically queue.</li> </ul> <p>See our client quickstart guide for more details.</p>"},{"location":"getting-started/#3-via-ai-agents-mcp-server","title":"3. Via AI Agents (MCP Server)","text":"<p>It's also possible to interact with challenges using AI agents like Cursor or Claude Desktop. This provides a natural language interface for downloading data, submitting, and more. See our AI Agents (MCP) guide for setup instructions.</p>"},{"location":"getting-started/#wait-for-scores","title":"Wait for Scores","text":"<p>After an inference period's submission deadline passes, predictions will be evaluated. Your submission status and scores will be updated on your profile and the challenge leaderboard.</p> <p>For more details on how scores are calculated and what the scores mean, see the Scoring page.</p>"},{"location":"getting-started/#check-the-leaderboard","title":"Check the Leaderboard","text":"<p>Navigate to the Leaderboard page to see how your submissions rank against other participants for each challenge. You can switch the leaderboard by challenge, sort by different scores, and view results by user or by individual submission slots.</p> <p></p>"},{"location":"getting-started/#watch-the-meta-model","title":"Watch the Meta-Model","text":"<p>For some challenges, the meta-model is published after an inference period ends. For now, this is only available for the hyperliquid-ranking challenge and may be subject to change. The meta-model represents relative signals for the investable universe.</p> <p>Meta-Model Disclaimer</p> <p>The meta-model represents CrowdCent's aggregation of participant submissions into a single model. While we strive to create robust meta-models, please note:</p> <ul> <li>Meta-models are provided for informational purposes only and should not be construed as investment advice</li> <li>Past performance of meta-models is not indicative of future results</li> <li>Meta-model methodologies may change over time without notice</li> </ul> <p></p>"},{"location":"hyperliquid-ranking/","title":"Hyperliquid Ranking","text":"<p>Zero to Submission in 20 Seconds</p> <p>Go from setup to your first prediction submission with our end-to-end tutorial notebook.</p>"},{"location":"hyperliquid-ranking/#objective","title":"Objective","text":"<p>The Hyperliquid Ranking Challenge requires participants to rank crypto assets on the Hyperliquid decentralized derivatives exchange by their expected relative returns over the next 10 and 30 days. The challenge universe comprises approximately 165-175 (and likely more in the future) liquid tokens on the Hyperliquid protocol. This universe may change periodically, with tokens added or removed to ensure it remains as actionable as possible. If a token does not have enough volume or liquidity, it will likely be removed from the universe.</p> <pre><code>from crowdcent_challenge import ChallengeClient\nclient = ChallengeClient(challenge_slug=\"hyperliquid-ranking\")\nclient.get_challenge() # Get more challenge details\n</code></pre>"},{"location":"hyperliquid-ranking/#training-data","title":"Training data","text":"<p>The training dataset is created just to get you started. Simple models can be built with just the features and targets, but don't expect to win the challenge with them. We recommend building your own training datasets with sources like ccxt, eodhd, coingecko, or yfinance.</p> <p>You can download our training data, including features and targets from crowdcent.com/challenge/hyperliquid-ranking or via the CrowdCent client: <pre><code>client.download_training_dataset(\"latest\")\n</code></pre></p> id eodhd_id date feature_1_lag15 feature_1_lag10 ... feature_1_lag0 target_10d target_30d BABY BABY32198-USD.CC 2024-03-20 0.123 0.145 ... 0.823 0.15 0.25 OM OM-USD.CC 2024-03-20 0.456 0.423 ... 0.756 0.35 0.45 IOTA IOTA-USD.CC 2024-03-20 0.789 0.812 ... 0.923 0.55 0.65 MOODENG MOODENG33093-USD.CC 2024-03-20 0.234 0.267 ... 0.445 0.75 0.85 ENS ENS-USD.CC 2024-03-20 0.567 0.534 ... 0.678 0.95 0.88"},{"location":"hyperliquid-ranking/#asset-ids","title":"Asset IDs","text":"<p>We currently provide <code>id</code> (the hyperliquid id) and <code>eodhd_id</code> (the id to download via EODHD) for each asset. You can request we include additional id mappings from other data vendors in the inference data if data licenses allow.</p> <p>If you're using CrowdCent's training data to build your models, the inference data features will always match that of the latest training dataset version. The assets are generally the same as you would find in the training data, but new additions or removals are possible. For the inference data, we aim to track the listed and tradeable perps on Hyperliquid.</p>"},{"location":"hyperliquid-ranking/#features","title":"Features","text":"<p>The training data contains 80 total features following the pattern <code>feature_{n}_lag{lag}</code>:</p> <ul> <li>20 unique features (n = 1 to 20)</li> <li>4 lag values per feature (0, 5, 10, 15 days)</li> </ul> <p>Features with the same number represent the same metric at different time points. For example, <code>feature_1_lag0</code> through <code>feature_1_lag15</code> track the same underlying metric over time. This structure preserves temporal relationships, allowing you to build sequence models (LSTM, GRU, Transformer), identify trends/patterns across different time horizons, and engineer additional features based on temporal changes.</p>"},{"location":"hyperliquid-ranking/#targets","title":"Targets","text":"<p>Targets are the rankings of an asset's 10d and 30d forward relative returns (with a 1d lag). Targets do not currently take funding rate or any other factors (e.g. market cap, volume, etc.) into account. It's possible that the targets will be updated in the future to include such factors.</p> <p>Why the 1-day lag? For our purposes, the crypto universe has a close time of 24:00 UTC. At 14:00 UTC when predictions are made, only data through the previous day's 24:00 UTC close is available. The lag ensures predictions use only historical data while forecasting returns starting from tonight's close.</p> <p>Timeline: <pre><code>Day D-1: 24:00 UTC \u2192 Close price finalized (latest available data)\nDay D:   14:00 UTC \u2192 Inference pipeline starts\n         14:00-18:00 UTC \u2192 Inference period lasts 4 hours\n         24:00 UTC \u2192 Prediction/Scoring period starts (Day D close)\nDay D+10/30: 24:00 UTC \u2192 Prediction/Scoring period ends\n</code></pre></p> <ul> <li>Predictions rank assets by expected performance over the next 10/30 days starting from tonight's close</li> <li>Rankings are relative. They say nothing about the expected absolute performance of an asset.</li> </ul>"},{"location":"hyperliquid-ranking/#inference-data","title":"Inference data","text":"<ul> <li>Inference Period Open: The internal pipeline starts at 14:00\u00a0UTC. The file usually becomes available a few seconds to a few minutes later, once data quality checks pass.</li> <li>Inference Period Close: 4 hours after the actual release timestamp (typically around 18:00\u00a0UTC).</li> </ul> <p>Each day, an inference dataset is released containing the universe of tokens for which predictions are required. The inference data contains features but has no targets as they do not exist at the time of your submission. Your predictions will be scored against resolving targets from real market data in the future.</p> <p>Polling and parameter options</p> <p>Current vs Latest: Use <code>\"current\"</code> to download today's active inference period (for making submissions). Use <code>\"latest\"</code> to download the most recently published period (which may be from a previous day if today's isn't ready yet).</p> <p>Polling: The inference file may not exist the very instant the clock strikes 14:00\u00a0UTC. If you use <code>release_date=\"current\"</code> with default parameters, polling is handled automatically. For <code>release_date=\"latest\"</code>, no polling is needed since it always fetches the most recent available period.</p> <p>For manual polling, use <code>client.wait_for_inference_data(\"inference_data.parquet\")</code> or gently poll the API until <code>GET /current</code> stops returning <code>404</code>. </p> <p>To download inference data: <pre><code># Download the current active inference period (for today's submissions)\nclient.download_inference_data(\"current\") # THIS WILL FAIL IF THERE IS NOT AN OPEN INFERENCE PERIOD\n\n# Download the most recently available inference period\nclient.download_inference_data(\"latest\")\n\n# Download a specific date's inference data\nclient.download_inference_data(\"2024-12-15\")\n</code></pre></p> id eodhd_id feature_1_lag15 feature_1_lag10 feature_1_lag5 feature_1_lag0 ... BABY BABY32198-USD.CC 0.123 0.145 0.112 0.156 ... OM OM-USD.CC 0.456 0.423 0.467 0.401 ... IOTA IOTA-USD.CC 0.789 0.812 0.798 0.823 ... MOODENG MOODENG33093-USD.CC 0.234 0.267 0.223 0.289 ... ENS ENS-USD.CC 0.567 0.534 0.578 0.512 ... <p>Tip</p> <p>You do not need to use the features included in the inference data. You can use and are encouraged to use your own features. It may still be helpful to use our inference data and id mappings to use the same universe of assets.</p>"},{"location":"hyperliquid-ranking/#submitting-predictions","title":"Submitting predictions","text":"<p>Minimum of 80 ids from the inference data are required for a valid submission. The following columns are also required (no index):</p> <ul> <li><code>id</code>: The id of the asset on Hyperliquid.</li> <li><code>pred_10d</code>: A float between 0 and 1 representing the predicted rank for the 10-day horizon.</li> <li><code>pred_30d</code>: A float between 0 and 1 representing the predicted rank for the 30-day horizon.</li> </ul> <p>To submit predictions, you have 5 submission slots available. These can be used to submit multiple predictions for the same day and are defined by the <code>slot</code> parameter: <pre><code>client.submit_predictions(df=predictions_df, slot=1) # Submit a dataframe\nclient.submit_predictions(file_path=\"submission.parquet\", slot=2) # or a parquet file\n</code></pre></p> <p>Flexible Timing</p> <p>You can submit anytime. If the window is open, your submission is accepted immediately. If the window is closed, your submission is queued and automatically submitted when the next window opens. By default, submissions are also queued for the following period (auto-rollover) if you submit during an open window. Use <code>queue_next=False</code> to opt out.</p> id pred_10d pred_30d BABY 0.2 0.3 OM 0.4 0.5 IOTA 0.1 0.2 MOODENG 1.0 1.0 ENS 0.3 0.4 <p>Note</p> <p>If you are submitting a dataframe, <code>id</code> must be a column in the dataframe, not the index. If you are submitting a file, all submissions must be in parquet format.</p>"},{"location":"hyperliquid-ranking/#scoring-and-evaluation","title":"Scoring and Evaluation","text":"<p>Before scoring, for each prediction timeframe, ids are uniform ranked [0, 1], and any missing ids are filled with 0.5.</p>"},{"location":"hyperliquid-ranking/#metrics","title":"Metrics","text":"<p>1) Symmetric Normalized Discounted Cumulative Gain (NDCG@40)</p> <p>When you see NDCG@40, think: \"how well did I rank the top 40 assets and how well did I rank the bottom 40 assets?\" With ~170 tokens in the universe, k=40 represents approximately the top/bottom 20-25% of assets. This metric equally rewards both:</p> <ul> <li>Top 40 identification: Finding the tokens that will have the highest returns (for long positions)</li> <li>Bottom 40 identification: Finding the tokens that will have the lowest returns (for short positions or avoidance)</li> </ul> <p>The logarithmic discount means getting the #1 ranked token correct is much more valuable than getting the #40 ranked token correct. A perfect NDCG@40 score of 1.0 means you perfectly ranked both tails of the distribution. This metric is particularly valuable for portfolio construction where you want to maximize exposure to the best performers while avoiding or shorting the worst.</p> <p>Random Baseline</p> <p>Random predictions score approximately 0.55 for NDCG@40 with ~170 tokens, not 0.5. See the detailed explanation for why this happens.</p> <p>2) Spearman Correlation</p> <p>Spearman's rank correlation (\u03c1) measures how well your predicted ranks align with the true ranks across the entire universe of ~170 tokens. Unlike NDCG@40 which focuses on the 40 extremes, \u03c1 treats all rank positions in the entire universe equally.</p>"},{"location":"hyperliquid-ranking/#composite-percentile","title":"Composite Percentile","text":"<p>During the initial warm-up phase of the challenge, the goal is to maximize all metrics across all timeframes equally. Since NDCG@40 (0-1 range) and Spearman correlation (-1 to 1 range) have different scales and distributions, we use a composite percentile for fair comparison.</p> <p>The composite percentile is calculated as the average of your percentile rankings across all four metrics:</p> \\[ \\text{Composite Percentile} = \\frac{1}{4} \\times \\left( \\begin{array}{l} \\text{percentile(NDCG@40}_{10d}) + \\\\ \\text{percentile(NDCG@40}_{30d}) + \\\\ \\text{percentile(spearman}_{10d}) + \\\\ \\text{percentile(spearman}_{30d}) \\end{array} \\right) \\] <p>Where each percentile represents your ranking (0-100) compared to other participants for that specific metric for a given day/inference period.</p> <p>Important: Composite percentiles are only calculated when ten (10) or more valid submissions (counted by submission slots, not users) are received for a given day. If fewer than ten submissions are present, the composite percentile will not be calculated, and you'll need to look at absolute metric scores instead.</p> <p>As we learn more about the challenge's metamodel, we may adjust the weighting or add/remove metrics.</p>"},{"location":"hyperliquid-ranking/#score-ranges-and-percentile-rankings","title":"Score Ranges and Percentile Rankings","text":"<p>Raw Score Ranges:</p> <ul> <li>NDCG@40: Ranges from 0.0 (worst possible) to 1.0 (perfect ranking of top and bottom 40 assets)</li> <li>\u03c1 (Spearman's Rank Correlation): Ranges from -1.0 (perfect inverse ranking) to 1.0 (perfect ranking), with 0.0 indicating random performance</li> </ul> <p>Why Are Scores Typically Low?</p> <p>Financial markets are characterized by extremely high noise-to-signal ratios. Seemingly \"low\" scores can be quite competitive in this domain. Additionally, market regimes shift over time, causing the distribution of achievable scores to fluctuate significantly.</p> <p>Percentile Rankings: Your Most Reliable Metric</p> <p>CrowdCent calculates percentile rankings that show where you stand relative to other participants. These percentiles are recalculated daily.</p> <p>Tracking your percentile scores over time is often more informative than focusing on absolute scores, as it accounts for evolving competition and regime shifts that affect all participants. A model that consistently ranks in the 75th percentile across different market conditions can often be more valuable than one that occasionally achieves top scores but performs poorly in other regimes.</p> <p>Minimum submissions for percentile</p> <p>Percentiles only calculated when ten (10) or more valid submissions are received for a given day. If fewer than ten submissions are present, you'll see individual metric scores, but no percentile.</p>"},{"location":"hyperliquid-ranking/#meta-model","title":"Meta-Model","text":"<p>The CrowdCent Meta-Model aggregates predictions from all participants, representing a \"wisdom of the crowd\" which is made available to all users with a valid CrowdCent account. This may change in the future with no notice.</p> <p>Meta-Model Disclaimer</p> <p>Meta-model signals are released for informational and educational purposes only. Not financial, investment, or trading advice. CrowdCent disclaims all liability for any losses, damages, or consequences arising from use of the meta-model. Users assume all risks.</p>"},{"location":"hyperliquid-ranking/#construction-methodology","title":"Construction Methodology","text":"<p>The meta-model is constructed daily using a points-weighted average of all submissions:</p> <ol> <li>Uniform Ranking: Each individual submission's predictions are first converted to uniform rankings [0, 1] for each prediction column (<code>pred_10d</code>, <code>pred_30d</code>)</li> <li>Missing ID Handling: Any asset IDs missing from individual submissions are filled with neutral rankings of 0.5 after the uniform ranking step</li> <li>Average slots: Create a single prediction for each user by taking the arithmetic mean of all normalized rankings across all submission slots for each user.</li> <li>Points-Weighted Average: The final meta-model is created by taking a weighted average across all users, where each user's weight is proportional to their CC Points EMA (Exponential Moving Average with 7-day half-life).</li> </ol> <p>Users with more accumulated CC Points have greater influence on the meta-model. See CC Points System for details on how points are earned and how weights are calculated.</p>"},{"location":"hyperliquid-ranking/#access-and-downloads","title":"Access and Downloads","text":"<p>The meta-model is available through multiple channels:</p> <ul> <li>Via web: https://crowdcent.com/challenge/hyperliquid-ranking/meta-model/</li> <li>Via API: <code>client.download_meta_model(dest_path=\"meta_model.parquet\")</code></li> </ul> <p>The meta-model is a parquet file with the following columns. New predictions are added daily, creating a time series with multiple release dates as shown in this sample:</p> id pred_10d pred_30d release_date BTC 0.85 0.82 2024-12-15 ETH 0.74 0.78 2024-12-15 SOL 0.91 0.89 2024-12-15 BTC 0.85 0.82 2024-12-16 ETH 0.74 0.78 2024-12-16 SOL 0.91 0.83 2024-12-16 ... ... ... ..."},{"location":"install-quickstart/","title":"Install & Quick Start","text":""},{"location":"install-quickstart/#install-the-client","title":"Install the client","text":"Using uv (Recommended)Using pip <pre><code>uv add crowdcent-challenge\n</code></pre> <pre><code>pip install crowdcent-challenge\n</code></pre>"},{"location":"install-quickstart/#get-an-api-key","title":"Get an API Key","text":"<p>You need an API key to use the CrowdCent Challenge API. You can get your key by clicking \"Generate New Key\" on your profile page. Write it down, as you won't be able to access it after you leave the page.</p> <p></p>"},{"location":"install-quickstart/#authenticate-and-initialize-the-client","title":"Authenticate and Initialize the Client","text":"<p>The API client requires authentication using your API key. This can be provided directly or via environment variables. You can interact with the API using the Python client or the CLI.</p> PythonCLI <pre><code>from crowdcent_challenge import ChallengeClient, CrowdCentAPIError\n\nchallenge_slug = \"hyperliquid-ranking\"  # Replace with your challenge\napi_key = \"your_api_key_here\" # Replace with your actual key\nclient = ChallengeClient(challenge_slug=challenge_slug, api_key=api_key)\n</code></pre> <pre><code>export CROWDCENT_API_KEY=your_api_key_here # Set the environment variable\necho \"CROWDCENT_API_KEY=your_api_key_here\" &gt; .env # or create .env\n\n# Set the default challenge\ncrowdcent set-default-challenge hyperliquid-ranking\n\n# Check current default challenge\ncrowdcent get-default-challenge\n</code></pre> <p>Note</p> <p>With a default challenge set, you can run most commands without explicitly specifying the challenge. If you need to override the default for a specific command, use the <code>--challenge</code> or <code>-c</code> option.</p>"},{"location":"install-quickstart/#training-data","title":"Training Data","text":"<p>Access training datasets for a challenge, including listing available versions, getting the latest version, and downloading datasets.</p> PythonCLI <pre><code># List all training datasets for the current challenge\nclient.list_training_datasets()\n\n# Get details about the latest training dataset\nclient.get_training_dataset(\"latest\")\n\n# Download the training dataset file\nversion = \"latest\" # or specify a version like `1.0`\noutput_path = \"data/training_data.parquet\"\nclient.download_training_dataset(version, output_path)\n</code></pre> <pre><code># List all training datasets\ncrowdcent list-training-data\n\n# Get details about a specific training dataset version\ncrowdcent get-training-data 1.0\n\n# Download latest version\ncrowdcent download-training-data latest -o ./data/training_data.parquet\n\n# Or a specific version\ncrowdcent download-training-data 1.0 -o ./data/training_data.parquet\n</code></pre>"},{"location":"install-quickstart/#inference-data","title":"Inference Data","text":"<p>Manage inference periods and download inference features.</p> PythonCLI <pre><code># Get today's inference data (will wait/poll until published)\noutput_path = \"data/inference_features.parquet\"\nclient.download_inference_data(\"current\", output_path)  # polls every 30s by default\n\n# Get the most recent available data (no waiting)\nclient.download_inference_data(\"latest\", output_path)\n\n# Get data for a specific date\nclient.download_inference_data(\"2025-01-15\", output_path)\n</code></pre> <pre><code># List all inference data periods\n# Download today's inference data (will poll until available)\ncrowdcent download-inference-data current -o ./data/inference_features.parquet\n\n# Download most recent available data (no waiting)\ncrowdcent download-inference-data latest -o ./data/inference_features.parquet\n\n# Download specific date\ncrowdcent download-inference-data 2025-01-15 -o ./data/inference_features.parquet\n</code></pre> <p>Choosing the Right Option</p> <ul> <li>Use <code>\"current\"</code> in your daily prediction workflow when you need today's features</li> <li>Use <code>\"latest\"</code> for when you need immediate access to the most recent inference period even if it's closed</li> <li>Use <code>YYYY-MM-DD</code> when working with historical periods or debugging</li> </ul>"},{"location":"install-quickstart/#meta-model","title":"Meta-Model","text":"<p>Download the consolidated meta-model for a challenge. The meta-model typically represents an aggregation of all valid user submissions for past inference periods.</p> PythonCLI <pre><code>output_path = \"data/meta_model.parquet\"\nclient.download_meta_model(output_path)\n</code></pre> <pre><code>crowdcent download-meta-model -o ./data/meta_model.parquet\n</code></pre>"},{"location":"install-quickstart/#submitting-predictions","title":"Submitting Predictions","text":"<p>Submit your model's predictions for a challenge. The file must include an <code>id</code> column and the specific prediction columns required by the challenge (e.g., <code>pred_10d</code>, <code>pred_30d</code> for some challenges, or <code>pred_1M</code>, <code>pred_3M</code>, etc., for others). Always check the specific challenge documentation for the exact column names.</p> <p>Flexible Submission Timing</p> <ul> <li>Window open: Your submission is accepted immediately. By default, it is also queued for the next period (auto-rollover).</li> <li>Window closed: Your submission is queued and will be automatically submitted when the next window opens.</li> </ul> <p>Use <code>queue_next=False</code> (Python) or <code>--no-queue-next</code> (CLI) to opt out of auto-rollover during open windows.</p> PythonCLI <pre><code>import polars as pl\nimport numpy as np\nfrom joblib import load\n\n# Create or load your predictions\ninference_data = pl.read_parquet(\"inference_data.parquet\")\nmodel = load(\"model.joblib\")\npredictions = model.predict(inference_data)\npred_df = pl.from_numpy(predictions, [\"pred_10d\", \"pred_30d\"])\ninference_data = inference_data.with_columns(pred_df)\n\n# Save predictions to a Parquet file\nsubmission_file = \"submission.parquet\"\npredictions.write_parquet(submission_file)\n\n# You can specify a submission slot (1-5), default is 1\nclient.submit_predictions(file_path=submission_file, slot=2)\n\n# Or submit a DataFrame directly (without saving to file first)\nclient.submit_predictions(df=pred_df)\n\n# Opt out of auto-rollover (don't queue for next period)\nclient.submit_predictions(df=pred_df, queue_next=False)\n</code></pre> <pre><code># Submit predictions to the default challenge (uses slot 1)\ncrowdcent submit submission.parquet\n\n# Submit to a specific slot (1-5)\ncrowdcent submit submission.parquet --slot 2\n\n# Submit to a specific challenge (overriding default)\ncrowdcent submit submission.parquet --challenge hyperliquid-ranking --slot 3\n\n# Opt out of auto-rollover\ncrowdcent submit submission.parquet --no-queue-next\n</code></pre>"},{"location":"install-quickstart/#retrieving-submissions","title":"Retrieving Submissions","text":"<p>Manage and review your submissions for a challenge, including listing all submissions, filtering by period, and getting details for a specific submission.</p> PythonCLI <pre><code># List your submissions for the current challenge\nclient.list_submissions()\n\n# Filter submissions by period\n# Get submissions for the current period only\nclient.list_submissions(period=\"current\")\n\n# Or for a specific period\nclient.list_submissions(period=\"2025-01-15\")\n\n# Get details for a specific submission\nsubmission_id = 123  # Replace with actual submission ID\nsubmission = client.get_submission(submission_id)\nif submission['score_details']:\n    print(f\"Score Details: {submission['score_details']}\")\n</code></pre> <pre><code># List all submissions\ncrowdcent list-submissions\n\n# Filter by period\ncrowdcent list-submissions --period current\ncrowdcent list-submissions --period 2025-01-15\n\n# Get details about a specific submission\ncrowdcent get-submission 123\n</code></pre>"},{"location":"install-quickstart/#track-your-performance","title":"Track Your Performance","text":"<pre><code>history = client.get_performance()  # List of scored submissions with scores &amp; percentiles\n</code></pre> <p>See the Track Your Performance tutorial for analysis and visualization examples.</p>"},{"location":"install-quickstart/#challenges","title":"Challenges","text":"<p>Get details for a challenge or switch between different challenges.</p> PythonCLI <pre><code>challenges = ChallengeClient.list_all_challenges()\n\n# Get details for the current challenge\nchallenge = client.get_challenge()\n\n# Switch to a different challenge\nnew_challenge_slug = \"another-challenge\"  # Replace with another actual challenge slug\nclient.switch_challenge(new_challenge_slug) # Now all operations will be for the new challenge\n</code></pre> <pre><code># List all available challenges\ncrowdcent list-challenges\n\n# Get details for the default challenge\ncrowdcent get-challenge\n\n# Or specify a challenge explicitly\ncrowdcent get-challenge --challenge hyperliquid-ranking\n\n# Switch to a different default challenge\ncrowdcent set-default-challenge another-challenge\n</code></pre> <p>If you need to work with multiple challenges simultaneously, we recommend using multiple client instances.</p> PythonCLI <pre><code># Initialize clients for different challenges\nclient_a = ChallengeClient(challenge_slug=\"challenge-a\")\nclient_b = ChallengeClient(challenge_slug=\"challenge-b\")\n\n# Use each client for its respective challenge\ndataset_a = client_a.get_training_dataset(\"latest\")\ndataset_b = client_b.get_training_dataset(\"latest\")\n</code></pre> <pre><code># Set default challenge training data\ncrowdcent get-training-data\n\n# Switch to a different challenge\ncrowdcent get-training-data --challenge challenge-a\ncrowdcent get-training-data --challenge challenge-b\n</code></pre> <p>CLI vs Python Approach</p> <p>The CLI doesn't have a direct equivalent to <code>client.switch_challenge()</code> because each CLI command is independent. Instead, use <code>set-default-challenge</code> to change your default, or use <code>--challenge</code> to override the default for specific commands. This approach is often more convenient for CLI usage.</p>"},{"location":"points-system/","title":"CrowdCent Points (CC Points)","text":"<p>The CrowdCent Points (CC Points) system rewards consistent participation and high-quality predictive performance.</p>"},{"location":"points-system/#summary","title":"Summary","text":"<ul> <li>Participation: Earn a base credit that grows with your daily streak.</li> <li>Performance: Earn the majority of points based on the composite percentile of your daily predictions.</li> <li>Influence: Your points translate directly to your weight in the Meta-Model.</li> </ul>"},{"location":"points-system/#daily-base-credit","title":"Daily Base Credit","text":"<p>You earn a guaranteed 0.5 points just for participating, provided you make at least one valid submission during the daily window.</p> <p>Maintaining a daily streak increases your base credit:</p> Streak Length Multiplier Daily Base Days 1\u201329 1.0x 0.5 pts Days 30\u201389 1.5x 0.75 pts Days 90+ 2.0x 1.0 pts <p>Missing a submission window resets your streak to 0.</p> <p>Auto-Rollover Helps Maintain Streaks</p> <p>By default, submissions are automatically queued for the next period (auto-rollover). This means you can submit once and have it count for the next day, helping maintain your streak for infrastructure hiccups.</p>"},{"location":"points-system/#performance-adjustment-the-core-driver","title":"Performance Adjustment (The Core Driver)","text":"<p>Your primary source of points is the quality of your predictions relative to other participants.</p> <p>At the end of the day, all your valid slots are evaluated against the target metrics. We calculate the composite percentile of all your evaluated slots, then apply a cosine curve to this percentile.</p>"},{"location":"points-system/#the-curve","title":"The Curve","text":"<p>The curve rewards consistency\u2014moving from \"average\" (50th) to \"good\" (70th) yields the steepest point gains. Bad days are penalized, but less severely than a linear model. The curve flattens at the extremes to discourage variance.</p> <p></p> <p>Formula:</p> \\[ \\text{Adjustment} = -\\cos\\left(\\text{AvgPercentile} \\times \\frac{\\pi}{100}\\right) \\times 10 \\] <p>Key Benchmarks:</p> <ul> <li>0th percentile: -10.0 pts</li> <li>50th percentile: 0.0 pts</li> <li>60th percentile: +3.1 pts</li> <li>70th percentile: +5.9 pts</li> <li>80th percentile: +8.1 pts</li> <li>100th percentile: +10.0 pts</li> </ul>"},{"location":"points-system/#final-daily-score","title":"Final Daily Score","text":"<p>Your final score for the day cannot be negative.</p> \\[ \\text{Daily Points} = \\max(0, (\\text{Base Credit} \\times \\text{Streak Multiplier}) + \\text{Performance Adjustment}) \\] <p>Example (Assuming &gt;90 day streak = 1.0 base):</p> Scenario Avg Percentile Base (w/ Mult) Adjustment Calculation Final Points Top Tier 80th 1.0 +8.1 <code>1.0 + 8.1</code> 9.1 Consistent 70th 1.0 +5.9 <code>1.0 + 5.9</code> 6.9 Average 50th 1.0 0.0 <code>1.0 + 0.0</code> 1.0 Poor Day 20th 1.0 -8.1 <code>1.0 - 8.1</code> 0.0"},{"location":"points-system/#tier-system","title":"Tier System","text":"<p>As you accumulate CC Points, you'll progress through 5 tiers\u2014each with a unique badge displayed on your profile and in the leaderboard:</p> Tier Points Required Description Citizen &gt;0 Welcome to CrowdCent Challenger 100+ Active participant Contender 500+ Rising competitor Centurion 1,500+ Commander of predictions Sovereign 5,000+ Pinnacle of mastery <p>Higher tiers unlock greater recognition in the community and demonstrate your track record of consistent, quality predictions.</p>"},{"location":"points-system/#the-meta-model-why-points-matter","title":"The Meta-Model: Why Points Matter","text":"<p>The ultimate goal of the CrowdCent Challenge is to build a meta-model\u2014an aggregate of all participants that outperforms any single participant on their own.</p> <p>Your Points = Your Weight. The influence of your predictions on the meta-model is proportional to the Exponential Moving Average (EMA) of your Total Points history. Recent performance is weighted significantly higher than past performance, so you cannot \"rest on your laurels.\" To maintain high influence, you must perform consistently.</p> <p></p> <p>Formula:</p> \\[ \\text{Weight}_u = \\sum_{t=0}^{\\text{today}} \\text{DailyPoints}_{u,t} \\times (0.5)^{\\frac{\\text{days\\_ago}}{7}} \\]"},{"location":"points-system/#program-terms","title":"Program Terms","text":"<p>CrowdCent reserves the right to modify, suspend, or cancel the points system, scoring rules, and reward structures at any time without prior notice.</p> <p>This includes, but is not limited to:</p> <ul> <li>Adjusting point values, multipliers, or calculation formulas retroactively or proactively.</li> <li>Removing points from users found to be in violation of the spirit of the competition.</li> <li>Changing the weighting mechanisms for the meta-model.</li> <li>Resetting point totals for specific challenges or globally.</li> </ul> <p>Participation in the points program does not create any property rights or contractual obligations. Points have no monetary value and are used solely for ranking and weighting purposes within the CrowdCent ecosystem.</p>"},{"location":"scoring/","title":"Scoring","text":"<p>All scoring functions used in the CrowdCent Challenge can be found in the <code>crowdcent_challenge.scoring</code> module.</p> <pre><code>from crowdcent_challenge.scoring import *\n</code></pre>"},{"location":"scoring/#symmetric-ndcgk","title":"Symmetric NDCG@k","text":"<p>One of the key metrics used in some challenges is Symmetric Normalized Discounted Cumulative Gain (Symmetric NDCG@k).</p>"},{"location":"scoring/#concept","title":"Concept","text":"<p>Normalized Discounted Cumulative Gain (NDCG@k) is a metric used to evaluate how well a model ranks items. It assesses the quality of the top k predictions by:</p> <ol> <li>Giving higher scores for ranking truly relevant items higher.</li> <li>Applying a logarithmic discount to items ranked lower (meaning relevance at rank 1 is more important than relevance at rank 10).</li> <li>Normalizing the score by the best possible ranking (IDCG) to get a value between 0 and 1.</li> </ol> <p>However, this commonly used metric only focuses on the top items in a list. In finance, identifying the worst performers (lowest true values) can be just as important as identifying the best.</p> <p>Our metric of Symmetric NDCG@k addresses this by evaluating ranking performance at both ends of the list:</p> <ol> <li>Top Performance: It calculates the standard <code>NDCG@k</code> based on your predicted scores (<code>y_pred</code>) compared to the actual true values (<code>y_true</code>). This measures how well you identify the items with the highest true values.</li> <li>Bottom Performance: It calculates another <code>NDCG@k</code> focused on the lowest ranks. It does this by:<ul> <li>Inverting both true values and predictions using <code>1 - value</code> transformation</li> <li>This makes originally low values (close to 0) become high values (close to 1), so standard NDCG rewards finding the originally lowest items</li> <li>Calculating <code>NDCG@k</code> for how well your lowest predictions match the items with the lowest true values</li> </ul> </li> <li>Averaging: The final <code>symmetric_ndcg_at_k</code> score is the simple average of the Top NDCG@k and the Bottom NDCG@k. <code>(NDCG_top + NDCG_bottom) / 2</code>.</li> </ol>"},{"location":"scoring/#calculation","title":"Calculation","text":"<p>The Symmetric NDCG@k is calculated as:</p> <ol> <li>Top NDCG@k: Calculate standard NDCG@k using true values and predicted scores</li> <li>Bottom NDCG@k: Invert both true values and predictions using <code>1 - value</code>, then calculate NDCG@k</li> <li>Final Score: Average of top and bottom NDCG@k scores: <code>(NDCG_top + NDCG_bottom) / 2</code></li> </ol> <p>The standard NDCG formula includes:</p> <ul> <li>DCG@k = \u03a3(relevance_i / log\u2082(i+1)) for i=1 to k</li> <li>IDCG@k = DCG@k for ideal ranking</li> <li>NDCG@k = DCG@k / IDCG@k</li> </ul>"},{"location":"scoring/#interpretation","title":"Interpretation","text":"<p>Notably, Symmetric NDCG@k does not give 0.5 for random predictions, but ~0.55 for our default k=40. Understanding the random baseline is crucial for interpreting your scores.</p> <ul> <li>NDCG@k = 1: perfect performance at identifying both the top k best and bottom k worst items according to their true values.</li> <li>NDCG@k = 0.55: random guessing.</li> <li>NDCG@k = 0: no overlap with top k or bottom k.</li> </ul> <p>How k Affects the Random Baseline:</p> <p>Key insights:</p> <ul> <li>Random baseline starts near 0.5 for small k and increases monotonically</li> <li>For Hyperliquid (k=40, n\u2248170-200), random predictions score ~0.55</li> </ul>"},{"location":"scoring/#usage","title":"Usage","text":"<pre><code>from crowdcent_challenge.scoring import symmetric_ndcg_at_k\nimport numpy as np\n\n# Example data\ny_true = np.array([0.1, 0.2, 0.9, 0.3, 0.7])\ny_pred = np.array([0.2, 0.1, 0.8, 0.4, 0.6])\nk = 3\n\nscore = symmetric_ndcg_at_k(y_true, y_pred, k)\n</code></pre> <p>This metric provides a more holistic view of ranking performance when both high and low extremes are important.</p>"},{"location":"scoring/#spearman-correlation","title":"Spearman Correlation","text":"<p>Spearman's rank correlation coefficient is a non-parametric measure of rank correlation that assesses how well the relationship between two variables can be described using a monotonic function.</p>"},{"location":"scoring/#concept_1","title":"Concept","text":"<p>In the context of ranking challenges, Spearman correlation measures how well your predicted rankings align with the true rankings. Unlike Pearson correlation (which measures linear relationships), Spearman correlation:</p> <ol> <li>Works with ranks: It converts both predicted and true values to ranks before computing correlation</li> <li>Captures monotonic relationships: Perfect Spearman correlation (\u00b11) means perfect monotonic relationship, even if not linear</li> <li>Robust to outliers: Since it uses ranks rather than raw values, extreme values have less influence</li> </ol>"},{"location":"scoring/#calculation_1","title":"Calculation","text":"<p>The Spearman correlation coefficient (\u03c1) is calculated as:</p> <ul> <li>First, convert both <code>y_true</code> and <code>y_pred</code> to ranks</li> <li>Then calculate the Pearson correlation coefficient on these ranks</li> <li>Formula: \u03c1 = 1 - (6 \u00d7 \u03a3d\u00b2) / (n \u00d7 (n\u00b2 - 1)), where d is the difference between paired ranks</li> </ul>"},{"location":"scoring/#interpretation_1","title":"Interpretation","text":"<ul> <li>\u03c1 = 1: Perfect positive correlation (your rankings perfectly match the true rankings)</li> <li>\u03c1 = 0: No correlation (your rankings are unrelated to the true rankings)  </li> <li>\u03c1 = -1: Perfect negative correlation (your rankings are exactly reversed)</li> </ul>"},{"location":"scoring/#usage_1","title":"Usage","text":"<pre><code>from scipy.stats import spearmanr\nimport numpy as np\n\n# Example data\ny_true = np.array([1.0, 0.5, 0.3, 0.2, 0.1])  # True values (will be ranked)\ny_pred = np.array([0.9, 0.6, 0.25, 0.22, 0.05])    # Predicted values (will be ranked)\n\n# Calculate Spearman correlation\ncorrelation, p_value = spearmanr(y_true, y_pred)\n</code></pre>"},{"location":"api-reference/cli/","title":"CLI Reference","text":"<p>This page provides auto-generated documentation for the <code>crowdcent</code> CLI for the CrowdCent Challenge.</p>"},{"location":"api-reference/cli/#crowdcent","title":"crowdcent","text":"<p>Command Line Interface for the CrowdCent Challenge.</p> <p>Usage:</p> <pre><code>crowdcent [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#download-inference-data","title":"download-inference-data","text":"<p>Usage:</p> <pre><code>crowdcent download-inference-data [OPTIONS] [RELEASE_DATE]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT     Challenge slug (uses default if not specified)\n  -o, --output TEXT        Output file path. Defaults to\n                           [challenge_slug]_inference_[release_date].parquet\n                           in current directory.\n  --no-poll                Disable polling when waiting for the current\n                           inference data to be published.\n  --poll-interval INTEGER  Seconds to wait between polling attempts when\n                           release_date='current'.  [default: 30]\n  --timeout INTEGER        Maximum seconds to wait when polling for current\n                           data (0 = wait indefinitely).  [default: 900]\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#download-meta-model","title":"download-meta-model","text":"<p>Usage:</p> <pre><code>crowdcent download-meta-model [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  -o, --output TEXT     Output file path. Defaults to\n                        [challenge_slug]_meta_model.parquet in current\n                        directory.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#download-training-data","title":"download-training-data","text":"<p>Usage:</p> <pre><code>crowdcent download-training-data [OPTIONS] [VERSION]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  -o, --output TEXT     Output file path. Defaults to\n                        [challenge_slug]_training_v[version].parquet in\n                        current directory.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-challenge","title":"get-challenge","text":"<p>Usage:</p> <pre><code>crowdcent get-challenge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-default-challenge","title":"get-default-challenge","text":"<p>Show the current default challenge slug.</p> <p>Usage:</p> <pre><code>crowdcent get-default-challenge [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-inference-data","title":"get-inference-data","text":"<p>Usage:</p> <pre><code>crowdcent get-inference-data [OPTIONS] RELEASE_DATE\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-submission","title":"get-submission","text":"<p>Usage:</p> <pre><code>crowdcent get-submission [OPTIONS] SUBMISSION_ID\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#get-training-data","title":"get-training-data","text":"<p>Usage:</p> <pre><code>crowdcent get-training-data [OPTIONS] [VERSION]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-challenges","title":"list-challenges","text":"<p>Usage:</p> <pre><code>crowdcent list-challenges [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-inference-data","title":"list-inference-data","text":"<p>Usage:</p> <pre><code>crowdcent list-inference-data [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-submissions","title":"list-submissions","text":"<p>Usage:</p> <pre><code>crowdcent list-submissions [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --period TEXT         Filter submissions by period: 'current' or a date in\n                        'YYYY-MM-DD' format\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#list-training-data","title":"list-training-data","text":"<p>Usage:</p> <pre><code>crowdcent list-training-data [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT  Challenge slug (uses default if not specified)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#set-default-challenge","title":"set-default-challenge","text":"<p>Usage:</p> <pre><code>crowdcent set-default-challenge [OPTIONS] CHALLENGE_SLUG\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"api-reference/cli/#submit","title":"submit","text":"<p>Usage:</p> <pre><code>crowdcent submit [OPTIONS] FILE_PATH\n</code></pre> <p>Options:</p> <pre><code>  -c, --challenge TEXT            Challenge slug (uses default if not\n                                  specified)\n  --slot INTEGER                  Submission slot number (1-based). Defaults\n                                  to 1.\n  --queue-next / --no-queue-next  Also queue for the next period (auto-\n                                  rollover). Defaults to --queue-next.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"api-reference/python/","title":"Python API Client","text":"<p>This page provides auto-generated documentation from the client library's docstrings.</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient","title":"<code>crowdcent_challenge.client.ChallengeClient</code>","text":"<p>Client for interacting with a specific CrowdCent Challenge.</p> <p>Handles authentication and provides methods for accessing challenge data, training datasets, inference data, and managing prediction submissions for a specific challenge identified by its slug.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>class ChallengeClient:\n    \"\"\"\n    Client for interacting with a specific CrowdCent Challenge.\n\n    Handles authentication and provides methods for accessing challenge data,\n    training datasets, inference data, and managing prediction submissions for\n    a specific challenge identified by its slug.\n    \"\"\"\n\n    DEFAULT_BASE_URL = \"https://crowdcent.com/api\"\n    API_KEY_ENV_VAR = \"CROWDCENT_API_KEY\"\n\n    def __init__(\n        self,\n        challenge_slug: str,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the ChallengeClient for a specific challenge.\n\n        Args:\n            challenge_slug: The unique identifier (slug) for the challenge.\n            api_key: Your CrowdCent API key. If not provided, it will attempt\n                     to load from the CROWDCENT_API_KEY environment variable\n                     or a .env file.\n            base_url: The base URL of the CrowdCent API. Defaults to\n                      https://crowdcent.com/api.\n        \"\"\"\n        load_dotenv()  # Load .env file if present\n        self.api_key = api_key or os.getenv(self.API_KEY_ENV_VAR)\n        if not self.api_key:\n            raise AuthenticationError(\n                f\"API key not provided and not found in environment variable \"\n                f\"'{self.API_KEY_ENV_VAR}' or .env file.\"\n            )\n\n        self.challenge_slug = challenge_slug\n        self.base_url = (base_url or self.DEFAULT_BASE_URL).rstrip(\"/\")\n        self.session = requests.Session()\n        self.session.headers.update({\"Authorization\": f\"Api-Key {self.api_key}\"})\n        logger.info(\n            f\"ChallengeClient initialized for '{challenge_slug}' at URL: {self.base_url}\"\n        )\n\n    def _request(\n        self,\n        method: str,\n        endpoint: str,\n        params: Optional[Dict] = None,\n        json_data: Optional[Dict] = None,\n        files: Optional[Dict[str, IO]] = None,\n        stream: bool = False,\n        data: Optional[Dict] = None,\n        max_retries: int = 3,\n        retry_delay: float = 1.0,\n    ) -&gt; requests.Response:\n        \"\"\"\n        Internal helper method to make authenticated API requests.\n\n        Args:\n            method: HTTP method (e.g., 'GET', 'POST').\n            endpoint: API endpoint path (e.g., '/challenges/').\n            params: URL parameters.\n            json_data: JSON data for the request body.\n            files: Files to upload (for multipart/form-data).\n            stream: Whether to stream the response (for downloads).\n            data: Dictionary of form data to send with multipart requests.\n            max_retries: Maximum number of retry attempts for connection errors.\n            retry_delay: Initial delay between retries (seconds). Will use exponential backoff.\n\n        Returns:\n            The requests.Response object.\n\n        Raises:\n            AuthenticationError: If the API key is invalid (401).\n            NotFoundError: If the resource is not found (404).\n            ClientError: For other 4xx errors.\n            ServerError: For 5xx errors.\n            CrowdCentAPIError: For other request exceptions.\n        \"\"\"\n        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n        logger.debug(\n            f\"Request: {method} {url} Params: {params} JSON: {json_data is not None} \"\n            f\"Data: {data is not None} Files: {files is not None}\"\n        )\n\n        for attempt in range(max_retries + 1):\n            try:\n                response = self.session.request(\n                    method,\n                    url,\n                    params=params,\n                    json=json_data,\n                    files=files,\n                    stream=stream,\n                    data=data,\n                )\n                response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n                logger.debug(f\"Response: {response.status_code}\")\n                return response\n            except requests_exceptions.HTTPError as e:\n                status_code = e.response.status_code\n\n                # Try to parse standardized error format: {\"error\": {\"code\": \"ERROR_CODE\", \"message\": \"Description\"}}\n                try:\n                    error_data = e.response.json()\n                    if \"error\" in error_data and isinstance(error_data[\"error\"], dict):\n                        error_code = error_data[\"error\"].get(\"code\", \"UNKNOWN_ERROR\")\n                        error_message = error_data[\"error\"].get(\n                            \"message\", e.response.text\n                        )\n                    else:\n                        error_code = \"API_ERROR\"\n                        error_message = e.response.text\n                except requests_exceptions.JSONDecodeError:\n                    error_code = \"API_ERROR\"\n                    error_message = e.response.text\n\n                logger.error(\n                    f\"API Error ({status_code}): {error_code} - {error_message} for {method} {url}\"\n                )\n\n                if status_code == 401:\n                    raise AuthenticationError(\n                        f\"Authentication failed (401): {error_message} [{error_code}]\"\n                    ) from e\n                elif status_code == 404:\n                    raise NotFoundError(\n                        f\"Resource not found (404): {error_message} [{error_code}]\"\n                    ) from e\n                elif 400 &lt;= status_code &lt; 500:\n                    raise ClientError(\n                        f\"Client error ({status_code}): {error_message} [{error_code}]\"\n                    ) from e\n                elif 500 &lt;= status_code &lt; 600:\n                    raise ServerError(\n                        f\"Server error ({status_code}): {error_message} [{error_code}]\"\n                    ) from e\n                else:\n                    raise CrowdCentAPIError(\n                        f\"HTTP error ({status_code}): {error_message} [{error_code}]\"\n                    ) from e\n            except (\n                requests_exceptions.ConnectionError,\n                requests_exceptions.Timeout,\n            ) as e:\n                # Connection errors and timeouts are retryable\n                if attempt &lt; max_retries:\n                    delay = retry_delay * (2**attempt)  # Exponential backoff\n                    logger.warning(\n                        f\"Connection error: {e}. Retrying in {delay:.1f}s... \"\n                        f\"(attempt {attempt + 1}/{max_retries})\"\n                    )\n                    time.sleep(delay)\n                    continue\n                logger.error(\n                    f\"Request failed after {max_retries} retries: {e} for {method} {url}\"\n                )\n                raise CrowdCentAPIError(\n                    f\"Request failed after {max_retries} retries: {e}\"\n                ) from e\n            except requests_exceptions.RequestException as e:\n                logger.error(f\"Request failed: {e} for {method} {url}\")\n                raise CrowdCentAPIError(f\"Request failed: {e}\") from e\n\n    def _download_file(self, endpoint: str, dest_path: str, description: str) -&gt; None:\n        \"\"\"Download a file from the API with progress bar.\n\n        Args:\n            endpoint: API endpoint to download from.\n            dest_path: Local file path to save to.\n            description: Human-readable description for logging (e.g., \"training data v1.0\").\n        \"\"\"\n        logger.info(f\"Downloading {description} to {dest_path}\")\n        response = self._request(\"GET\", endpoint, stream=True)\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        try:\n            from tqdm import tqdm\n\n            with open(dest_path, \"wb\") as f:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(dest_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        f.write(chunk)\n                        pbar.update(len(chunk))\n            logger.info(f\"Successfully downloaded {description} to {dest_path}\")\n        except IOError as e:\n            logger.error(f\"Failed to write to {dest_path}: {e}\")\n            raise CrowdCentAPIError(f\"Failed to write file: {e}\") from e\n\n    # --- Class Method for Listing All Challenges ---\n\n    @classmethod\n    def list_all_challenges(\n        cls, api_key: Optional[str] = None, base_url: Optional[str] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists all active challenges.\n\n        This is a class method that doesn't require a challenge_slug.\n        Use this to discover available challenges before initializing a ChallengeClient.\n\n        Args:\n            api_key: Your CrowdCent API key. If not provided, it will attempt\n                     to load from the CROWDCENT_API_KEY environment variable\n                     or a .env file.\n            base_url: The base URL of the CrowdCent API. Defaults to\n                      http://crowdcent.com/api.\n\n        Returns:\n            A list of dictionaries, each representing an active challenge.\n        \"\"\"\n        # Create a temporary session for this request\n        load_dotenv()\n        api_key = api_key or os.getenv(cls.API_KEY_ENV_VAR)\n        if not api_key:\n            raise AuthenticationError(\n                f\"API key not provided and not found in environment variable \"\n                f\"'{cls.API_KEY_ENV_VAR}' or .env file.\"\n            )\n\n        base_url = (base_url or cls.DEFAULT_BASE_URL).rstrip(\"/\")\n        session = requests.Session()\n        session.headers.update({\"Authorization\": f\"Api-Key {api_key}\"})\n\n        url = f\"{base_url}/challenges/\"\n        try:\n            response = session.get(url)\n            response.raise_for_status()\n            return response.json()\n        except requests_exceptions.HTTPError as e:\n            status_code = e.response.status_code\n            if status_code == 401:\n                raise AuthenticationError(\"Authentication failed (401)\")\n            elif status_code == 404:\n                raise NotFoundError(\"Resource not found (404)\")\n            elif 400 &lt;= status_code &lt; 500:\n                raise ClientError(f\"Client error ({status_code})\")\n            elif 500 &lt;= status_code &lt; 600:\n                raise ServerError(f\"Server error ({status_code})\")\n            else:\n                raise CrowdCentAPIError(f\"HTTP error ({status_code})\")\n        except requests_exceptions.RequestException as e:\n            raise CrowdCentAPIError(f\"Request failed: {e}\")\n\n    # --- Challenge Methods ---\n\n    def get_challenge(self) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for this challenge.\n\n        Returns:\n            A dictionary representing this challenge.\n\n        Raises:\n            NotFoundError: If the challenge with the given slug is not found.\n        \"\"\"\n        response = self._request(\"GET\", f\"/challenges/{self.challenge_slug}/\")\n        return response.json()\n\n    # --- Training Data Methods ---\n\n    def list_training_datasets(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists all training dataset versions for this challenge.\n\n        Returns:\n            A list of dictionaries, each representing a training dataset version.\n\n        Raises:\n            NotFoundError: If the challenge is not found.\n        \"\"\"\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/training_data/\"\n        )\n        return response.json()\n\n    def get_training_dataset(self, version: str) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for a specific training dataset version.\n\n        Args:\n            version: The version string of the training dataset (e.g., '1.0', '2.1')\n                     or the special value ``\"latest\"`` to get the latest version.\n\n        Returns:\n            A dictionary representing the specified training dataset.\n\n        Raises:\n            NotFoundError: If the challenge or the specified training dataset is not found.\n        \"\"\"\n        if version == \"latest\":\n            response = self._request(\n                \"GET\", f\"/challenges/{self.challenge_slug}/training_data/latest/\"\n            )\n            return response.json()\n\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/training_data/{version}/\"\n        )\n        return response.json()\n\n    def download_training_dataset(self, version: str, dest_path: str):\n        \"\"\"Downloads the training data file for a specific dataset version.\n\n        Args:\n            version: The version string of the training dataset (e.g., '1.0', '2.1')\n                    or 'latest' to get the latest version.\n            dest_path: The local file path to save the downloaded dataset.\n\n        Raises:\n            NotFoundError: If the challenge, dataset, or its file is not found.\n        \"\"\"\n        if version == \"latest\":\n            latest_info = self.get_training_dataset(\"latest\")\n            version = latest_info[\"version\"]\n\n        endpoint = (\n            f\"/challenges/{self.challenge_slug}/training_data/{version}/download/\"\n        )\n        self._download_file(endpoint, dest_path, f\"training data v{version}\")\n\n    # --- Inference Data Methods ---\n\n    def list_inference_data(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists all inference data periods for this challenge.\n\n        Returns:\n            A list of dictionaries, each representing an inference data period.\n\n        Raises:\n            NotFoundError: If the challenge is not found.\n        \"\"\"\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/\"\n        )\n        return response.json()\n\n    def get_inference_data(self, release_date: str) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for a specific inference data period by its release date.\n\n        Args:\n            release_date: The release date of the inference data in 'YYYY-MM-DD' format.\n                          You can also pass the special values:\n                          - ``\"current\"`` to fetch the current active inference period\n                          - ``\"latest\"`` to fetch the most recently *available* inference period\n\n        Returns:\n            A dictionary representing the specified inference data period.\n\n        Raises:\n            NotFoundError: If the challenge or the specified inference data is not found.\n            ClientError: If the date format is invalid.\n        \"\"\"\n        if release_date == \"current\":\n            response = self._request(\n                \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/current/\"\n            )\n            return response.json()\n\n        if release_date == \"latest\":\n            # Simply resolve via list_inference_data(); avoid noisy probe.\n            periods = self.list_inference_data()\n            if not periods:\n                raise NotFoundError(\n                    \"No inference data periods found for this challenge.\"\n                )\n\n            latest_period = max(periods, key=lambda p: p[\"release_date\"])\n            release_date_iso = latest_period[\"release_date\"]\n            release_date = release_date_iso.split(\"T\")[0]\n\n        # Validate date format for explicit dates\n        try:\n            datetime.strptime(release_date, \"%Y-%m-%d\")\n        except ValueError:\n            raise ClientError(\n                f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n            )\n\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/\"\n        )\n        return response.json()\n\n    def download_inference_data(\n        self,\n        release_date: str,\n        dest_path: str,\n        poll: bool = True,\n        poll_interval: int = 30,\n        timeout: Optional[int] = 900,\n    ):\n        \"\"\"Downloads the inference features file for a specific period.\n\n        Args:\n            release_date: The release date of the inference data in 'YYYY-MM-DD' format\n                          or the special values ``\"current\"`` or ``\"latest\"``.\n            dest_path: The local file path to save the downloaded features file.\n            poll: Whether to wait for the inference data to be available before downloading.\n            poll_interval: Seconds to wait between retries when polling.\n            timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n                ``None`` waits indefinitely.\n\n        Raises:\n            NotFoundError: If the challenge, inference data, or its file is not found.\n            ClientError: If the date format is invalid.\n        \"\"\"\n        if release_date == \"current\":\n            # If polling is enabled, delegate to wait_for_inference_data which wraps\n            # this method and adds retry logic. Otherwise attempt a single direct\n            # download request.\n            if poll:\n                self.wait_for_inference_data(dest_path, poll_interval, timeout)\n                return\n\n            # Polling disabled \u2192 attempt once and propagate NotFoundError on 404.\n            endpoint = (\n                f\"/challenges/{self.challenge_slug}/inference_data/current/download/\"\n            )\n        else:\n            if release_date == \"latest\":\n                latest_info = self.get_inference_data(\"latest\")\n                release_date_iso = latest_info.get(\"release_date\")\n                release_date = (\n                    release_date_iso.split(\"T\")[0] if release_date_iso else None\n                )\n                if not release_date:\n                    raise CrowdCentAPIError(\n                        \"Malformed response when resolving latest inference period.\"\n                    )\n\n            # Validate date format after any resolution.\n            try:\n                datetime.strptime(release_date, \"%Y-%m-%d\")\n            except ValueError:\n                raise ClientError(\n                    f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n                )\n\n            endpoint = f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/download/\"\n\n        self._download_file(endpoint, dest_path, f\"inference data {release_date}\")\n\n    def wait_for_inference_data(\n        self,\n        dest_path: str,\n        poll_interval: int = 30,\n        timeout: Optional[int] = 900,\n    ) -&gt; None:\n        \"\"\"Waits for the *current* inference data release to appear and downloads it.\n\n        The internal data-generation pipeline begins around 14:00 UTC, but the\n        public inference file becomes available only after it passes data-quality\n        checks. This helper repeatedly calls\n        :py:meth:`download_inference_data` with ``release_date=\"current\"`` until\n        the file is ready (HTTP 404s are silently retried).\n\n        Args:\n            dest_path: Local path where the parquet file will be saved once available.\n            poll_interval: Seconds to wait between retries.\n            timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n                ``None`` waits indefinitely.\n\n        Raises:\n            TimeoutError: If *timeout* seconds pass without a successful download.\n            CrowdCentAPIError: For unrecoverable errors returned by the API.\n        \"\"\"\n        start_time = time.time()\n        attempts = 0\n\n        while True:\n            attempts += 1\n            try:\n                # Try to download the *current* period *once*. Pass poll=False to avoid\n                # the mutual recursion between `wait_for_inference_data` and\n                # `download_inference_data` which would otherwise trigger an infinite\n                # loop when the file is not yet available.\n                self.download_inference_data(\"current\", dest_path, poll=False)\n                logger.info(\n                    f\"Successfully downloaded inference data after {attempts} attempt(s) to {dest_path}\"\n                )\n                return  # Success \u2013 exit the loop\n            except NotFoundError:\n                # File not published yet \u2013 check timeout and sleep before retrying.\n                elapsed = time.time() - start_time\n                if timeout is not None and elapsed &gt;= timeout:\n                    raise TimeoutError(\n                        f\"Inference data was not available after waiting {timeout} seconds.\"\n                    )\n                logger.debug(\n                    f\"Inference data not yet available (attempt {attempts}). \"\n                    f\"Sleeping {poll_interval}s before retrying.\"\n                )\n                time.sleep(poll_interval)\n\n    # --- Submission Methods ---\n\n    def list_submissions(self, period: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n        \"\"\"Lists the authenticated user's submissions for this challenge.\n\n        Args:\n            period: Optional filter for submissions by period:\n                  - 'current': Only show submissions for the current active period\n                  - 'YYYY-MM-DD': Only show submissions for a specific inference period date\n\n        Returns:\n            A list of dictionaries, each representing a submission.\n        \"\"\"\n        params = {}\n        if period:\n            params[\"period\"] = period\n\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/submissions/\", params=params\n        )\n        return response.json()\n\n    def get_submission(self, submission_id: int) -&gt; Dict[str, Any]:\n        \"\"\"Gets details for a specific submission by its ID.\n\n        Args:\n            submission_id: The ID of the submission to retrieve.\n\n        Returns:\n            A dictionary representing the specified submission.\n\n        Raises:\n            NotFoundError: If the submission with the given ID is not found\n                           or doesn't belong to the user.\n        \"\"\"\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/submissions/{submission_id}/\"\n        )\n        return response.json()\n\n    @nw.narwhalify\n    def submit_predictions(\n        self,\n        file_path: str = \"submission.parquet\",\n        df: Optional[IntoFrameT] = None,\n        slot: int = 1,\n        queue_next: bool = True,\n        temp: bool = True,\n        max_retries: int = 3,\n        retry_delay: float = 1.0,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Submit predictions for this challenge.\n\n        If a submission window is currently open, the prediction is submitted immediately.\n        If no window is open, the prediction is queued and will be automatically submitted\n        when the next window opens.\n\n        You can provide either a file path to an existing Parquet file or a DataFrame\n        that will be temporarily saved as Parquet for submission.\n\n        The data must contain the required prediction columns specified by the challenge\n        (e.g., id, pred_10d, pred_30d).\n\n        Args:\n            file_path: Optional path to an existing prediction Parquet file.\n            df: Optional DataFrame with the prediction columns. If provided,\n                it will be temporarily saved as Parquet for submission.\n            slot: Submission slot number (1-based).\n            queue_next: Whether to also queue this submission for the next period\n                (auto-rollover). Defaults to True. When submitting during an open\n                window, this queues a copy for the following period.\n            temp: Whether to save the DataFrame to a temporary file.\n            max_retries: Maximum number of retry attempts for connection errors (default: 3).\n            retry_delay: Initial delay between retries in seconds (default: 1.0).\n\n        Returns:\n            A dictionary with submission details. The shape depends on context:\n\n            - **Window open (immediate submission)**: Contains submission fields like\n                `id`, `status`, `slot`, `submitted_at`, plus `queued_for_next` (bool).\n            - **Window closed (queued)**: Contains `status: \"queued\"`, `slot`,\n                `challenge`, and a `message` describing when it will be submitted.\n\n        Raises:\n            ValueError: If neither file_path nor df is provided, or if both are provided.\n            FileNotFoundError: If the specified file_path does not exist.\n            ClientError: If the submission is invalid (e.g., wrong format, missing columns).\n\n        Examples:\n            # Submit from a DataFrame\n            client.submit_predictions(df=predictions_df)\n\n            # Submit from a file\n            client.submit_predictions(file_path=\"predictions.parquet\")\n\n            # Submit and opt-out of auto-queueing for next period\n            client.submit_predictions(df=predictions_df, queue_next=False)\n        \"\"\"\n        if df is not None:\n            df.write_parquet(file_path)\n            logger.info(f\"Wrote DataFrame to temporary file: {file_path}\")\n\n        logger.info(\n            f\"Submitting predictions from {file_path} to challenge '{self.challenge_slug}' (Slot: {slot or '1'})\"\n        )\n\n        try:\n            with open(file_path, \"rb\") as f:\n                files = {\n                    \"prediction_file\": (\n                        os.path.basename(file_path),\n                        f,\n                        \"application/octet-stream\",\n                    )\n                }\n                data_payload = {\n                    \"slot\": str(slot),\n                    \"also_queue_next\": str(queue_next).lower(),\n                }\n                response = self._request(\n                    \"POST\",\n                    f\"/challenges/{self.challenge_slug}/submissions/\",\n                    files=files,\n                    data=data_payload,  # Pass slot and queue flag in data\n                    max_retries=max_retries,\n                    retry_delay=retry_delay,\n                )\n\n            resp_data = response.json()\n\n            # 202=queued, 200=updated, 201=created\n            msg = {202: \"queued\", 200: \"updated\", 201: \"created\"}.get(\n                response.status_code, \"submitted\"\n            )\n            logger.info(f\"Submission {msg} (slot {slot})\")\n            if resp_data.get(\"queued_for_next\"):\n                logger.info(\"Also queued for next period.\")\n\n            return resp_data\n        except FileNotFoundError as e:\n            logger.error(f\"Prediction file not found at {file_path}\")\n            raise FileNotFoundError(f\"Prediction file not found at {file_path}\") from e\n        except IOError as e:\n            logger.error(f\"Failed to read prediction file {file_path}: {e}\")\n            raise CrowdCentAPIError(f\"Failed to read prediction file: {e}\") from e\n        finally:\n            # Clean up the temporary file if we created one\n            if df is not None and temp:\n                try:\n                    os.unlink(file_path)\n                    logger.debug(f\"Cleaned up temporary file: {file_path}\")\n                except Exception as e:\n                    logger.warning(\n                        f\"Failed to clean up temporary file {file_path}: {e}\"\n                    )\n\n    # --- Challenge Switching ---\n\n    def switch_challenge(self, new_challenge_slug: str) -&gt; None:\n        \"\"\"Switch this client to interact with a different challenge.\n\n        Args:\n            new_challenge_slug: The slug identifier for the new challenge.\n\n        Returns:\n            None. The client is modified in-place.\n        \"\"\"\n        self.challenge_slug = new_challenge_slug\n        logger.info(f\"Client switched to challenge '{new_challenge_slug}'\")\n\n    # --- Historical Performance Methods ---\n\n    def get_performance(\n        self,\n        user: Optional[str] = None,\n        scored_only: bool = True,\n        slot: Optional[int] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get performance history for a user (defaults to authenticated user).\n\n        Fetches submissions with their scores and percentiles, flattens the\n        nested score data, and returns a list ready to wrap in pandas/polars.\n\n        Args:\n            user: Username to fetch performance for. If None (default), fetches\n                performance for the authenticated user.\n                *Note: Fetching other users' performance is not yet supported.*\n            scored_only: If True (default), only include submissions that have been scored.\n                For pending submissions, only the most recent (partially resolved) score\n                is available \u2014 daily granularity is not currently exposed by the API.\n            slot: Optional slot filter. If provided, only include submissions from this slot.\n\n        Returns:\n            A list of dictionaries, each containing:\n            - id: Submission ID\n            - slot: Submission slot number\n            - release_date: The inference period date (ISO string)\n            - submitted_at: When the submission was made (ISO string)\n            - status: Submission status (\"pending\" or \"evaluated\")\n            - score_*: Individual score metrics (e.g., score_spearman_10d)\n            - percentile_*: Individual percentile metrics (e.g., percentile_spearman_10d)\n            - composite_percentile: Overall percentile ranking (if available)\n\n        Note:\n            - For submissions with status=\"pending\", scores reflect the most recent\n              partial evaluation (e.g., day 5 of a 10-day prediction). Daily score\n              progression is tracked server-side but not yet available via the API.\n\n            - Percentile fields (e.g., composite_percentile=0.75) indicate rank\n              relative to all participants \u2014 0.75 means outperforming 75% of\n              submissions for that period.\n\n        Example:\n            &gt;&gt;&gt; client = ChallengeClient(\"momentum-alpha\")\n            &gt;&gt;&gt; history = client.get_performance()\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; df = pd.DataFrame(history)\n        \"\"\"\n        if user is not None:\n            raise NotImplementedError(\n                \"Fetching performance for specific users is not yet supported via the API. \"\n                \"Leave `user=None` to fetch your own performance.\"\n            )\n\n        logger.info(f\"Fetching submission history for '{self.challenge_slug}'...\")\n        submissions = self.list_submissions()\n\n        if not submissions:\n            logger.info(\"No submissions found.\")\n            return []\n\n        rows = []\n        for sub in submissions:\n            # Skip unscored if requested\n            if scored_only and not sub.get(\"score_details\"):\n                continue\n\n            # Skip if slot filter doesn't match\n            if slot is not None and sub.get(\"slot\") != slot:\n                continue\n\n            row = {\n                \"id\": sub.get(\"id\"),\n                \"slot\": sub.get(\"slot\"),\n                \"release_date\": sub.get(\"inference_data_release_date\", \"\")[:10]\n                if sub.get(\"inference_data_release_date\")\n                else None,\n                \"submitted_at\": sub.get(\"submitted_at\"),\n                \"status\": sub.get(\"status\"),\n            }\n\n            # Flatten score_details (avoid redundant prefix if key already contains it)\n            score_details = sub.get(\"score_details\") or {}\n            for key, value in score_details.items():\n                col = key if \"score\" in key else f\"score_{key}\"\n                row[col] = value\n\n            # Flatten percentile_details (avoid redundant prefix if key already contains it)\n            percentile_details = sub.get(\"percentile_details\") or {}\n            for key, value in percentile_details.items():\n                col = key if \"percentile\" in key else f\"percentile_{key}\"\n                row[col] = value\n\n            rows.append(row)\n\n        # Sort by release_date descending (most recent first)\n        rows.sort(key=lambda x: x.get(\"release_date\") or \"\", reverse=True)\n\n        logger.info(f\"Loaded {len(rows)} scored submissions.\")\n        return rows\n\n    # --- Meta-Model Download ---\n\n    def download_meta_model(self, dest_path: str):\n        \"\"\"Downloads the consolidated meta-model file for this challenge.\n\n        The meta-model is typically an aggregation (e.g., average) of all valid\n        submissions for past inference periods.\n\n        Args:\n            dest_path: The local file path to save the downloaded meta-model.\n\n        Raises:\n            NotFoundError: If the challenge or its meta-model file is not found.\n            CrowdCentAPIError: For issues during download or file writing.\n            PermissionDenied: If the meta-model is not public and user lacks permission.\n        \"\"\"\n        endpoint = f\"/challenges/{self.challenge_slug}/meta_model/download/\"\n        self._download_file(endpoint, dest_path, \"meta-model\")\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.__init__","title":"<code>__init__(challenge_slug, api_key=None, base_url=None)</code>","text":"<p>Initializes the ChallengeClient for a specific challenge.</p> <p>Parameters:</p> Name Type Description Default <code>challenge_slug</code> <code>str</code> <p>The unique identifier (slug) for the challenge.</p> required <code>api_key</code> <code>Optional[str]</code> <p>Your CrowdCent API key. If not provided, it will attempt      to load from the CROWDCENT_API_KEY environment variable      or a .env file.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL of the CrowdCent API. Defaults to       https://crowdcent.com/api.</p> <code>None</code> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def __init__(\n    self,\n    challenge_slug: str,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the ChallengeClient for a specific challenge.\n\n    Args:\n        challenge_slug: The unique identifier (slug) for the challenge.\n        api_key: Your CrowdCent API key. If not provided, it will attempt\n                 to load from the CROWDCENT_API_KEY environment variable\n                 or a .env file.\n        base_url: The base URL of the CrowdCent API. Defaults to\n                  https://crowdcent.com/api.\n    \"\"\"\n    load_dotenv()  # Load .env file if present\n    self.api_key = api_key or os.getenv(self.API_KEY_ENV_VAR)\n    if not self.api_key:\n        raise AuthenticationError(\n            f\"API key not provided and not found in environment variable \"\n            f\"'{self.API_KEY_ENV_VAR}' or .env file.\"\n        )\n\n    self.challenge_slug = challenge_slug\n    self.base_url = (base_url or self.DEFAULT_BASE_URL).rstrip(\"/\")\n    self.session = requests.Session()\n    self.session.headers.update({\"Authorization\": f\"Api-Key {self.api_key}\"})\n    logger.info(\n        f\"ChallengeClient initialized for '{challenge_slug}' at URL: {self.base_url}\"\n    )\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_all_challenges","title":"<code>list_all_challenges(api_key=None, base_url=None)</code>  <code>classmethod</code>","text":"<p>Lists all active challenges.</p> <p>This is a class method that doesn't require a challenge_slug. Use this to discover available challenges before initializing a ChallengeClient.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Your CrowdCent API key. If not provided, it will attempt      to load from the CROWDCENT_API_KEY environment variable      or a .env file.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL of the CrowdCent API. Defaults to       http://crowdcent.com/api.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing an active challenge.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>@classmethod\ndef list_all_challenges(\n    cls, api_key: Optional[str] = None, base_url: Optional[str] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists all active challenges.\n\n    This is a class method that doesn't require a challenge_slug.\n    Use this to discover available challenges before initializing a ChallengeClient.\n\n    Args:\n        api_key: Your CrowdCent API key. If not provided, it will attempt\n                 to load from the CROWDCENT_API_KEY environment variable\n                 or a .env file.\n        base_url: The base URL of the CrowdCent API. Defaults to\n                  http://crowdcent.com/api.\n\n    Returns:\n        A list of dictionaries, each representing an active challenge.\n    \"\"\"\n    # Create a temporary session for this request\n    load_dotenv()\n    api_key = api_key or os.getenv(cls.API_KEY_ENV_VAR)\n    if not api_key:\n        raise AuthenticationError(\n            f\"API key not provided and not found in environment variable \"\n            f\"'{cls.API_KEY_ENV_VAR}' or .env file.\"\n        )\n\n    base_url = (base_url or cls.DEFAULT_BASE_URL).rstrip(\"/\")\n    session = requests.Session()\n    session.headers.update({\"Authorization\": f\"Api-Key {api_key}\"})\n\n    url = f\"{base_url}/challenges/\"\n    try:\n        response = session.get(url)\n        response.raise_for_status()\n        return response.json()\n    except requests_exceptions.HTTPError as e:\n        status_code = e.response.status_code\n        if status_code == 401:\n            raise AuthenticationError(\"Authentication failed (401)\")\n        elif status_code == 404:\n            raise NotFoundError(\"Resource not found (404)\")\n        elif 400 &lt;= status_code &lt; 500:\n            raise ClientError(f\"Client error ({status_code})\")\n        elif 500 &lt;= status_code &lt; 600:\n            raise ServerError(f\"Server error ({status_code})\")\n        else:\n            raise CrowdCentAPIError(f\"HTTP error ({status_code})\")\n    except requests_exceptions.RequestException as e:\n        raise CrowdCentAPIError(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_challenge","title":"<code>get_challenge()</code>","text":"<p>Gets details for this challenge.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing this challenge.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge with the given slug is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_challenge(self) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for this challenge.\n\n    Returns:\n        A dictionary representing this challenge.\n\n    Raises:\n        NotFoundError: If the challenge with the given slug is not found.\n    \"\"\"\n    response = self._request(\"GET\", f\"/challenges/{self.challenge_slug}/\")\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_training_datasets","title":"<code>list_training_datasets()</code>","text":"<p>Lists all training dataset versions for this challenge.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing a training dataset version.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def list_training_datasets(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists all training dataset versions for this challenge.\n\n    Returns:\n        A list of dictionaries, each representing a training dataset version.\n\n    Raises:\n        NotFoundError: If the challenge is not found.\n    \"\"\"\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/training_data/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_training_dataset","title":"<code>get_training_dataset(version)</code>","text":"<p>Gets details for a specific training dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version string of the training dataset (e.g., '1.0', '2.1')      or the special value <code>\"latest\"</code> to get the latest version.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing the specified training dataset.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge or the specified training dataset is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_training_dataset(self, version: str) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for a specific training dataset version.\n\n    Args:\n        version: The version string of the training dataset (e.g., '1.0', '2.1')\n                 or the special value ``\"latest\"`` to get the latest version.\n\n    Returns:\n        A dictionary representing the specified training dataset.\n\n    Raises:\n        NotFoundError: If the challenge or the specified training dataset is not found.\n    \"\"\"\n    if version == \"latest\":\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/training_data/latest/\"\n        )\n        return response.json()\n\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/training_data/{version}/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.download_training_dataset","title":"<code>download_training_dataset(version, dest_path)</code>","text":"<p>Downloads the training data file for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version string of the training dataset (e.g., '1.0', '2.1')     or 'latest' to get the latest version.</p> required <code>dest_path</code> <code>str</code> <p>The local file path to save the downloaded dataset.</p> required <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge, dataset, or its file is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def download_training_dataset(self, version: str, dest_path: str):\n    \"\"\"Downloads the training data file for a specific dataset version.\n\n    Args:\n        version: The version string of the training dataset (e.g., '1.0', '2.1')\n                or 'latest' to get the latest version.\n        dest_path: The local file path to save the downloaded dataset.\n\n    Raises:\n        NotFoundError: If the challenge, dataset, or its file is not found.\n    \"\"\"\n    if version == \"latest\":\n        latest_info = self.get_training_dataset(\"latest\")\n        version = latest_info[\"version\"]\n\n    endpoint = (\n        f\"/challenges/{self.challenge_slug}/training_data/{version}/download/\"\n    )\n    self._download_file(endpoint, dest_path, f\"training data v{version}\")\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_inference_data","title":"<code>list_inference_data()</code>","text":"<p>Lists all inference data periods for this challenge.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing an inference data period.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge is not found.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def list_inference_data(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists all inference data periods for this challenge.\n\n    Returns:\n        A list of dictionaries, each representing an inference data period.\n\n    Raises:\n        NotFoundError: If the challenge is not found.\n    \"\"\"\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_inference_data","title":"<code>get_inference_data(release_date)</code>","text":"<p>Gets details for a specific inference data period by its release date.</p> <p>Parameters:</p> Name Type Description Default <code>release_date</code> <code>str</code> <p>The release date of the inference data in 'YYYY-MM-DD' format.           You can also pass the special values:           - <code>\"current\"</code> to fetch the current active inference period           - <code>\"latest\"</code> to fetch the most recently available inference period</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing the specified inference data period.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge or the specified inference data is not found.</p> <code>ClientError</code> <p>If the date format is invalid.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_inference_data(self, release_date: str) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for a specific inference data period by its release date.\n\n    Args:\n        release_date: The release date of the inference data in 'YYYY-MM-DD' format.\n                      You can also pass the special values:\n                      - ``\"current\"`` to fetch the current active inference period\n                      - ``\"latest\"`` to fetch the most recently *available* inference period\n\n    Returns:\n        A dictionary representing the specified inference data period.\n\n    Raises:\n        NotFoundError: If the challenge or the specified inference data is not found.\n        ClientError: If the date format is invalid.\n    \"\"\"\n    if release_date == \"current\":\n        response = self._request(\n            \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/current/\"\n        )\n        return response.json()\n\n    if release_date == \"latest\":\n        # Simply resolve via list_inference_data(); avoid noisy probe.\n        periods = self.list_inference_data()\n        if not periods:\n            raise NotFoundError(\n                \"No inference data periods found for this challenge.\"\n            )\n\n        latest_period = max(periods, key=lambda p: p[\"release_date\"])\n        release_date_iso = latest_period[\"release_date\"]\n        release_date = release_date_iso.split(\"T\")[0]\n\n    # Validate date format for explicit dates\n    try:\n        datetime.strptime(release_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise ClientError(\n            f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n        )\n\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.download_inference_data","title":"<code>download_inference_data(release_date, dest_path, poll=True, poll_interval=30, timeout=900)</code>","text":"<p>Downloads the inference features file for a specific period.</p> <p>Parameters:</p> Name Type Description Default <code>release_date</code> <code>str</code> <p>The release date of the inference data in 'YYYY-MM-DD' format           or the special values <code>\"current\"</code> or <code>\"latest\"</code>.</p> required <code>dest_path</code> <code>str</code> <p>The local file path to save the downloaded features file.</p> required <code>poll</code> <code>bool</code> <p>Whether to wait for the inference data to be available before downloading.</p> <code>True</code> <code>poll_interval</code> <code>int</code> <p>Seconds to wait between retries when polling.</p> <code>30</code> <code>timeout</code> <code>Optional[int]</code> <p>Maximum seconds to wait before raising :class:<code>TimeoutError</code>. <code>None</code> waits indefinitely.</p> <code>900</code> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge, inference data, or its file is not found.</p> <code>ClientError</code> <p>If the date format is invalid.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def download_inference_data(\n    self,\n    release_date: str,\n    dest_path: str,\n    poll: bool = True,\n    poll_interval: int = 30,\n    timeout: Optional[int] = 900,\n):\n    \"\"\"Downloads the inference features file for a specific period.\n\n    Args:\n        release_date: The release date of the inference data in 'YYYY-MM-DD' format\n                      or the special values ``\"current\"`` or ``\"latest\"``.\n        dest_path: The local file path to save the downloaded features file.\n        poll: Whether to wait for the inference data to be available before downloading.\n        poll_interval: Seconds to wait between retries when polling.\n        timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n            ``None`` waits indefinitely.\n\n    Raises:\n        NotFoundError: If the challenge, inference data, or its file is not found.\n        ClientError: If the date format is invalid.\n    \"\"\"\n    if release_date == \"current\":\n        # If polling is enabled, delegate to wait_for_inference_data which wraps\n        # this method and adds retry logic. Otherwise attempt a single direct\n        # download request.\n        if poll:\n            self.wait_for_inference_data(dest_path, poll_interval, timeout)\n            return\n\n        # Polling disabled \u2192 attempt once and propagate NotFoundError on 404.\n        endpoint = (\n            f\"/challenges/{self.challenge_slug}/inference_data/current/download/\"\n        )\n    else:\n        if release_date == \"latest\":\n            latest_info = self.get_inference_data(\"latest\")\n            release_date_iso = latest_info.get(\"release_date\")\n            release_date = (\n                release_date_iso.split(\"T\")[0] if release_date_iso else None\n            )\n            if not release_date:\n                raise CrowdCentAPIError(\n                    \"Malformed response when resolving latest inference period.\"\n                )\n\n        # Validate date format after any resolution.\n        try:\n            datetime.strptime(release_date, \"%Y-%m-%d\")\n        except ValueError:\n            raise ClientError(\n                f\"Invalid date format: {release_date}. Use 'YYYY-MM-DD' format.\"\n            )\n\n        endpoint = f\"/challenges/{self.challenge_slug}/inference_data/{release_date}/download/\"\n\n    self._download_file(endpoint, dest_path, f\"inference data {release_date}\")\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.wait_for_inference_data","title":"<code>wait_for_inference_data(dest_path, poll_interval=30, timeout=900)</code>","text":"<p>Waits for the current inference data release to appear and downloads it.</p> <p>The internal data-generation pipeline begins around 14:00 UTC, but the public inference file becomes available only after it passes data-quality checks. This helper repeatedly calls meth:<code>download_inference_data</code> with <code>release_date=\"current\"</code> until the file is ready (HTTP 404s are silently retried).</p> <p>Parameters:</p> Name Type Description Default <code>dest_path</code> <code>str</code> <p>Local path where the parquet file will be saved once available.</p> required <code>poll_interval</code> <code>int</code> <p>Seconds to wait between retries.</p> <code>30</code> <code>timeout</code> <code>Optional[int]</code> <p>Maximum seconds to wait before raising :class:<code>TimeoutError</code>. <code>None</code> waits indefinitely.</p> <code>900</code> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If timeout seconds pass without a successful download.</p> <code>CrowdCentAPIError</code> <p>For unrecoverable errors returned by the API.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def wait_for_inference_data(\n    self,\n    dest_path: str,\n    poll_interval: int = 30,\n    timeout: Optional[int] = 900,\n) -&gt; None:\n    \"\"\"Waits for the *current* inference data release to appear and downloads it.\n\n    The internal data-generation pipeline begins around 14:00 UTC, but the\n    public inference file becomes available only after it passes data-quality\n    checks. This helper repeatedly calls\n    :py:meth:`download_inference_data` with ``release_date=\"current\"`` until\n    the file is ready (HTTP 404s are silently retried).\n\n    Args:\n        dest_path: Local path where the parquet file will be saved once available.\n        poll_interval: Seconds to wait between retries.\n        timeout: Maximum seconds to wait before raising :class:`TimeoutError`.\n            ``None`` waits indefinitely.\n\n    Raises:\n        TimeoutError: If *timeout* seconds pass without a successful download.\n        CrowdCentAPIError: For unrecoverable errors returned by the API.\n    \"\"\"\n    start_time = time.time()\n    attempts = 0\n\n    while True:\n        attempts += 1\n        try:\n            # Try to download the *current* period *once*. Pass poll=False to avoid\n            # the mutual recursion between `wait_for_inference_data` and\n            # `download_inference_data` which would otherwise trigger an infinite\n            # loop when the file is not yet available.\n            self.download_inference_data(\"current\", dest_path, poll=False)\n            logger.info(\n                f\"Successfully downloaded inference data after {attempts} attempt(s) to {dest_path}\"\n            )\n            return  # Success \u2013 exit the loop\n        except NotFoundError:\n            # File not published yet \u2013 check timeout and sleep before retrying.\n            elapsed = time.time() - start_time\n            if timeout is not None and elapsed &gt;= timeout:\n                raise TimeoutError(\n                    f\"Inference data was not available after waiting {timeout} seconds.\"\n                )\n            logger.debug(\n                f\"Inference data not yet available (attempt {attempts}). \"\n                f\"Sleeping {poll_interval}s before retrying.\"\n            )\n            time.sleep(poll_interval)\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.list_submissions","title":"<code>list_submissions(period=None)</code>","text":"<p>Lists the authenticated user's submissions for this challenge.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>Optional[str]</code> <p>Optional filter for submissions by period:   - 'current': Only show submissions for the current active period   - 'YYYY-MM-DD': Only show submissions for a specific inference period date</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing a submission.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def list_submissions(self, period: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists the authenticated user's submissions for this challenge.\n\n    Args:\n        period: Optional filter for submissions by period:\n              - 'current': Only show submissions for the current active period\n              - 'YYYY-MM-DD': Only show submissions for a specific inference period date\n\n    Returns:\n        A list of dictionaries, each representing a submission.\n    \"\"\"\n    params = {}\n    if period:\n        params[\"period\"] = period\n\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/submissions/\", params=params\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_submission","title":"<code>get_submission(submission_id)</code>","text":"<p>Gets details for a specific submission by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>submission_id</code> <code>int</code> <p>The ID of the submission to retrieve.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary representing the specified submission.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the submission with the given ID is not found            or doesn't belong to the user.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_submission(self, submission_id: int) -&gt; Dict[str, Any]:\n    \"\"\"Gets details for a specific submission by its ID.\n\n    Args:\n        submission_id: The ID of the submission to retrieve.\n\n    Returns:\n        A dictionary representing the specified submission.\n\n    Raises:\n        NotFoundError: If the submission with the given ID is not found\n                       or doesn't belong to the user.\n    \"\"\"\n    response = self._request(\n        \"GET\", f\"/challenges/{self.challenge_slug}/submissions/{submission_id}/\"\n    )\n    return response.json()\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions","title":"<code>submit_predictions(file_path='submission.parquet', df=None, slot=1, queue_next=True, temp=True, max_retries=3, retry_delay=1.0)</code>","text":"<p>Submit predictions for this challenge.</p> <p>If a submission window is currently open, the prediction is submitted immediately. If no window is open, the prediction is queued and will be automatically submitted when the next window opens.</p> <p>You can provide either a file path to an existing Parquet file or a DataFrame that will be temporarily saved as Parquet for submission.</p> <p>The data must contain the required prediction columns specified by the challenge (e.g., id, pred_10d, pred_30d).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Optional path to an existing prediction Parquet file.</p> <code>'submission.parquet'</code> <code>df</code> <code>Optional[IntoFrameT]</code> <p>Optional DataFrame with the prediction columns. If provided, it will be temporarily saved as Parquet for submission.</p> <code>None</code> <code>slot</code> <code>int</code> <p>Submission slot number (1-based).</p> <code>1</code> <code>queue_next</code> <code>bool</code> <p>Whether to also queue this submission for the next period (auto-rollover). Defaults to True. When submitting during an open window, this queues a copy for the following period.</p> <code>True</code> <code>temp</code> <code>bool</code> <p>Whether to save the DataFrame to a temporary file.</p> <code>True</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for connection errors (default: 3).</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds (default: 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with submission details. The shape depends on context:</p> <code>Dict[str, Any]</code> <ul> <li>Window open (immediate submission): Contains submission fields like <code>id</code>, <code>status</code>, <code>slot</code>, <code>submitted_at</code>, plus <code>queued_for_next</code> (bool).</li> </ul> <code>Dict[str, Any]</code> <ul> <li>Window closed (queued): Contains <code>status: \"queued\"</code>, <code>slot</code>, <code>challenge</code>, and a <code>message</code> describing when it will be submitted.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither file_path nor df is provided, or if both are provided.</p> <code>FileNotFoundError</code> <p>If the specified file_path does not exist.</p> <code>ClientError</code> <p>If the submission is invalid (e.g., wrong format, missing columns).</p> <p>Examples:</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions--submit-from-a-dataframe","title":"Submit from a DataFrame","text":"<p>client.submit_predictions(df=predictions_df)</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions--submit-from-a-file","title":"Submit from a file","text":"<p>client.submit_predictions(file_path=\"predictions.parquet\")</p>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.submit_predictions--submit-and-opt-out-of-auto-queueing-for-next-period","title":"Submit and opt-out of auto-queueing for next period","text":"<p>client.submit_predictions(df=predictions_df, queue_next=False)</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>@nw.narwhalify\ndef submit_predictions(\n    self,\n    file_path: str = \"submission.parquet\",\n    df: Optional[IntoFrameT] = None,\n    slot: int = 1,\n    queue_next: bool = True,\n    temp: bool = True,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n) -&gt; Dict[str, Any]:\n    \"\"\"Submit predictions for this challenge.\n\n    If a submission window is currently open, the prediction is submitted immediately.\n    If no window is open, the prediction is queued and will be automatically submitted\n    when the next window opens.\n\n    You can provide either a file path to an existing Parquet file or a DataFrame\n    that will be temporarily saved as Parquet for submission.\n\n    The data must contain the required prediction columns specified by the challenge\n    (e.g., id, pred_10d, pred_30d).\n\n    Args:\n        file_path: Optional path to an existing prediction Parquet file.\n        df: Optional DataFrame with the prediction columns. If provided,\n            it will be temporarily saved as Parquet for submission.\n        slot: Submission slot number (1-based).\n        queue_next: Whether to also queue this submission for the next period\n            (auto-rollover). Defaults to True. When submitting during an open\n            window, this queues a copy for the following period.\n        temp: Whether to save the DataFrame to a temporary file.\n        max_retries: Maximum number of retry attempts for connection errors (default: 3).\n        retry_delay: Initial delay between retries in seconds (default: 1.0).\n\n    Returns:\n        A dictionary with submission details. The shape depends on context:\n\n        - **Window open (immediate submission)**: Contains submission fields like\n            `id`, `status`, `slot`, `submitted_at`, plus `queued_for_next` (bool).\n        - **Window closed (queued)**: Contains `status: \"queued\"`, `slot`,\n            `challenge`, and a `message` describing when it will be submitted.\n\n    Raises:\n        ValueError: If neither file_path nor df is provided, or if both are provided.\n        FileNotFoundError: If the specified file_path does not exist.\n        ClientError: If the submission is invalid (e.g., wrong format, missing columns).\n\n    Examples:\n        # Submit from a DataFrame\n        client.submit_predictions(df=predictions_df)\n\n        # Submit from a file\n        client.submit_predictions(file_path=\"predictions.parquet\")\n\n        # Submit and opt-out of auto-queueing for next period\n        client.submit_predictions(df=predictions_df, queue_next=False)\n    \"\"\"\n    if df is not None:\n        df.write_parquet(file_path)\n        logger.info(f\"Wrote DataFrame to temporary file: {file_path}\")\n\n    logger.info(\n        f\"Submitting predictions from {file_path} to challenge '{self.challenge_slug}' (Slot: {slot or '1'})\"\n    )\n\n    try:\n        with open(file_path, \"rb\") as f:\n            files = {\n                \"prediction_file\": (\n                    os.path.basename(file_path),\n                    f,\n                    \"application/octet-stream\",\n                )\n            }\n            data_payload = {\n                \"slot\": str(slot),\n                \"also_queue_next\": str(queue_next).lower(),\n            }\n            response = self._request(\n                \"POST\",\n                f\"/challenges/{self.challenge_slug}/submissions/\",\n                files=files,\n                data=data_payload,  # Pass slot and queue flag in data\n                max_retries=max_retries,\n                retry_delay=retry_delay,\n            )\n\n        resp_data = response.json()\n\n        # 202=queued, 200=updated, 201=created\n        msg = {202: \"queued\", 200: \"updated\", 201: \"created\"}.get(\n            response.status_code, \"submitted\"\n        )\n        logger.info(f\"Submission {msg} (slot {slot})\")\n        if resp_data.get(\"queued_for_next\"):\n            logger.info(\"Also queued for next period.\")\n\n        return resp_data\n    except FileNotFoundError as e:\n        logger.error(f\"Prediction file not found at {file_path}\")\n        raise FileNotFoundError(f\"Prediction file not found at {file_path}\") from e\n    except IOError as e:\n        logger.error(f\"Failed to read prediction file {file_path}: {e}\")\n        raise CrowdCentAPIError(f\"Failed to read prediction file: {e}\") from e\n    finally:\n        # Clean up the temporary file if we created one\n        if df is not None and temp:\n            try:\n                os.unlink(file_path)\n                logger.debug(f\"Cleaned up temporary file: {file_path}\")\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to clean up temporary file {file_path}: {e}\"\n                )\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.switch_challenge","title":"<code>switch_challenge(new_challenge_slug)</code>","text":"<p>Switch this client to interact with a different challenge.</p> <p>Parameters:</p> Name Type Description Default <code>new_challenge_slug</code> <code>str</code> <p>The slug identifier for the new challenge.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None. The client is modified in-place.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def switch_challenge(self, new_challenge_slug: str) -&gt; None:\n    \"\"\"Switch this client to interact with a different challenge.\n\n    Args:\n        new_challenge_slug: The slug identifier for the new challenge.\n\n    Returns:\n        None. The client is modified in-place.\n    \"\"\"\n    self.challenge_slug = new_challenge_slug\n    logger.info(f\"Client switched to challenge '{new_challenge_slug}'\")\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.get_performance","title":"<code>get_performance(user=None, scored_only=True, slot=None)</code>","text":"<p>Get performance history for a user (defaults to authenticated user).</p> <p>Fetches submissions with their scores and percentiles, flattens the nested score data, and returns a list ready to wrap in pandas/polars.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>Optional[str]</code> <p>Username to fetch performance for. If None (default), fetches performance for the authenticated user. Note: Fetching other users' performance is not yet supported.</p> <code>None</code> <code>scored_only</code> <code>bool</code> <p>If True (default), only include submissions that have been scored. For pending submissions, only the most recent (partially resolved) score is available \u2014 daily granularity is not currently exposed by the API.</p> <code>True</code> <code>slot</code> <code>Optional[int]</code> <p>Optional slot filter. If provided, only include submissions from this slot.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each containing:</p> <code>List[Dict[str, Any]]</code> <ul> <li>id: Submission ID</li> </ul> <code>List[Dict[str, Any]]</code> <ul> <li>slot: Submission slot number</li> </ul> <code>List[Dict[str, Any]]</code> <ul> <li>release_date: The inference period date (ISO string)</li> </ul> <code>List[Dict[str, Any]]</code> <ul> <li>submitted_at: When the submission was made (ISO string)</li> </ul> <code>List[Dict[str, Any]]</code> <ul> <li>status: Submission status (\"pending\" or \"evaluated\")</li> </ul> <code>List[Dict[str, Any]]</code> <ul> <li>score_*: Individual score metrics (e.g., score_spearman_10d)</li> </ul> <code>List[Dict[str, Any]]</code> <ul> <li>percentile_*: Individual percentile metrics (e.g., percentile_spearman_10d)</li> </ul> <code>List[Dict[str, Any]]</code> <ul> <li>composite_percentile: Overall percentile ranking (if available)</li> </ul> Note <ul> <li> <p>For submissions with status=\"pending\", scores reflect the most recent   partial evaluation (e.g., day 5 of a 10-day prediction). Daily score   progression is tracked server-side but not yet available via the API.</p> </li> <li> <p>Percentile fields (e.g., composite_percentile=0.75) indicate rank   relative to all participants \u2014 0.75 means outperforming 75% of   submissions for that period.</p> </li> </ul> Example <p>client = ChallengeClient(\"momentum-alpha\") history = client.get_performance() import pandas as pd df = pd.DataFrame(history)</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def get_performance(\n    self,\n    user: Optional[str] = None,\n    scored_only: bool = True,\n    slot: Optional[int] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get performance history for a user (defaults to authenticated user).\n\n    Fetches submissions with their scores and percentiles, flattens the\n    nested score data, and returns a list ready to wrap in pandas/polars.\n\n    Args:\n        user: Username to fetch performance for. If None (default), fetches\n            performance for the authenticated user.\n            *Note: Fetching other users' performance is not yet supported.*\n        scored_only: If True (default), only include submissions that have been scored.\n            For pending submissions, only the most recent (partially resolved) score\n            is available \u2014 daily granularity is not currently exposed by the API.\n        slot: Optional slot filter. If provided, only include submissions from this slot.\n\n    Returns:\n        A list of dictionaries, each containing:\n        - id: Submission ID\n        - slot: Submission slot number\n        - release_date: The inference period date (ISO string)\n        - submitted_at: When the submission was made (ISO string)\n        - status: Submission status (\"pending\" or \"evaluated\")\n        - score_*: Individual score metrics (e.g., score_spearman_10d)\n        - percentile_*: Individual percentile metrics (e.g., percentile_spearman_10d)\n        - composite_percentile: Overall percentile ranking (if available)\n\n    Note:\n        - For submissions with status=\"pending\", scores reflect the most recent\n          partial evaluation (e.g., day 5 of a 10-day prediction). Daily score\n          progression is tracked server-side but not yet available via the API.\n\n        - Percentile fields (e.g., composite_percentile=0.75) indicate rank\n          relative to all participants \u2014 0.75 means outperforming 75% of\n          submissions for that period.\n\n    Example:\n        &gt;&gt;&gt; client = ChallengeClient(\"momentum-alpha\")\n        &gt;&gt;&gt; history = client.get_performance()\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame(history)\n    \"\"\"\n    if user is not None:\n        raise NotImplementedError(\n            \"Fetching performance for specific users is not yet supported via the API. \"\n            \"Leave `user=None` to fetch your own performance.\"\n        )\n\n    logger.info(f\"Fetching submission history for '{self.challenge_slug}'...\")\n    submissions = self.list_submissions()\n\n    if not submissions:\n        logger.info(\"No submissions found.\")\n        return []\n\n    rows = []\n    for sub in submissions:\n        # Skip unscored if requested\n        if scored_only and not sub.get(\"score_details\"):\n            continue\n\n        # Skip if slot filter doesn't match\n        if slot is not None and sub.get(\"slot\") != slot:\n            continue\n\n        row = {\n            \"id\": sub.get(\"id\"),\n            \"slot\": sub.get(\"slot\"),\n            \"release_date\": sub.get(\"inference_data_release_date\", \"\")[:10]\n            if sub.get(\"inference_data_release_date\")\n            else None,\n            \"submitted_at\": sub.get(\"submitted_at\"),\n            \"status\": sub.get(\"status\"),\n        }\n\n        # Flatten score_details (avoid redundant prefix if key already contains it)\n        score_details = sub.get(\"score_details\") or {}\n        for key, value in score_details.items():\n            col = key if \"score\" in key else f\"score_{key}\"\n            row[col] = value\n\n        # Flatten percentile_details (avoid redundant prefix if key already contains it)\n        percentile_details = sub.get(\"percentile_details\") or {}\n        for key, value in percentile_details.items():\n            col = key if \"percentile\" in key else f\"percentile_{key}\"\n            row[col] = value\n\n        rows.append(row)\n\n    # Sort by release_date descending (most recent first)\n    rows.sort(key=lambda x: x.get(\"release_date\") or \"\", reverse=True)\n\n    logger.info(f\"Loaded {len(rows)} scored submissions.\")\n    return rows\n</code></pre>"},{"location":"api-reference/python/#crowdcent_challenge.client.ChallengeClient.download_meta_model","title":"<code>download_meta_model(dest_path)</code>","text":"<p>Downloads the consolidated meta-model file for this challenge.</p> <p>The meta-model is typically an aggregation (e.g., average) of all valid submissions for past inference periods.</p> <p>Parameters:</p> Name Type Description Default <code>dest_path</code> <code>str</code> <p>The local file path to save the downloaded meta-model.</p> required <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the challenge or its meta-model file is not found.</p> <code>CrowdCentAPIError</code> <p>For issues during download or file writing.</p> <code>PermissionDenied</code> <p>If the meta-model is not public and user lacks permission.</p> Source code in <code>src/crowdcent_challenge/client.py</code> <pre><code>def download_meta_model(self, dest_path: str):\n    \"\"\"Downloads the consolidated meta-model file for this challenge.\n\n    The meta-model is typically an aggregation (e.g., average) of all valid\n    submissions for past inference periods.\n\n    Args:\n        dest_path: The local file path to save the downloaded meta-model.\n\n    Raises:\n        NotFoundError: If the challenge or its meta-model file is not found.\n        CrowdCentAPIError: For issues during download or file writing.\n        PermissionDenied: If the meta-model is not public and user lacks permission.\n    \"\"\"\n    endpoint = f\"/challenges/{self.challenge_slug}/meta_model/download/\"\n    self._download_file(endpoint, dest_path, \"meta-model\")\n</code></pre>"},{"location":"tutorials/advanced-custom-dataset-eodhd/","title":"Build a Custom Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install crowdcent-challenge numerblox eod centimators polars altair vegafusion vl-convert-python\n</pre> !pip install crowdcent-challenge numerblox eod centimators polars altair vegafusion vl-convert-python In\u00a0[\u00a0]: Copied! <pre>import os\nimport polars as pl\nfrom datetime import datetime\nimport requests\n\n# Set your EODHD API key\nEOD_API_KEY = \"YOUR_EODHD_API_KEY_HERE\"  # Get from https://eodhd.com/cp/dashboard\n\nif EOD_API_KEY == \"YOUR_EODHD_API_KEY_HERE\":\n    print(\"\u26a0\ufe0f Please set your EODHD API key above\")\n    print(\"Get one free at: https://eodhd.com/pricing-special-10?via=crowdcent\")\n</pre> import os import polars as pl from datetime import datetime import requests  # Set your EODHD API key EOD_API_KEY = \"YOUR_EODHD_API_KEY_HERE\"  # Get from https://eodhd.com/cp/dashboard  if EOD_API_KEY == \"YOUR_EODHD_API_KEY_HERE\":     print(\"\u26a0\ufe0f Please set your EODHD API key above\")     print(\"Get one free at: https://eodhd.com/pricing-special-10?via=crowdcent\") In\u00a0[\u00a0]: Copied! <pre>def get_hyperliquid_perpetuals(include_delisted: bool = False):\n    \"\"\"Get perpetual IDs as a polars DataFrame\n\n    Args:\n        include_delisted: If True, include delisted perpetuals in the output\n    \"\"\"\n\n    url = \"https://api.hyperliquid.xyz/info\"\n    payload = {\"type\": \"meta\"}\n    special_map_dict = {\n        \"POPCAT-USD.CC\": \"POPCAT28782-USD.CC\",\n        \"VVV-USD.CC\": \"VVV.CC\",\n        \"BRETT-USD.CC\": \"BRETT29743-USD.CC\",\n        \"UNIBOT-USD.CC\": \"UNIBOT27009-USD.CC\",\n        \"ZRO-USD.CC\": \"ZRO26997-USD.CC\",\n        \"MOVE-USD.CC\": \"MOVE32452-USD.CC\",\n        \"STG-USD.CC\": \"STG18934-USD.CC\",\n        \"GOAT-USD.CC\": \"GOAT33440-USD.CC\",\n        \"PEPE-USD.CC\": \"PEPE24478-USD.CC\",\n        \"PROMPT-USD.CC\": \"PROMPT-USD.CC\",\n        \"NIL-USD.CC\": \"NIL35702-USD.CC\",\n        \"MNT-USD.CC\": \"MNT27075-USD.CC\",\n        \"ACE-USD.CC\": \"ACE28674-USD.CC\",\n        \"HYPE-USD.CC\": \"HYPE32196-USD.CC\",\n        \"IMX-USD.CC\": \"IMX10603-USD.CC\",\n        \"INIT-USD.CC\": \"INIT-USD.CC\",\n        \"PURR-USD.CC\": \"PURR34332-USD.CC\",\n        \"MOODENG-USD.CC\": \"MOODENG33093-USD.CC\",\n        \"CHILLGUY-USD.CC\": \"CHILLGUY-USD.CC\",\n        \"FARTCOIN-USD.CC\": \"FARTCOIN-USD.CC\",\n        \"GRASS-USD.CC\": \"GRASS32956-USD.CC\",\n        \"GRIFFAIN-USD.CC\": \"GRIFFAIN-USD.CC\",\n        \"MELANIA-USD.CC\": \"MELANIA35347-USD.CC\",\n        \"KAITO-USD.CC\": \"KAITO-USD.CC\",\n        \"SUI-USD.CC\": \"SUI20947-USD.CC\",\n        \"BERA-USD.CC\": \"BERA-USD.CC\",\n        \"MEW-USD.CC\": \"MEW30126-USD.CC\",\n        \"ANIME-USD.CC\": \"ANIME35319-USD.CC\",\n        \"NEIRO-USD.CC\": \"NEIRO32521-USD.CC\",\n        \"DOGS-USD.CC\": \"DOGS32698-USD.CC\",\n        \"STX-USD.CC\": \"STX4847-USD.CC\",\n        \"S-USD.CC\": \"S32684-USD.CC\",\n        \"COMP-USD.CC\": \"COMP5692-USD.CC\",\n        \"TRUMP-USD.CC\": \"TRUMP-OFFICIAL-USD.CC\",\n        \"BLAST-USD.CC\": \"BLAST28480-USD.CC\",\n        \"TAO-USD.CC\": \"TAO22974-USD.CC\",\n        \"SAGA-USD.CC\": \"SAGA30372-USD.CC\",\n        \"TON-USD.CC\": \"TON11419-USD.CC\",\n        \"BIO-USD.CC\": \"BIO.CC\",\n        \"GMX-USD.CC\": \"GMX11857-USD.CC\",\n        \"NTRN-USD.CC\": \"NTRN26680-USD.CC\",\n        \"SUPER-USD.CC\": \"SUPER8290-USD.CC\",\n        \"SCR-USD.CC\": \"SCR26998-USD.CC\",\n        \"BANANA-USD.CC\": \"BANANA28066-USD.CC\",\n        \"ME-USD.CC\": \"ME32197-USD.CC\",\n        \"GMT-USD.CC\": \"GMT18069-USD.CC\",\n        \"IO-USD.CC\": \"IO29835-USD.CC\",\n        \"ZK-USD.CC\": \"ZKSYNC.CC\",\n        \"ALT-USD.CC\": \"ALT29073-USD.CC\",\n        \"POL-USD.CC\": \"POL28321-USD.CC\",\n        \"WCT-USD.CC\": \"WCT33152-USD.CC\",\n        \"XAI-USD.CC\": \"XAI28933-USD.CC\",\n        \"JUP-USD.CC\": \"JUP29210-USD.CC\",\n        \"APE-USD.CC\": \"APE3-USD.CC\",\n        \"SPX-USD.CC\": \"SPX28081-USD.CC\",\n        \"HYPER-USD.CC\": \"HYPER36281-USD.CC\",\n        \"IP-USD.CC\": \"IP-USD.CC\",\n        \"ZORA-USD.CC\": \"ZORA35931-USD.CC\",\n        \"PEOPLE-USD.CC\": \"PEOPLE-USD.CC\",\n        \"BABY-USD.CC\": \"BABY32198-USD.CC\",\n        \"ARB-USD.CC\": \"ARB11841-USD.CC\",\n        \"UNI-USD.CC\": \"UNI7083-USD.CC\",\n        \"OMNI-USD.CC\": \"OMNI30315-USD.CC\",\n        \"SOPH-USD.CC\": \"SOPHON-USD.CC\",\n        \"NEIROETH-USD.CC\": \"NEIRO-USD.CC\",\n        \"APT-USD.CC\": \"APT21794-USD.CC\",\n        \"STRK-USD.CC\": \"STRK22691-USD.CC\",\n        \"RESOLV-USD.CC\": \"RESOLV-USD.CC\",\n        \"TST-USD.CC\": \"TST35647-USD.CC\",\n        \"PUMP-USD.CC\": \"PUMP29601-USD.CC\",\n        \"WLFI-USD.CC\": \"WLFI33251-USD.CC\",\n        \"ASTER-USD.CC\": \"ASTER36341-USD.CC\",\n        \"SKY-USD.CC\": \"SKY33038-USD.CC\",\n    }\n    response = requests.post(url, json=payload)\n    response.raise_for_status()\n\n    data = response.json()\n    universe = data.get(\"universe\", [])\n\n    # Extract names of perpetuals based on delisted status\n    perpetual_ids = [\n        perp[\"name\"]\n        for perp in universe\n        if include_delisted or not perp.get(\"isDelisted\", False)\n    ]\n\n    # Create DataFrame with just id column\n    df = pl.DataFrame({\"id\": perpetual_ids})\n    df = df.with_columns(\n        (pl.col(\"id\") + \"-USD.CC\")\n        .str.replace(\"k\", \"\")\n        .replace(special_map_dict)\n        .alias(\"eodhd_id\")\n    )\n\n    return df\n\n\n# Get the Hyperliquid universe\nperpetuals_df = get_hyperliquid_perpetuals(include_delisted=True)\nprint(perpetuals_df)\n# Extract ticker lists\neodhd_tickers = perpetuals_df[\"eodhd_id\"].to_list()\nid_mapping = dict(perpetuals_df.select(\"eodhd_id\", \"id\").iter_rows())\n\nprint(f\"\\n\ud83d\udcca Dataset will include {len(eodhd_tickers)} cryptocurrencies\")\nprint(f\"Sample tickers: {eodhd_tickers[:10]}\")\nprint(f\"Sample IDs: {list(id_mapping.values())[:10]}\")\n</pre> def get_hyperliquid_perpetuals(include_delisted: bool = False):     \"\"\"Get perpetual IDs as a polars DataFrame      Args:         include_delisted: If True, include delisted perpetuals in the output     \"\"\"      url = \"https://api.hyperliquid.xyz/info\"     payload = {\"type\": \"meta\"}     special_map_dict = {         \"POPCAT-USD.CC\": \"POPCAT28782-USD.CC\",         \"VVV-USD.CC\": \"VVV.CC\",         \"BRETT-USD.CC\": \"BRETT29743-USD.CC\",         \"UNIBOT-USD.CC\": \"UNIBOT27009-USD.CC\",         \"ZRO-USD.CC\": \"ZRO26997-USD.CC\",         \"MOVE-USD.CC\": \"MOVE32452-USD.CC\",         \"STG-USD.CC\": \"STG18934-USD.CC\",         \"GOAT-USD.CC\": \"GOAT33440-USD.CC\",         \"PEPE-USD.CC\": \"PEPE24478-USD.CC\",         \"PROMPT-USD.CC\": \"PROMPT-USD.CC\",         \"NIL-USD.CC\": \"NIL35702-USD.CC\",         \"MNT-USD.CC\": \"MNT27075-USD.CC\",         \"ACE-USD.CC\": \"ACE28674-USD.CC\",         \"HYPE-USD.CC\": \"HYPE32196-USD.CC\",         \"IMX-USD.CC\": \"IMX10603-USD.CC\",         \"INIT-USD.CC\": \"INIT-USD.CC\",         \"PURR-USD.CC\": \"PURR34332-USD.CC\",         \"MOODENG-USD.CC\": \"MOODENG33093-USD.CC\",         \"CHILLGUY-USD.CC\": \"CHILLGUY-USD.CC\",         \"FARTCOIN-USD.CC\": \"FARTCOIN-USD.CC\",         \"GRASS-USD.CC\": \"GRASS32956-USD.CC\",         \"GRIFFAIN-USD.CC\": \"GRIFFAIN-USD.CC\",         \"MELANIA-USD.CC\": \"MELANIA35347-USD.CC\",         \"KAITO-USD.CC\": \"KAITO-USD.CC\",         \"SUI-USD.CC\": \"SUI20947-USD.CC\",         \"BERA-USD.CC\": \"BERA-USD.CC\",         \"MEW-USD.CC\": \"MEW30126-USD.CC\",         \"ANIME-USD.CC\": \"ANIME35319-USD.CC\",         \"NEIRO-USD.CC\": \"NEIRO32521-USD.CC\",         \"DOGS-USD.CC\": \"DOGS32698-USD.CC\",         \"STX-USD.CC\": \"STX4847-USD.CC\",         \"S-USD.CC\": \"S32684-USD.CC\",         \"COMP-USD.CC\": \"COMP5692-USD.CC\",         \"TRUMP-USD.CC\": \"TRUMP-OFFICIAL-USD.CC\",         \"BLAST-USD.CC\": \"BLAST28480-USD.CC\",         \"TAO-USD.CC\": \"TAO22974-USD.CC\",         \"SAGA-USD.CC\": \"SAGA30372-USD.CC\",         \"TON-USD.CC\": \"TON11419-USD.CC\",         \"BIO-USD.CC\": \"BIO.CC\",         \"GMX-USD.CC\": \"GMX11857-USD.CC\",         \"NTRN-USD.CC\": \"NTRN26680-USD.CC\",         \"SUPER-USD.CC\": \"SUPER8290-USD.CC\",         \"SCR-USD.CC\": \"SCR26998-USD.CC\",         \"BANANA-USD.CC\": \"BANANA28066-USD.CC\",         \"ME-USD.CC\": \"ME32197-USD.CC\",         \"GMT-USD.CC\": \"GMT18069-USD.CC\",         \"IO-USD.CC\": \"IO29835-USD.CC\",         \"ZK-USD.CC\": \"ZKSYNC.CC\",         \"ALT-USD.CC\": \"ALT29073-USD.CC\",         \"POL-USD.CC\": \"POL28321-USD.CC\",         \"WCT-USD.CC\": \"WCT33152-USD.CC\",         \"XAI-USD.CC\": \"XAI28933-USD.CC\",         \"JUP-USD.CC\": \"JUP29210-USD.CC\",         \"APE-USD.CC\": \"APE3-USD.CC\",         \"SPX-USD.CC\": \"SPX28081-USD.CC\",         \"HYPER-USD.CC\": \"HYPER36281-USD.CC\",         \"IP-USD.CC\": \"IP-USD.CC\",         \"ZORA-USD.CC\": \"ZORA35931-USD.CC\",         \"PEOPLE-USD.CC\": \"PEOPLE-USD.CC\",         \"BABY-USD.CC\": \"BABY32198-USD.CC\",         \"ARB-USD.CC\": \"ARB11841-USD.CC\",         \"UNI-USD.CC\": \"UNI7083-USD.CC\",         \"OMNI-USD.CC\": \"OMNI30315-USD.CC\",         \"SOPH-USD.CC\": \"SOPHON-USD.CC\",         \"NEIROETH-USD.CC\": \"NEIRO-USD.CC\",         \"APT-USD.CC\": \"APT21794-USD.CC\",         \"STRK-USD.CC\": \"STRK22691-USD.CC\",         \"RESOLV-USD.CC\": \"RESOLV-USD.CC\",         \"TST-USD.CC\": \"TST35647-USD.CC\",         \"PUMP-USD.CC\": \"PUMP29601-USD.CC\",         \"WLFI-USD.CC\": \"WLFI33251-USD.CC\",         \"ASTER-USD.CC\": \"ASTER36341-USD.CC\",         \"SKY-USD.CC\": \"SKY33038-USD.CC\",     }     response = requests.post(url, json=payload)     response.raise_for_status()      data = response.json()     universe = data.get(\"universe\", [])      # Extract names of perpetuals based on delisted status     perpetual_ids = [         perp[\"name\"]         for perp in universe         if include_delisted or not perp.get(\"isDelisted\", False)     ]      # Create DataFrame with just id column     df = pl.DataFrame({\"id\": perpetual_ids})     df = df.with_columns(         (pl.col(\"id\") + \"-USD.CC\")         .str.replace(\"k\", \"\")         .replace(special_map_dict)         .alias(\"eodhd_id\")     )      return df   # Get the Hyperliquid universe perpetuals_df = get_hyperliquid_perpetuals(include_delisted=True) print(perpetuals_df) # Extract ticker lists eodhd_tickers = perpetuals_df[\"eodhd_id\"].to_list() id_mapping = dict(perpetuals_df.select(\"eodhd_id\", \"id\").iter_rows())  print(f\"\\n\ud83d\udcca Dataset will include {len(eodhd_tickers)} cryptocurrencies\") print(f\"Sample tickers: {eodhd_tickers[:10]}\") print(f\"Sample IDs: {list(id_mapping.values())[:10]}\") <pre>shape: (228, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id    \u2506 eodhd_id     \u2502\n\u2502 ---   \u2506 ---          \u2502\n\u2502 str   \u2506 str          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 BTC   \u2506 BTC-USD.CC   \u2502\n\u2502 ETH   \u2506 ETH-USD.CC   \u2502\n\u2502 ATOM  \u2506 ATOM-USD.CC  \u2502\n\u2502 MATIC \u2506 MATIC-USD.CC \u2502\n\u2502 DYDX  \u2506 DYDX-USD.CC  \u2502\n\u2502 \u2026     \u2506 \u2026            \u2502\n\u2502 LIT   \u2506 LIT-USD.CC   \u2502\n\u2502 XMR   \u2506 XMR-USD.CC   \u2502\n\u2502 AXS   \u2506 AXS-USD.CC   \u2502\n\u2502 DASH  \u2506 DASH-USD.CC  \u2502\n\u2502 SKR   \u2506 SKR-USD.CC   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udcca Dataset will include 228 cryptocurrencies\nSample tickers: ['BTC-USD.CC', 'ETH-USD.CC', 'ATOM-USD.CC', 'MATIC-USD.CC', 'DYDX-USD.CC', 'SOL-USD.CC', 'AVAX-USD.CC', 'BNB-USD.CC', 'APE3-USD.CC', 'OP-USD.CC']\nSample IDs: ['BTC', 'ETH', 'ATOM', 'MATIC', 'DYDX', 'SOL', 'AVAX', 'BNB', 'APE', 'OP']\n</pre> In\u00a0[\u00a0]: Copied! <pre>from numerblox.download import EODDownloader\n\n# Set date range\nstart_date = \"20200101\"\nend_date = datetime.now()\n\n# Initialize EOD downloader\neod = EODDownloader(directory_path=\"data\", key=EOD_API_KEY, tickers=eodhd_tickers)\neod.end_date = end_date\n\nprint(\"Downloading historical data...\")\nprint(\"This may take a few minutes depending on number of tickers\")\n\n# Download data\neod.download_training_data(start=start_date)\n\n# Load the downloaded data\nfilename = f\"data/eod_{start_date}_{end_date.strftime('%Y%m%d')}.parquet\"\neod_df = pl.read_parquet(filename)\neod_df = eod_df.with_columns(pl.col(\"date\").str.to_datetime())\n\n# Add clean ID column using Hyperliquid naming\neod_df = eod_df.with_columns(pl.col(\"ticker\").replace(id_mapping).alias(\"id\"))\n\n# Check coverage\nrequested_tickers = len(eodhd_tickers)\ndownloaded_tickers = eod_df[\"ticker\"].n_unique()\ncoverage_pct = (downloaded_tickers / requested_tickers) * 100\n\nprint(f\"\u2705 Downloaded {len(eod_df)} rows for {downloaded_tickers} tickers\")\nprint(\n    f\"\ud83d\udcca Coverage: {downloaded_tickers}/{requested_tickers} tickers ({coverage_pct:.1f}%)\"\n)\nprint(f\"\ud83d\udcc5 Date range: {eod_df['date'].min()} to {eod_df['date'].max()}\")\n\neod_df.head()\n</pre> from numerblox.download import EODDownloader  # Set date range start_date = \"20200101\" end_date = datetime.now()  # Initialize EOD downloader eod = EODDownloader(directory_path=\"data\", key=EOD_API_KEY, tickers=eodhd_tickers) eod.end_date = end_date  print(\"Downloading historical data...\") print(\"This may take a few minutes depending on number of tickers\")  # Download data eod.download_training_data(start=start_date)  # Load the downloaded data filename = f\"data/eod_{start_date}_{end_date.strftime('%Y%m%d')}.parquet\" eod_df = pl.read_parquet(filename) eod_df = eod_df.with_columns(pl.col(\"date\").str.to_datetime())  # Add clean ID column using Hyperliquid naming eod_df = eod_df.with_columns(pl.col(\"ticker\").replace(id_mapping).alias(\"id\"))  # Check coverage requested_tickers = len(eodhd_tickers) downloaded_tickers = eod_df[\"ticker\"].n_unique() coverage_pct = (downloaded_tickers / requested_tickers) * 100  print(f\"\u2705 Downloaded {len(eod_df)} rows for {downloaded_tickers} tickers\") print(     f\"\ud83d\udcca Coverage: {downloaded_tickers}/{requested_tickers} tickers ({coverage_pct:.1f}%)\" ) print(f\"\ud83d\udcc5 Date range: {eod_df['date'].min()} to {eod_df['date'].max()}\")  eod_df.head() <pre>Downloading historical data...\nThis may take a few minutes depending on number of tickers\n</pre> <pre>EOD price data extraction:   0%|          | 0/228 [00:00&lt;?, ?it/s]</pre> <pre>WARNING: Date pull failed on ticker: 'HPOS-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/HPOS-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'OX-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/OX-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'FRIEND-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/FRIEND-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'SHIA-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/FRIEND-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'CANTO-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/CANTO-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'NFTI-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/NFTI-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'PANDORA-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/PANDORA-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'AI-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/AI-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'JELLY-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/JELLY-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'HEMI-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/HEMI-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'APEX-USD.CC'. Exception: \"None of ['date'] are in the columns\"\nWARNING: Date pull failed on ticker: 'AERO-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/AERO-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'STABLE-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/STABLE-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\nWARNING: Date pull failed on ticker: 'SKR-USD.CC'. Exception: 404 Client Error: Not Found for url: https://eodhistoricaldata.com/api/eod/SKR-USD.CC?period=d&amp;to=2026-02-04+21%3A33%3A10.856500&amp;fmt=json&amp;api_token=621661e8653533.21413374&amp;from=20200101\n\u2705 Downloaded 256943 rows for 214 tickers\n\ud83d\udcca Coverage: 214/228 tickers (93.9%)\n\ud83d\udcc5 Date range: 2020-01-01 00:00:00 to 2026-02-04 00:00:00\n</pre> Out[\u00a0]: shape: (5, 9)openhighlowcloseadjusted_closevolumetickerdateidf64f64f64f64f64i64strdatetime[\u03bcs]str1.8984e-107.6549e-91.8984e-107.6203e-97.6203e-91988\"PEPE24478-USD.CC\"2023-04-15 00:00:00\"kPEPE\"6.6731e-92.3219e-83.4868e-92.0852e-82.0852e-81630\"PEPE24478-USD.CC\"2023-04-16 00:00:00\"kPEPE\"2.0664e-87.9566e-81.6641e-86.6375e-86.6375e-846385210\"PEPE24478-USD.CC\"2023-04-17 00:00:00\"kPEPE\"6.5796e-82.2615e-75.4839e-81.8607e-71.8607e-773954201\"PEPE24478-USD.CC\"2023-04-18 00:00:00\"kPEPE\"1.8730e-73.6779e-71.3686e-72.6419e-72.6419e-7170056322\"PEPE24478-USD.CC\"2023-04-19 00:00:00\"kPEPE\" In\u00a0[\u00a0]: Copied! <pre>import altair as alt\n\nalt.data_transformers.enable(\"vegafusion\")\n\n# Select top tickers by data coverage for visualization\ntop_tickers = (\n    eod_df.group_by(\"id\")\n    .agg(pl.col(\"date\").count().alias(\"count\"))\n    .sort(\"count\", descending=True)\n    .head(20)[\"id\"]\n    .to_list()\n)\n\n# Create visualization of raw prices\nviz_df = eod_df.filter(pl.col(\"id\").is_in(top_tickers)).to_pandas()\n\nchart = (\n    alt.Chart(viz_df)\n    .mark_line(opacity=0.6)\n    .encode(\n        x=alt.X(\"date:T\", title=\"Date\"),\n        y=alt.Y(\"close:Q\", title=\"Close Price (USD)\", scale=alt.Scale(type=\"log\")),\n        color=alt.Color(\"id:N\", title=\"Ticker\", legend=alt.Legend(columns=2)),\n        tooltip=[\"id:N\", \"date:T\", \"close:Q\"],\n    )\n    .properties(\n        width=700, height=300, title=\"Input: Raw Stock Prices Over Time (Log Scale)\"\n    )\n)\n\nchart\n</pre> import altair as alt  alt.data_transformers.enable(\"vegafusion\")  # Select top tickers by data coverage for visualization top_tickers = (     eod_df.group_by(\"id\")     .agg(pl.col(\"date\").count().alias(\"count\"))     .sort(\"count\", descending=True)     .head(20)[\"id\"]     .to_list() )  # Create visualization of raw prices viz_df = eod_df.filter(pl.col(\"id\").is_in(top_tickers)).to_pandas()  chart = (     alt.Chart(viz_df)     .mark_line(opacity=0.6)     .encode(         x=alt.X(\"date:T\", title=\"Date\"),         y=alt.Y(\"close:Q\", title=\"Close Price (USD)\", scale=alt.Scale(type=\"log\")),         color=alt.Color(\"id:N\", title=\"Ticker\", legend=alt.Legend(columns=2)),         tooltip=[\"id:N\", \"date:T\", \"close:Q\"],     )     .properties(         width=700, height=300, title=\"Input: Raw Stock Prices Over Time (Log Scale)\"     ) )  chart  Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>from crowdcent_challenge.scoring import create_ranking_targets\n\n# Create standard 10d and 30d ranking targets using CrowdCent's official function\nprint(\"\ud83c\udfaf Creating ranking targets using CrowdCent's methodology...\")\n\ndf = create_ranking_targets(\n    eod_df,\n    horizons=[10, 30],  # Standard CrowdCent horizons\n    price_col=\"close\",\n    date_col=\"date\",\n    ticker_col=\"ticker\",\n    return_raw_returns=False,  # We only need the normalized targets\n    drop_incomplete=True,  # Drop rows without complete targets\n)\n\nprint(f\"\u2705 Created targets: target_10d, target_30d\")\nprint(f\"\ud83d\udcca Rows with complete targets: {len(df):,}\")\n\n# Show sample targets\nprint(\"\\nSample targets:\")\ndf.select([\"date\", \"ticker\", \"id\", \"target_10d\", \"target_30d\"]).tail(10)\n</pre> from crowdcent_challenge.scoring import create_ranking_targets  # Create standard 10d and 30d ranking targets using CrowdCent's official function print(\"\ud83c\udfaf Creating ranking targets using CrowdCent's methodology...\")  df = create_ranking_targets(     eod_df,     horizons=[10, 30],  # Standard CrowdCent horizons     price_col=\"close\",     date_col=\"date\",     ticker_col=\"ticker\",     return_raw_returns=False,  # We only need the normalized targets     drop_incomplete=True,  # Drop rows without complete targets )  print(f\"\u2705 Created targets: target_10d, target_30d\") print(f\"\ud83d\udcca Rows with complete targets: {len(df):,}\")  # Show sample targets print(\"\\nSample targets:\") df.select([\"date\", \"ticker\", \"id\", \"target_10d\", \"target_30d\"]).tail(10) <pre>\ud83c\udfaf Creating ranking targets using CrowdCent's methodology...\n\u2705 Created targets: target_10d, target_30d\n\ud83d\udcca Rows with complete targets: 250,315\n\nSample targets:\n</pre> Out[\u00a0]: shape: (10, 5)datetickeridtarget_10dtarget_30ddatetime[\u03bcs]strstrf64f642025-12-26 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.5862070.9802962025-12-27 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.6600990.9802962025-12-28 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.7438420.9852222025-12-29 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.7290640.9950742025-12-30 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.7832510.9852222025-12-31 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.8374380.9852222026-01-01 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.8522170.9802962026-01-02 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.9014780.9802962026-01-03 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.9145730.9798992026-01-04 00:00:00\"ZRO26997-USD.CC\"\"ZRO\"0.9480520.987013 In\u00a0[\u00a0]: Copied! <pre># Sample recent data for visualization\nsample_dates = df[\"date\"].unique().sort().tail(30).to_list()\ntarget_viz_df = df.filter(pl.col(\"date\").is_in(sample_dates)).to_pandas()\n\n# Create histograms for both targets\nhist_10d = (\n    alt.Chart(target_viz_df)\n    .mark_bar(opacity=0.7)\n    .encode(\n        x=alt.X(\"target_10d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 10d Value\"),\n        y=alt.Y(\"count()\", title=\"Count\"),\n        tooltip=[\"count()\"],\n    )\n    .properties(\n        width=350, height=200, title=\"10-Day Target Distribution (Last 30 Days)\"\n    )\n)\n\nhist_30d = (\n    alt.Chart(target_viz_df)\n    .mark_bar(opacity=0.7, color=\"orange\")\n    .encode(\n        x=alt.X(\"target_30d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 30d Value\"),\n        y=alt.Y(\"count()\", title=\"Count\"),\n        tooltip=[\"count()\"],\n    )\n    .properties(\n        width=350, height=200, title=\"30-Day Target Distribution (Last 30 Days)\"\n    )\n)\n\nalt.hconcat(hist_10d, hist_30d)\n</pre> # Sample recent data for visualization sample_dates = df[\"date\"].unique().sort().tail(30).to_list() target_viz_df = df.filter(pl.col(\"date\").is_in(sample_dates)).to_pandas()  # Create histograms for both targets hist_10d = (     alt.Chart(target_viz_df)     .mark_bar(opacity=0.7)     .encode(         x=alt.X(\"target_10d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 10d Value\"),         y=alt.Y(\"count()\", title=\"Count\"),         tooltip=[\"count()\"],     )     .properties(         width=350, height=200, title=\"10-Day Target Distribution (Last 30 Days)\"     ) )  hist_30d = (     alt.Chart(target_viz_df)     .mark_bar(opacity=0.7, color=\"orange\")     .encode(         x=alt.X(\"target_30d:Q\", bin=alt.Bin(maxbins=30), title=\"Target 30d Value\"),         y=alt.Y(\"count()\", title=\"Count\"),         tooltip=[\"count()\"],     )     .properties(         width=350, height=200, title=\"30-Day Target Distribution (Last 30 Days)\"     ) )  alt.hconcat(hist_10d, hist_30d)  Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>from sklearn import set_config\nfrom sklearn.pipeline import make_pipeline\nfrom centimators.feature_transformers import (\n    LogReturnTransformer,\n    RankTransformer,\n    LagTransformer,\n    MovingAverageTransformer,\n)\n\n# Enable metadata routing for sklearn\nset_config(enable_metadata_routing=True)\n\nprint(\"\ud83d\udd27 Building feature engineering pipeline...\")\n\n# Define transformers with custom parameters\nlog_return_transformer = LogReturnTransformer().set_transform_request(\n    ticker_series=True\n)\nranker = RankTransformer().set_transform_request(date_series=True)\nma_transformer = MovingAverageTransformer(\n    windows=[2, 10]  # Custom moving average windows\n).set_transform_request(ticker_series=True)\nlagger = LagTransformer(windows=[0, 5, 10, 15, 20]).set_transform_request(\n    ticker_series=True\n)  # Custom lag windows\n\n# Create feature pipeline\nfeature_pipeline = make_pipeline(\n    log_return_transformer, ranker, ma_transformer, lagger, verbose=True\n)\n\nfeature_pipeline\n</pre> from sklearn import set_config from sklearn.pipeline import make_pipeline from centimators.feature_transformers import (     LogReturnTransformer,     RankTransformer,     LagTransformer,     MovingAverageTransformer, )  # Enable metadata routing for sklearn set_config(enable_metadata_routing=True)  print(\"\ud83d\udd27 Building feature engineering pipeline...\")  # Define transformers with custom parameters log_return_transformer = LogReturnTransformer().set_transform_request(     ticker_series=True ) ranker = RankTransformer().set_transform_request(date_series=True) ma_transformer = MovingAverageTransformer(     windows=[2, 10]  # Custom moving average windows ).set_transform_request(ticker_series=True) lagger = LagTransformer(windows=[0, 5, 10, 15, 20]).set_transform_request(     ticker_series=True )  # Custom lag windows  # Create feature pipeline feature_pipeline = make_pipeline(     log_return_transformer, ranker, ma_transformer, lagger, verbose=True )  feature_pipeline <pre>\ud83d\udd27 Building feature engineering pipeline...\n</pre> Out[\u00a0]: <pre>Pipeline(steps=[('logreturntransformer', LogReturnTransformer()),\n                ('ranktransformer', RankTransformer()),\n                ('movingaveragetransformer',\n                 MovingAverageTransformer(windows=[2, 10])),\n                ('lagtransformer', LagTransformer(windows=[20, 15, 10, 5, 0]))],\n         verbose=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted Parameters              steps             steps: list of tuplesList of (name of step, estimator) tuples that are to be chained insequential order. To be compatible with the scikit-learn API, all stepsmust define `fit`. All non-last steps must also define `transform`. See:ref:`Combining Estimators ` for more details. [('logreturntransformer', ...), ('ranktransformer', ...), ...]              transform_input             transform_input: list of str, default=NoneThe names of the :term:`metadata` parameters that should be transformed by thepipeline before passing it to the step consuming it.This enables transforming some input arguments to ``fit`` (other than ``X``)to be transformed by the steps of the pipeline up to the step which requiresthem. Requirement is defined via :ref:`metadata routing `.For instance, this can be used to pass a validation set through the pipeline.You can only set this if metadata routing is enabled, which youcan enable using ``sklearn.set_config(enable_metadata_routing=True)``... versionadded:: 1.6 None              memory             memory: str or object with the joblib.Memory interface, default=NoneUsed to cache the fitted transformers of the pipeline. The last stepwill never be cached, even if it is a transformer. By default, nocaching is performed. If a string is given, it is the path to thecaching directory. Enabling caching triggers a clone of the transformersbefore fitting. Therefore, the transformer instance given to thepipeline cannot be inspected directly. Use the attribute ``named_steps``or ``steps`` to inspect estimators within the pipeline. Caching thetransformers is advantageous when fitting is time consuming. See:ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`for an example on how to enable caching. None              verbose             verbose: bool, default=FalseIf True, the time elapsed while fitting each step will be printed as itis completed. True LogReturnTransformer Parameters feature_names None RankTransformer Parameters feature_names None MovingAverageTransformer Parameters windows [2, 10] feature_names None LagTransformer Parameters windows [20, 15, ...] feature_names None In\u00a0[\u00a0]: Copied! <pre># Apply feature engineering\ninput_features = [\"close\", \"open\", \"high\", \"low\", \"volume\"]\n\nprint(f\"Transforming {input_features} into features...\")\n\nfeature_df = feature_pipeline.fit_transform(\n    df[input_features], date_series=df[\"date\"], ticker_series=df[\"ticker\"]\n)\n\n# Get feature names and add to dataframe\nfeature_names = feature_pipeline.get_feature_names_out()\ndf = df.with_columns(feature_df).drop_nulls(subset=feature_names)\n\nprint(f\"\\n\u2705 Created {len(feature_names)} features\")\nprint(f\"Final dataset shape: {df.shape}\")\nprint(f\"\\nFeature names:\")\nfor i, name in enumerate(feature_names[:10]):\n    print(f\"  {name}\")\nif len(feature_names) &gt; 10:\n    print(f\"  ... and {len(feature_names) - 10} more\")\n\ndf.head()\n</pre> # Apply feature engineering input_features = [\"close\", \"open\", \"high\", \"low\", \"volume\"]  print(f\"Transforming {input_features} into features...\")  feature_df = feature_pipeline.fit_transform(     df[input_features], date_series=df[\"date\"], ticker_series=df[\"ticker\"] )  # Get feature names and add to dataframe feature_names = feature_pipeline.get_feature_names_out() df = df.with_columns(feature_df).drop_nulls(subset=feature_names)  print(f\"\\n\u2705 Created {len(feature_names)} features\") print(f\"Final dataset shape: {df.shape}\") print(f\"\\nFeature names:\") for i, name in enumerate(feature_names[:10]):     print(f\"  {name}\") if len(feature_names) &gt; 10:     print(f\"  ... and {len(feature_names) - 10} more\")  df.head() <pre>Transforming ['close', 'open', 'high', 'low', 'volume'] into features...\n[Pipeline]  (step 1 of 4) Processing logreturntransformer, total=   0.0s\n[Pipeline] ... (step 2 of 4) Processing ranktransformer, total=   0.0s\n[Pipeline]  (step 3 of 4) Processing movingaveragetransformer, total=   0.0s\n[Pipeline] .... (step 4 of 4) Processing lagtransformer, total=   0.0s\n\n\u2705 Created 50 features\nFinal dataset shape: (243925, 61)\n\nFeature names:\n  close_logreturn_rank_ma2_lag20\n  close_logreturn_rank_ma10_lag20\n  open_logreturn_rank_ma2_lag20\n  open_logreturn_rank_ma10_lag20\n  high_logreturn_rank_ma2_lag20\n  high_logreturn_rank_ma10_lag20\n  low_logreturn_rank_ma2_lag20\n  low_logreturn_rank_ma10_lag20\n  volume_logreturn_rank_ma2_lag20\n  volume_logreturn_rank_ma10_lag20\n  ... and 40 more\n</pre> Out[\u00a0]: shape: (5, 61)openhighlowcloseadjusted_closevolumetickerdateidtarget_10dtarget_30dclose_logreturn_rank_ma2_lag20close_logreturn_rank_ma10_lag20open_logreturn_rank_ma2_lag20open_logreturn_rank_ma10_lag20high_logreturn_rank_ma2_lag20high_logreturn_rank_ma10_lag20low_logreturn_rank_ma2_lag20low_logreturn_rank_ma10_lag20volume_logreturn_rank_ma2_lag20volume_logreturn_rank_ma10_lag20close_logreturn_rank_ma2_lag15close_logreturn_rank_ma10_lag15open_logreturn_rank_ma2_lag15open_logreturn_rank_ma10_lag15high_logreturn_rank_ma2_lag15high_logreturn_rank_ma10_lag15low_logreturn_rank_ma2_lag15low_logreturn_rank_ma10_lag15volume_logreturn_rank_ma2_lag15volume_logreturn_rank_ma10_lag15close_logreturn_rank_ma2_lag10close_logreturn_rank_ma10_lag10open_logreturn_rank_ma2_lag10open_logreturn_rank_ma10_lag10high_logreturn_rank_ma2_lag10high_logreturn_rank_ma10_lag10low_logreturn_rank_ma2_lag10low_logreturn_rank_ma10_lag10volume_logreturn_rank_ma2_lag10volume_logreturn_rank_ma10_lag10close_logreturn_rank_ma2_lag5close_logreturn_rank_ma10_lag5open_logreturn_rank_ma2_lag5open_logreturn_rank_ma10_lag5high_logreturn_rank_ma2_lag5high_logreturn_rank_ma10_lag5low_logreturn_rank_ma2_lag5low_logreturn_rank_ma10_lag5volume_logreturn_rank_ma2_lag5volume_logreturn_rank_ma10_lag5close_logreturn_rank_ma2_lag0close_logreturn_rank_ma10_lag0open_logreturn_rank_ma2_lag0open_logreturn_rank_ma10_lag0high_logreturn_rank_ma2_lag0high_logreturn_rank_ma10_lag0low_logreturn_rank_ma2_lag0low_logreturn_rank_ma10_lag0volume_logreturn_rank_ma2_lag0volume_logreturn_rank_ma10_lag0f64f64f64f64f64i64strdatetime[\u03bcs]strf64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f641.8886951.927571.758521.802231.8022352852006\"0G-USD.CC\"2025-10-22 00:00:00\"0G\"0.0098040.5882350.450980.2956030.4436270.386260.4460780.1974960.4779410.4475250.4705880.4852820.5416670.3322980.4632350.3857580.4705880.3653710.6250.507850.4705880.4435170.3333330.3921020.6936270.3857580.4485290.4619390.1127450.4441250.50.4410660.3921570.3063730.3897060.3176470.4901960.3955880.3406860.225490.5588240.475490.4362750.3303920.8210780.324020.5024510.4058820.5833330.2583330.4975490.5441181.8021521.8345031.7202981.7606741.76067442306131\"0G-USD.CC\"2025-10-23 00:00:00\"0G\"0.0735290.6372550.524510.2965880.4508610.2955720.5000240.2137620.4903160.4426470.4927310.4848230.1789220.3234740.5441180.3328170.0833330.3668410.2524510.431380.50.4572430.200980.3215140.3308820.3916640.1813730.3918360.0735290.3588070.4632350.3537950.1053920.3034310.3921570.3053920.4313730.40.1617650.2367650.524510.4289220.100490.3034310.4387250.3308820.0343140.3828430.100490.250.3308820.61.7606741.8000951.7306661.7646621.76466245293686\"0G-USD.CC\"2025-10-24 00:00:00\"0G\"0.0588240.5147060.5024390.2960910.524390.2965570.8170730.2766740.9560980.4466260.5073170.4887160.1127450.3357290.1789220.3235030.1838240.3854690.0637250.4362820.4705880.4057720.2058820.3327910.200980.3210760.4166670.3818580.1372550.2803560.4142160.4224460.075980.299020.1053920.3029410.1250.3838240.100490.2328430.5416670.4897060.100490.3093140.100490.3039220.0539220.3333330.0955880.250.6421570.5897061.7646621.7716181.6927781.7551591.75515934671902\"0G-USD.CC\"2025-10-25 00:00:00\"0G\"0.024510.50.0560980.3053440.5024390.2960590.3268290.2766640.5170730.4529580.1292680.5047650.4828430.4156310.1102940.3357580.5073530.4614490.4534310.5171640.1568630.4351840.1299020.3362750.2058820.3323530.325980.3916670.1348040.2823530.649510.4578430.5147060.3098040.075980.2985290.4828430.3950980.0906860.1642160.7352940.5446080.2303920.3235290.0980390.3093140.0857840.3348040.100490.2431370.5906860.5882351.7551591.7915961.7234521.7626491.76264927081331\"0G-USD.CC\"2025-10-26 00:00:00\"0G\"0.0343140.5833330.4629720.3857290.0563610.3053660.4534070.3624290.4458990.532850.5411530.4974390.6960780.386220.4828430.415660.7965690.4555670.5147060.4421640.2352940.3596940.3897060.3181370.1299020.3357840.1960780.3303920.3186270.2549020.549020.424020.8186270.3235290.5147060.3093140.9387250.4122550.5735290.2446080.9191180.6264710.1936270.2700980.2279410.3235290.1642160.3269610.4142160.2691180.325980.545098 In\u00a0[\u00a0]: Copied! <pre># Select a few representative features to visualize\nsample_features = [\n    \"close_logreturn_rank_ma10_lag0\",  # Most recent smoothed close feature\n    \"volume_logreturn_rank_ma10_lag0\",  # Most recent smoothed volume feature\n]\n\n# Filter to top tickers and recent dates for cleaner visualization\nrecent_dates = df[\"date\"].unique().sort().tail(100).to_list()\nfeature_viz_df = (\n    df.filter(pl.col(\"id\").is_in(top_tickers[:10]))\n    .filter(pl.col(\"date\").is_in(recent_dates))\n    .select([\"id\", \"date\"] + sample_features)\n    .to_pandas()\n)\n\n# Melt for plotting multiple features\nfeature_viz_melted = feature_viz_df.melt(\n    id_vars=[\"id\", \"date\"],\n    value_vars=sample_features,\n    var_name=\"feature\",\n    value_name=\"value\",\n)\n\n# Create feature visualization\nfeature_chart = (\n    alt.Chart(feature_viz_melted)\n    .mark_line(opacity=0.6)\n    .encode(\n        x=alt.X(\"date:T\", title=\"Date\"),\n        y=alt.Y(\n            \"value:Q\",\n            title=\"Normalized Feature Value [0, 1]\",\n            scale=alt.Scale(domain=[0, 1]),\n        ),\n        color=alt.Color(\"id:N\", title=\"Ticker\"),\n        strokeDash=alt.StrokeDash(\"feature:N\", title=\"Feature Type\"),\n        tooltip=[\"id:N\", \"date:T\", \"feature:N\", \"value:Q\"],\n    )\n    .properties(\n        width=700,\n        height=300,\n        title=\"Pipeline Output: Normalized/Smoothed Features (Recent 100 Days)\",\n    )\n)\n\nfeature_chart\n</pre> # Select a few representative features to visualize sample_features = [     \"close_logreturn_rank_ma10_lag0\",  # Most recent smoothed close feature     \"volume_logreturn_rank_ma10_lag0\",  # Most recent smoothed volume feature ]  # Filter to top tickers and recent dates for cleaner visualization recent_dates = df[\"date\"].unique().sort().tail(100).to_list() feature_viz_df = (     df.filter(pl.col(\"id\").is_in(top_tickers[:10]))     .filter(pl.col(\"date\").is_in(recent_dates))     .select([\"id\", \"date\"] + sample_features)     .to_pandas() )  # Melt for plotting multiple features feature_viz_melted = feature_viz_df.melt(     id_vars=[\"id\", \"date\"],     value_vars=sample_features,     var_name=\"feature\",     value_name=\"value\", )  # Create feature visualization feature_chart = (     alt.Chart(feature_viz_melted)     .mark_line(opacity=0.6)     .encode(         x=alt.X(\"date:T\", title=\"Date\"),         y=alt.Y(             \"value:Q\",             title=\"Normalized Feature Value [0, 1]\",             scale=alt.Scale(domain=[0, 1]),         ),         color=alt.Color(\"id:N\", title=\"Ticker\"),         strokeDash=alt.StrokeDash(\"feature:N\", title=\"Feature Type\"),         tooltip=[\"id:N\", \"date:T\", \"feature:N\", \"value:Q\"],     )     .properties(         width=700,         height=300,         title=\"Pipeline Output: Normalized/Smoothed Features (Recent 100 Days)\",     ) )  feature_chart  Out[\u00a0]: <p>What changed after the pipeline:</p> <ul> <li>All features now range from 0 to 1 (normalized through ranking)</li> <li>Different price levels no longer matter - we're comparing relative performance</li> <li>Features are smoothed (moving averages reduce noise)</li> <li>The data is now ready for machine learning models to find patterns</li> </ul> <p>This transformation is crucial for cross-sectional ranking models where we predict which assets will outperform others, not their absolute price movements.</p> In\u00a0[\u00a0]: Copied! <pre># Create final dataset structure\nfinal_df = df.rename({\"ticker\": \"eodhd_id\"}).select(\n    [\"id\", \"eodhd_id\", \"date\"] + list(feature_names) + [\"target_10d\", \"target_30d\"]\n)\n\nprint(f\"\ud83d\udcca Final Dataset Summary:\")\nprint(f\"Shape: {final_df.shape}\")\nprint(f\"Date range: {final_df['date'].min()} to {final_df['date'].max()}\")\nprint(f\"Cryptocurrencies: {final_df['id'].n_unique()}\")\nprint(f\"Features: {len(feature_names)}\")\nprint(f\"Targets: 2 (target_10d, target_30d)\")\n\n# Show sample with first few features\nsample_cols = [\"id\", \"date\"] + list(feature_names)[:3] + [\"target_10d\", \"target_30d\"]\nprint(f\"\\nSample data:\")\nfinal_df.select(sample_cols).head()\n</pre> # Create final dataset structure final_df = df.rename({\"ticker\": \"eodhd_id\"}).select(     [\"id\", \"eodhd_id\", \"date\"] + list(feature_names) + [\"target_10d\", \"target_30d\"] )  print(f\"\ud83d\udcca Final Dataset Summary:\") print(f\"Shape: {final_df.shape}\") print(f\"Date range: {final_df['date'].min()} to {final_df['date'].max()}\") print(f\"Cryptocurrencies: {final_df['id'].n_unique()}\") print(f\"Features: {len(feature_names)}\") print(f\"Targets: 2 (target_10d, target_30d)\")  # Show sample with first few features sample_cols = [\"id\", \"date\"] + list(feature_names)[:3] + [\"target_10d\", \"target_30d\"] print(f\"\\nSample data:\") final_df.select(sample_cols).head() <pre>\ud83d\udcca Final Dataset Summary:\nShape: (243925, 55)\nDate range: 2020-01-31 00:00:00 to 2026-01-04 00:00:00\nCryptocurrencies: 213\nFeatures: 50\nTargets: 2 (target_10d, target_30d)\n\nSample data:\n</pre> Out[\u00a0]: shape: (5, 7)iddateclose_logreturn_rank_ma2_lag20close_logreturn_rank_ma10_lag20open_logreturn_rank_ma2_lag20target_10dtarget_30dstrdatetime[\u03bcs]f64f64f64f64f64\"0G\"2025-10-22 00:00:000.450980.2956030.4436270.0098040.588235\"0G\"2025-10-23 00:00:000.524510.2965880.4508610.0735290.637255\"0G\"2025-10-24 00:00:000.5024390.2960910.524390.0588240.514706\"0G\"2025-10-25 00:00:000.0560980.3053440.5024390.024510.5\"0G\"2025-10-26 00:00:000.4629720.3857290.0563610.0343140.583333 In\u00a0[14]: Copied! <pre># Save to parquet\noutput_path = \"custom_crypto_dataset.parquet\"\nfinal_df.write_parquet(output_path)\n\n# Get file size, why not?\nfile_size_mb = os.path.getsize(output_path) / 1024 / 1024\n\nprint(f\"\u2705 Dataset saved to: {output_path}\")\nprint(f\"File size: {file_size_mb:.2f} MB\")\nprint(f\"Rows: {len(final_df):,}\")\nprint(f\"\\nYou can now use this dataset for ML experiments!\")\n</pre> # Save to parquet output_path = \"custom_crypto_dataset.parquet\" final_df.write_parquet(output_path)  # Get file size, why not? file_size_mb = os.path.getsize(output_path) / 1024 / 1024  print(f\"\u2705 Dataset saved to: {output_path}\") print(f\"File size: {file_size_mb:.2f} MB\") print(f\"Rows: {len(final_df):,}\") print(f\"\\nYou can now use this dataset for ML experiments!\") <pre>\u2705 Dataset saved to: custom_crypto_dataset.parquet\nFile size: 64.52 MB\nRows: 243,925\n\nYou can now use this dataset for ML experiments!\n</pre> In\u00a0[\u00a0]: Copied! <pre># Prepare features\nfeature_cols = list(feature_names)\nprint(f\"Feature columns: {len(feature_cols)} features\")\n\n# Time-based split with embargo period\nembargo_days = 30\nsorted_dates = final_df[\"date\"].unique().sort()\nsplit_idx = int(len(sorted_dates) * 0.8)\nsplit_date = sorted_dates[split_idx]\nembargo_end = split_date + pl.duration(days=embargo_days)\n\nprint(f\"Split date: {split_date}\")\nprint(f\"Embargo: {embargo_days} days\")\n\ntrain_df = final_df.filter(pl.col(\"date\") &lt; split_date)\ntest_df = final_df.filter(pl.col(\"date\") &gt; embargo_end)\n\nprint(\n    f\"Train period: {train_df['date'].min()} to {train_df['date'].max()} ({len(train_df)} rows)\"\n)\nprint(\n    f\"Test period: {test_df['date'].min()} to {test_df['date'].max()} ({len(test_df)} rows)\"\n)\n</pre> # Prepare features feature_cols = list(feature_names) print(f\"Feature columns: {len(feature_cols)} features\")  # Time-based split with embargo period embargo_days = 30 sorted_dates = final_df[\"date\"].unique().sort() split_idx = int(len(sorted_dates) * 0.8) split_date = sorted_dates[split_idx] embargo_end = split_date + pl.duration(days=embargo_days)  print(f\"Split date: {split_date}\") print(f\"Embargo: {embargo_days} days\")  train_df = final_df.filter(pl.col(\"date\") &lt; split_date) test_df = final_df.filter(pl.col(\"date\") &gt; embargo_end)  print(     f\"Train period: {train_df['date'].min()} to {train_df['date'].max()} ({len(train_df)} rows)\" ) print(     f\"Test period: {test_df['date'].min()} to {test_df['date'].max()} ({len(test_df)} rows)\" ) <pre>Feature columns: 50 features\nSplit date: 2024-10-28 00:00:00\nEmbargo: 30 days\nTrain period: 2020-01-31 00:00:00 to 2024-10-27 00:00:00 (163218 rows)\nTest period: 2024-11-28 00:00:00 to 2026-01-04 00:00:00 (75802 rows)\n</pre> In\u00a0[18]: Copied! <pre>from xgboost import XGBRegressor\nfrom crowdcent_challenge.scoring import evaluate_hyperliquid_submission\n\n# Train simple, untuned multi-output XGBoost model for both 10d and 30d targets\nmodel = XGBRegressor(n_estimators=500, random_state=42, verbosity=0, device=\"cuda\")\n\nX_train = train_df[feature_cols].to_pandas()\ny_train = train_df[[\"target_10d\", \"target_30d\"]].to_pandas()\n\nprint(f\"Training on {X_train.shape} features, {y_train.shape} targets\")\nprint(\n    \"This may take a few minutes depending on number of features, model parameters, GPU, etc...\"\n)\nmodel.fit(X_train, y_train)\n\n# Make predictions for both horizons\nX_test = test_df[feature_cols].to_pandas()\ntest_preds = model.predict(X_test)\n\n# Extract predictions for both horizons and create predictions dataframe\ntest_results = test_df.select([\"date\", \"id\", \"target_10d\", \"target_30d\"]).with_columns(\n    [pl.Series(\"pred_10d\", test_preds[:, 0]), pl.Series(\"pred_30d\", test_preds[:, 1])]\n)\nmetrics = [\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"]\n\n# Evaluate per date (cross-sectionally) then average\ndaily_scores = (\n    test_results.group_by(\"date\")\n    .agg(\n        [\n            pl.col(\"target_10d\"),\n            pl.col(\"pred_10d\"),\n            pl.col(\"target_30d\"),\n            pl.col(\"pred_30d\"),\n        ]\n    )\n    .with_columns(\n        pl.struct([\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"])\n        .map_elements(\n            lambda x: evaluate_hyperliquid_submission(\n                y_true_10d=x[\"target_10d\"],\n                y_pred_10d=x[\"pred_10d\"],\n                y_true_30d=x[\"target_30d\"],\n                y_pred_30d=x[\"pred_30d\"],\n            ),\n            return_dtype=pl.Struct(\n                {\n                    metric: pl.Float64\n                    for metric in metrics\n                }\n            ),\n        )\n        .alias(\"metrics\")\n    )\n    .unnest(\"metrics\")\n    .sort(\"date\")\n)\n\n\n# Average scores across all test dates\navg_scores = daily_scores.select(\n    [pl.col(metric).mean() for metric in metrics]\n).to_dicts()[0]\n\nprint(f\"\\n\ud83d\udcc8 Model Validation Results (CrowdCent Official Metrics):\")\nprint(f\"Spearman Correlation 10d: {avg_scores['spearman_10d']:.4f}\")\nprint(f\"Spearman Correlation 30d: {avg_scores['spearman_30d']:.4f}\")\nprint(f\"NDCG@40 10d: {avg_scores['ndcg@40_10d']:.4f}\")\nprint(f\"NDCG@40 30d: {avg_scores['ndcg@40_30d']:.4f}\")\n\nprint(f\"\\n\ud83c\udfaf Your custom dataset is ready for advanced modeling!\")\n</pre> from xgboost import XGBRegressor from crowdcent_challenge.scoring import evaluate_hyperliquid_submission  # Train simple, untuned multi-output XGBoost model for both 10d and 30d targets model = XGBRegressor(n_estimators=500, random_state=42, verbosity=0, device=\"cuda\")  X_train = train_df[feature_cols].to_pandas() y_train = train_df[[\"target_10d\", \"target_30d\"]].to_pandas()  print(f\"Training on {X_train.shape} features, {y_train.shape} targets\") print(     \"This may take a few minutes depending on number of features, model parameters, GPU, etc...\" ) model.fit(X_train, y_train)  # Make predictions for both horizons X_test = test_df[feature_cols].to_pandas() test_preds = model.predict(X_test)  # Extract predictions for both horizons and create predictions dataframe test_results = test_df.select([\"date\", \"id\", \"target_10d\", \"target_30d\"]).with_columns(     [pl.Series(\"pred_10d\", test_preds[:, 0]), pl.Series(\"pred_30d\", test_preds[:, 1])] ) metrics = [\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"]  # Evaluate per date (cross-sectionally) then average daily_scores = (     test_results.group_by(\"date\")     .agg(         [             pl.col(\"target_10d\"),             pl.col(\"pred_10d\"),             pl.col(\"target_30d\"),             pl.col(\"pred_30d\"),         ]     )     .with_columns(         pl.struct([\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"])         .map_elements(             lambda x: evaluate_hyperliquid_submission(                 y_true_10d=x[\"target_10d\"],                 y_pred_10d=x[\"pred_10d\"],                 y_true_30d=x[\"target_30d\"],                 y_pred_30d=x[\"pred_30d\"],             ),             return_dtype=pl.Struct(                 {                     metric: pl.Float64                     for metric in metrics                 }             ),         )         .alias(\"metrics\")     )     .unnest(\"metrics\")     .sort(\"date\") )   # Average scores across all test dates avg_scores = daily_scores.select(     [pl.col(metric).mean() for metric in metrics] ).to_dicts()[0]  print(f\"\\n\ud83d\udcc8 Model Validation Results (CrowdCent Official Metrics):\") print(f\"Spearman Correlation 10d: {avg_scores['spearman_10d']:.4f}\") print(f\"Spearman Correlation 30d: {avg_scores['spearman_30d']:.4f}\") print(f\"NDCG@40 10d: {avg_scores['ndcg@40_10d']:.4f}\") print(f\"NDCG@40 30d: {avg_scores['ndcg@40_30d']:.4f}\")  print(f\"\\n\ud83c\udfaf Your custom dataset is ready for advanced modeling!\") <pre>Training on (163218, 50) features, (163218, 2) targets\nThis may take a few minutes depending on number of features, model parameters, GPU, etc...\n\n\ud83d\udcc8 Model Validation Results (CrowdCent Official Metrics):\nSpearman Correlation 10d: 0.0341\nSpearman Correlation 30d: 0.0476\nNDCG@40 10d: 0.5649\nNDCG@40 30d: 0.5699\n\n\ud83c\udfaf Your custom dataset is ready for advanced modeling!\n</pre>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#create-a-custom-training-dataset-with-eodhd","title":"Create a custom training dataset with EODHD\u00b6","text":"<p>Learn how to create custom crypto training datasets (features and targets) from scratch. This tutorial demonstrates the full pipeline from data download to ML-ready features and modeling.</p> <p>What you'll learn:</p> <ul> <li>Download historical crypto data with numerblox and eodhd</li> <li>Engineer cross-sectional features with centimators</li> <li>Create ranking targets for prediction</li> <li>Validate data quality</li> <li>Structure datasets for ML</li> </ul> <p>Why build custom datasets? While CrowdCent provides training data, building your own allows you to:</p> <ul> <li>Experiment with different features and time windows</li> <li>Include additional data sources</li> <li>Test hypotheses on historical data</li> <li>Develop a unique edge in predictions</li> </ul>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#get-eodhd-api-access","title":"Get EODHD API Access\u00b6","text":"<p>Special offer for CrowdCent users: Get 10% off all EODHD plans \ud83d\udc49 https://eodhd.com/pricing-special-10?via=crowdcent</p> <p>Recommended plan: EOD Historical Data - All World ($17.99/mo) which provides 100k calls/day - sufficient for most use cases</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-1-fetch-live-hyperliquid-universe","title":"Step 1: Fetch Live Hyperliquid Universe\u00b6","text":"<p>We'll get the current cryptocurrency universe directly from the Hyperliquid API. This includes both active and delisted perpetuals, giving us maximum historical data coverage.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-2-download-historical-data-with-numerblox","title":"Step 2: Download Historical Data with numerblox\u00b6","text":""},{"location":"tutorials/advanced-custom-dataset-eodhd/#visualize-raw-price-data","title":"Visualize Raw Price Data\u00b6","text":"<p>Let's look at what the raw downloaded data looks like - this is the \"Input\" to our feature engineering pipeline.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-3-create-ranking-targets-with-crowdcent-library","title":"Step 3: Create Ranking Targets with CrowdCent Library\u00b6","text":"<p>We'll use the <code>create_ranking_targets</code> function from crowdcent-challenge to create standard 10-day and 30-day ranking targets. This matches exactly what CrowdCent uses in the hyperliquid-ranking challenge.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#visualize-target-distributions","title":"Visualize Target Distributions\u00b6","text":"<p>The ranking targets are normalized to [0, 1] where 0 means worst performer and 1 means best performer on that day. This cross-sectional ranking approach means our model predicts relative performance, not absolute returns.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-4-engineer-features-with-centimators","title":"Step 4: Engineer features with centimators\u00b6","text":"<p>This is the step where you can and should put most of your energy. Try different feature engineering strategies, different moving average windows, lag windows, and anything else you can think of.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#visualize-pipeline-output-engineered-features","title":"Visualize Pipeline Output: Engineered Features\u00b6","text":"<p>After feature engineering, all features are normalized and cross-sectionally ranked. This makes them comparable across different tickers and time periods. Let's visualize a few features for our top tickers.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-5-structure-dataset-for-ml","title":"Step 5: Structure Dataset for ML\u00b6","text":""},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-6-save-dataset","title":"Step 6: Save Dataset\u00b6","text":""},{"location":"tutorials/advanced-custom-dataset-eodhd/#step-7-quick-model-validation","title":"Step 7: Quick Model Validation\u00b6","text":"<p>Let's verify our dataset works with a simple train-test split and xgboost model to ensure data quality.</p>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#next-steps","title":"Next Steps\u00b6","text":"<p>\ud83c\udf89 Congratulations! You've built a complete crypto prediction dataset. Here's what to try next, although the list is endless:</p> <p>Ready to submit predictions with your new dataset? Check out the Hyperliquid End-to-End Tutorial where you'll learn how to download inference data, make predictions, and submit to CrowdCent competitions using your custom model. You will need to replace the download data steps with your own production pipeline using your new custom dataset!</p> <p>Immediate experiments and next steps:</p> <ol> <li>Try different models: Random Forest, Neural Networks, and other model estimators from Centimators like LSTMRegressor</li> <li>Adjust time horizons: Create 3d, 14d, 50d targets</li> <li>Additional data cleaning: Check the raw data for anomolies and remove/adjust samples.</li> <li>Feature engineering: Add lags, differences, and other combinatorial feature combinations. Centimators provides additional <code>feature transformers</code> like GroupStatsTransformer and will always be adding more!</li> <li>Cross-validation: Use walk-forward, time-series CV for robust validation like in this advanced tutorial notebook.</li> <li>More data sources: Add social sentiment, onchain data, or macro indicators</li> <li>Ensemble methods: Combine multiple models and time horizons</li> <li>Real-time pipeline: Set up automated data updates and retraining</li> <li>Automate submissions: Use techniques from the submission automation guide to set it and forget it.</li> </ol>"},{"location":"tutorials/advanced-custom-dataset-eodhd/#other-resources","title":"Other Resources\u00b6","text":"<ul> <li>EODHD API: https://eodhd.com/pricing-special-10?via=crowdcent</li> <li>Centimators GitHub: https://github.com/crowdcent/centimators</li> <li>CrowdCent Docs: https://docs.crowdcent.com</li> <li>Join our Discord: Join here to get help and share your results with the community</li> </ul> <p>Happy modeling! \ud83e\uddea</p>"},{"location":"tutorials/advanced-cv-lstm/","title":"XGBoost vs LSTM with CV","text":"<p>Learn how to leverage the temporal structure in CrowdCent's training data to dramatically improve prediction performance using an LSTM model compared to a traditional XGBoost approach. We'll use the <code>sklego.model_selection.TimeGapSplit</code> to set up proper time-series cross-validation with gap periods to prevent leakage and then visualize the performance on various metrics over time with moving averages.</p> In\u00a0[1]: Copied! <pre>import crowdcent_challenge as cc\nimport polars as pl\nfrom xgboost import XGBRegressor\nfrom datetime import timedelta\nfrom sklego.model_selection import TimeGapSplit\nimport altair as alt\nimport numpy as np\nimport os\nfrom crowdcent_challenge.scoring import evaluate_hyperliquid_submission\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nfrom centimators.model_estimators import LSTMRegressor\n</pre> import crowdcent_challenge as cc import polars as pl from xgboost import XGBRegressor from datetime import timedelta from sklego.model_selection import TimeGapSplit import altair as alt import numpy as np import os from crowdcent_challenge.scoring import evaluate_hyperliquid_submission  os.environ[\"KERAS_BACKEND\"] = \"jax\"  from centimators.model_estimators import LSTMRegressor In\u00a0[2]: Copied! <pre>client = cc.ChallengeClient(\n    challenge_slug=\"hyperliquid-ranking\",\n)\n</pre> client = cc.ChallengeClient(     challenge_slug=\"hyperliquid-ranking\", ) <pre>2026-02-03 21:00:42,153 - INFO - ChallengeClient initialized for 'hyperliquid-ranking' at URL: https://crowdcent.com/api\n</pre> In\u00a0[\u00a0]: Copied! <pre>client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")\n\ndata = pl.read_parquet(\"training_data.parquet\")\ndata.head()\n</pre> client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")  data = pl.read_parquet(\"training_data.parquet\") data.head() <pre>2026-02-03 21:00:42,162 - INFO - Downloading training data v2.0 to training_data.parquet\nDownloading training_data.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 124M/124M [00:02&lt;00:00, 59.4MB/s] \n2026-02-03 21:00:44,852 - INFO - Successfully downloaded training data v2.0 to training_data.parquet\n</pre> Out[\u00a0]: shape: (5, 85)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_5_lag5feature_6_lag5feature_7_lag5feature_12_lag5feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0target_10dtarget_30dstrstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\"0G\"\"0G-USD.CC\"2025-11-16 00:00:000.1576920.1563360.2397620.3133490.0519230.1351220.2697350.3003530.1894230.2000070.2277810.3365930.1615380.1452040.2380610.2911210.6057690.5878160.5668550.5161270.2644230.2110580.2416170.2967680.3298080.1908650.2565350.319130.4076920.2985580.2770470.3219810.2576920.209615\u20260.3533490.2442350.339430.5490110.4783520.3391790.3445930.4961170.3769050.2610540.3119140.2701460.3244960.4561560.4752740.5224880.50930.3601790.3282060.6114830.4941870.3425260.3510890.5368420.5429260.4207420.3313810.5244020.5102590.3599370.3226470.5885170.4293320.460820.4946380.8325360.186603\"0G\"\"0G-USD.CC\"2025-11-17 00:00:000.1605770.1451920.2382970.2911240.0355770.1341350.2571640.2809250.0211540.2004810.2243950.3120740.0336540.1461540.2247680.2672060.6096150.6033650.6031320.5120470.2576920.2091350.2365360.2776810.3326920.1841350.251690.3177940.4173080.2192310.2717170.3028910.3346150.184135\u20260.4207930.2774640.3326120.6398560.5285820.3645310.3339540.5942810.4644480.3053010.3096410.4317310.3384620.4709130.4747990.5244020.510720.3599270.3226390.4708130.4898530.3369940.3481870.4669860.5534210.3863260.3338760.412440.5033610.3437480.3289640.4009570.4163440.4218740.4714460.7799040.167464\"0G\"\"0G-USD.CC\"2025-11-18 00:00:000.0326920.1456730.2250030.2672080.2250.2288460.2805940.3055610.2153850.2817310.2640130.3136560.2259620.2283650.2684890.2918430.7692310.6394230.6164180.5366840.3346150.1836540.254310.2839110.3326920.2788460.2975830.3413450.3923080.3038460.3137810.3241640.3355770.280769\u20260.3394530.284150.3208110.628510.5104090.396070.3300110.5014580.4185180.3234420.3229180.2422620.2437270.4415750.4747930.412440.5038210.3437370.3290760.4842110.4152120.3470290.3483680.3550240.4917670.3978070.3381660.4535890.4775230.3791460.3438820.4765550.3594080.433310.481850.8851670.22488\"0G\"\"0G-USD.CC\"2025-11-19 00:00:000.2250.2278850.2687240.2918450.3278850.2706730.3024420.3190210.4096150.3774040.308220.3380530.2615380.2177880.2706170.2961660.6076920.5846150.5883950.5359520.3355770.2802880.299720.308670.3721150.350.2903990.3597330.3942310.4019230.3558110.3478370.4951920.378365\u20260.3167030.2936880.3285850.4897590.4419950.4096990.3358650.3359220.4155570.3166730.3241010.2287220.2504190.4175170.4717780.4535890.4779840.3791360.3439940.5674640.4143770.3821890.3474650.365550.4276550.4147890.325520.4401910.3880570.3832110.3250190.5521530.3904380.4151710.4858460.8708130.301435\"0G\"\"0G-USD.CC\"2025-11-20 00:00:000.2605770.2173080.2708530.2961680.3298080.2519230.2891520.3185310.4144230.3004810.2833310.3385290.2653850.2115380.2413810.2967640.4451920.5788460.5644720.5133690.4951920.3778850.3014140.3324640.3778850.3538460.2430440.3388340.5461540.4802880.3466810.34460.4951920.380288\u20260.4023390.3271310.3281320.3617220.4539380.3772090.319160.5224880.508840.3601890.3280940.4191390.3427420.4607940.473960.4401910.3885170.3832010.3251310.4028710.4148330.3843390.3278870.424880.3933010.4367950.3324150.3081340.4153110.39780.3185430.4277510.4234450.3896070.4902310.7990430.124402 In\u00a0[4]: Copied! <pre>cv_kwargs = {\n    \"val_days\": 100,\n    \"gap_days\": 30,\n    \"n_splits\": 3,\n    \"cv_window_type\": \"rolling\",\n}\n\ncv = TimeGapSplit(\n    date_serie=data[\"date\"],\n    valid_duration=timedelta(days=cv_kwargs[\"val_days\"]),\n    gap_duration=timedelta(days=cv_kwargs[\"gap_days\"]),\n    n_splits=cv_kwargs[\"n_splits\"],\n    window=cv_kwargs[\"cv_window_type\"],\n)\n\ncv.summary(data)\n</pre> cv_kwargs = {     \"val_days\": 100,     \"gap_days\": 30,     \"n_splits\": 3,     \"cv_window_type\": \"rolling\", }  cv = TimeGapSplit(     date_serie=data[\"date\"],     valid_duration=timedelta(days=cv_kwargs[\"val_days\"]),     gap_duration=timedelta(days=cv_kwargs[\"gap_days\"]),     n_splits=cv_kwargs[\"n_splits\"],     window=cv_kwargs[\"cv_window_type\"], )  cv.summary(data) Out[4]: shape: (6, 7)Start dateEnd datePeriodUnique daysnbr samplespartfolddatetime[\u03bcs]datetime[\u03bcs]duration[\u03bcs]i64i64stri642019-07-26 00:00:002025-02-04 00:00:002020d2021182226\"train\"02025-03-07 00:00:002025-06-14 00:00:0099d10018348\"valid\"02019-11-03 00:00:002025-05-15 00:00:002020d2021196281\"train\"12025-06-15 00:00:002025-09-22 00:00:0099d10019601\"valid\"12020-02-11 00:00:002025-08-23 00:00:002020d2021211223\"train\"22025-09-23 00:00:002025-12-31 00:00:0099d10020309\"valid\"2 In\u00a0[5]: Copied! <pre>feature_cols = [\n    col\n    for col in data.columns\n    if col not in [\"id\", \"eodhd_id\", \"date\", \"target_10d\", \"target_30d\"]\n]\ntarget_cols = [\"target_10d\", \"target_30d\"]\nlag_windows = [0, 5, 10, 15]\nn_features_per_timestep = len(feature_cols) // len(lag_windows)\n</pre> feature_cols = [     col     for col in data.columns     if col not in [\"id\", \"eodhd_id\", \"date\", \"target_10d\", \"target_30d\"] ] target_cols = [\"target_10d\", \"target_30d\"] lag_windows = [0, 5, 10, 15] n_features_per_timestep = len(feature_cols) // len(lag_windows) In\u00a0[6]: Copied! <pre>print(\"Training and evaluating XGBoost model with detailed scoring...\")\n\nfold_data = []\nmodels = [\n    XGBRegressor(n_estimators=2000, device=\"cuda\"),\n    LSTMRegressor(\n        output_units=2,\n        lag_windows=lag_windows,\n        n_features_per_timestep=n_features_per_timestep,\n    ),\n]\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(data)):\n    print(f\"\\nFold {fold + 1}/{cv.n_splits}\")\n\n    # Get train and validation data\n    train_data = data[train_idx]\n    val_data = data[val_idx]\n\n    print(f\"  Train dates: {train_data['date'].min()} to {train_data['date'].max()}\")\n    print(f\"  Val dates: {val_data['date'].min()} to {val_data['date'].max()}\")\n    print(f\"  Train samples: {len(train_data)}, Val samples: {len(val_data)}\")\n\n    # Train model\n    for model in models:\n        fit_kwargs = {}\n        if isinstance(model, LSTMRegressor):\n            fit_kwargs[\"epochs\"] = 5\n            fit_kwargs[\"validation_data\"] = (\n                val_data[feature_cols],\n                val_data[target_cols],\n            )\n        model.fit(\n            train_data[feature_cols],\n            train_data[target_cols],\n            **fit_kwargs,\n        )\n\n        # Make predictions\n        preds = pl.from_numpy(\n            np.asarray(model.predict(val_data[feature_cols])),\n            schema={\"pred_10d\": pl.Float64, \"pred_30d\": pl.Float64},\n        )\n        preds = preds.with_columns(\n            pl.lit(fold).alias(\"fold\"), pl.lit(model.__class__.__name__).alias(\"model\")\n        )\n\n        fold_data.append(val_data.with_columns(preds))\n</pre> print(\"Training and evaluating XGBoost model with detailed scoring...\")  fold_data = [] models = [     XGBRegressor(n_estimators=2000, device=\"cuda\"),     LSTMRegressor(         output_units=2,         lag_windows=lag_windows,         n_features_per_timestep=n_features_per_timestep,     ), ]  for fold, (train_idx, val_idx) in enumerate(cv.split(data)):     print(f\"\\nFold {fold + 1}/{cv.n_splits}\")      # Get train and validation data     train_data = data[train_idx]     val_data = data[val_idx]      print(f\"  Train dates: {train_data['date'].min()} to {train_data['date'].max()}\")     print(f\"  Val dates: {val_data['date'].min()} to {val_data['date'].max()}\")     print(f\"  Train samples: {len(train_data)}, Val samples: {len(val_data)}\")      # Train model     for model in models:         fit_kwargs = {}         if isinstance(model, LSTMRegressor):             fit_kwargs[\"epochs\"] = 5             fit_kwargs[\"validation_data\"] = (                 val_data[feature_cols],                 val_data[target_cols],             )         model.fit(             train_data[feature_cols],             train_data[target_cols],             **fit_kwargs,         )          # Make predictions         preds = pl.from_numpy(             np.asarray(model.predict(val_data[feature_cols])),             schema={\"pred_10d\": pl.Float64, \"pred_30d\": pl.Float64},         )         preds = preds.with_columns(             pl.lit(fold).alias(\"fold\"), pl.lit(model.__class__.__name__).alias(\"model\")         )          fold_data.append(val_data.with_columns(preds)) <pre>Training and evaluating XGBoost model with detailed scoring...\n\nFold 1/3\n  Train dates: 2019-07-26 00:00:00 to 2025-02-04 00:00:00\n  Val dates: 2025-03-07 00:00:00 to 2025-06-14 00:00:00\n  Train samples: 182226, Val samples: 18348\n</pre> <pre>/home/exx/projects/crowdcent-challenge/.venv/lib/python3.12/site-packages/xgboost/core.py:774: UserWarning: [21:01:01] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  return func(**kwargs)\nINFO:2026-02-03 21:01:01,218:jax._src.xla_bridge:834: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n2026-02-03 21:01:01,218 - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\nWARNING:2026-02-03 21:01:01,219:jax._src.xla_bridge:876: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n2026-02-03 21:01:01,219 - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</pre> <pre>Epoch 1/5\n5695/5695 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 2ms/step - loss: 0.9988 - mse: 0.9988 - val_loss: 1.0020 - val_mse: 1.0020\nEpoch 2/5\n5695/5695 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 2ms/step - loss: 0.9965 - mse: 0.9965 - val_loss: 0.9976 - val_mse: 0.9976\nEpoch 3/5\n5695/5695 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 2ms/step - loss: 0.9958 - mse: 0.9958 - val_loss: 0.9988 - val_mse: 0.9988\nEpoch 4/5\n5695/5695 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 2ms/step - loss: 0.9945 - mse: 0.9945 - val_loss: 0.9959 - val_mse: 0.9959\nEpoch 5/5\n5695/5695 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 2ms/step - loss: 0.9933 - mse: 0.9933 - val_loss: 1.0026 - val_mse: 1.0026\n36/36 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 15ms/step\n\nFold 2/3\n  Train dates: 2019-11-03 00:00:00 to 2025-05-15 00:00:00\n  Val dates: 2025-06-15 00:00:00 to 2025-09-22 00:00:00\n  Train samples: 196281, Val samples: 19601\nEpoch 1/5\n6134/6134 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 2ms/step - loss: 0.9928 - mse: 0.9928 - val_loss: 1.0110 - val_mse: 1.0110\nEpoch 2/5\n6134/6134 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 2ms/step - loss: 0.9923 - mse: 0.9923 - val_loss: 1.0119 - val_mse: 1.0119\nEpoch 3/5\n6134/6134 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 2ms/step - loss: 0.9919 - mse: 0.9919 - val_loss: 1.0109 - val_mse: 1.0109\nEpoch 4/5\n6134/6134 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 2ms/step - loss: 0.9917 - mse: 0.9917 - val_loss: 1.0095 - val_mse: 1.0095\nEpoch 5/5\n6134/6134 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 2ms/step - loss: 0.9915 - mse: 0.9915 - val_loss: 1.0165 - val_mse: 1.0165\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step\n\nFold 3/3\n  Train dates: 2020-02-11 00:00:00 to 2025-08-23 00:00:00\n  Val dates: 2025-09-23 00:00:00 to 2025-12-31 00:00:00\n  Train samples: 211223, Val samples: 20309\nEpoch 1/5\n6601/6601 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 2ms/step - loss: 0.9900 - mse: 0.9900 - val_loss: 0.9999 - val_mse: 0.9999\nEpoch 2/5\n6601/6601 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 2ms/step - loss: 0.9901 - mse: 0.9901 - val_loss: 0.9919 - val_mse: 0.9919\nEpoch 3/5\n6601/6601 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 2ms/step - loss: 0.9897 - mse: 0.9897 - val_loss: 0.9919 - val_mse: 0.9919\nEpoch 4/5\n6601/6601 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 2ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 0.9932 - val_mse: 0.9932\nEpoch 5/5\n6601/6601 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 2ms/step - loss: 0.9890 - mse: 0.9890 - val_loss: 0.9922 - val_mse: 0.9922\n40/40 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step\n</pre> In\u00a0[7]: Copied! <pre>combined_preds = pl.concat(fold_data)\ncombined_preds.head()\n</pre> combined_preds = pl.concat(fold_data) combined_preds.head() Out[7]: shape: (5, 89)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0target_10dtarget_30dpred_10dpred_30dfoldmodelstrstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64i32str\"AAVE\"\"AAVE-USD.CC\"2025-03-07 00:00:000.6010810.5595950.5458750.5689610.5545950.4807270.4863280.5527620.6464860.584050.5572590.5861230.60.5290470.5196030.5584230.4843240.4385820.456560.5244450.3788950.4899880.4635270.5301640.3866160.4706050.4462410.5176140.2509750.4487310.4490080.5359980.3075150.453757\u20260.3292920.4566710.484530.5054350.4064750.4677610.476680.5086960.5371330.4878570.4923030.5945650.4777170.4838530.5036550.5934780.4777170.4741610.4878330.6163040.5119570.4803440.5062140.4782610.4918480.4728030.4970130.5597830.5342390.5295930.516420.1684780.2771740.6102430.7624820\"XGBRegressor\"\"ACE\"\"ACE28674-USD.CC\"2025-03-07 00:00:000.5805410.5343380.4758950.4920220.6410810.6016250.5104320.5175010.6248650.5947690.4756770.4764540.710270.5745410.4885010.5038810.6281080.5275760.5286720.5475970.6153470.5979440.5540410.4991660.5774790.609280.5757960.5118610.6120680.6184670.561290.4860130.5872560.648763\u20260.5277730.5612710.494520.4782610.5327590.553650.5093660.4152170.5419360.5347560.5338220.3597830.426630.5122870.4830070.3619570.4288040.5190420.4861010.3217390.3826090.5005380.4631030.3543480.4163040.5325340.4781660.5978260.5065220.5774520.538490.5163040.1847830.4235550.5214630\"XGBRegressor\"\"ADA\"\"ADA-USD.CC\"2025-03-07 00:00:000.6005410.5911740.5324860.5686920.6167570.6146330.5625530.570770.5886490.6156230.5594190.5777560.570270.5726630.547220.5567760.4556760.429340.4782190.4801990.4630490.5317950.519380.5403440.4677560.5422560.5216190.5428690.3991360.4938920.5096170.5446680.4122390.491254\u20260.4232090.5194160.5206040.5472830.4797610.5262120.5320370.6081520.579640.504490.4927270.5342390.4646740.4982340.5105660.4538040.4798910.5110740.5368070.53750.4923910.4931420.5188640.3440220.4456520.4684530.4965010.4092390.5086960.5060490.4977050.1820650.4592390.3138850.4513940\"XGBRegressor\"\"AERO\"\"AERO29270-USD.CC\"2025-03-07 00:00:000.3535140.4250390.4386030.4659370.4951350.4881020.4612370.4798410.2864860.3799970.4319030.488740.3470270.4443240.42450.4803060.609730.5643570.4713450.4840010.5873090.4704110.4564250.4748030.4338480.4644920.4348930.4521510.5036550.3950710.394280.460030.4689190.407973\u20260.4447620.412380.4438830.4771740.4730460.4586850.4741020.4195650.5288780.5466170.5108110.5619570.439130.4547710.4772290.4782610.4467390.4556150.4859940.6684780.5271740.4611220.4914510.4793480.4782610.4431170.4846110.5163040.4679350.5459470.5092820.114130.0271740.4232250.5182080\"XGBRegressor\"\"AI16Z\"\"AI16Z-USD.CC\"2025-03-07 00:00:000.0378380.3168930.4013560.3901410.2118920.2462180.3297480.3339690.2356760.392090.4784270.4259930.2227030.3184080.4452350.4140230.5210810.4560690.5500690.4924830.49460.2662190.4294260.3790840.4924560.3521740.3959560.3443960.5239780.3798270.5226810.4231280.463960.343331\u20260.5935110.49280.4740.6184780.5412190.4298140.4142980.5934780.4873210.4716950.5133240.2217390.4076090.3369140.3971680.060870.4282610.3902170.3560540.1184780.3907610.3852940.4518570.039130.3288040.3360680.3958420.5152170.5543480.5027350.5268780.1195650.0434780.1160330.5319050\"XGBRegressor\" In\u00a0[8]: Copied! <pre>def calculate_daily_scores(predictions_df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Calculate daily scores for predictions using hyperliquid evaluation metrics.\n\n    Args:\n        predictions_df: DataFrame containing predictions and targets\n\n    Returns:\n        DataFrame with daily evaluation metrics\n    \"\"\"\n    columns = [\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"]\n\n    # Define the struct schema for the scoring output\n    score_schema = pl.Struct(\n        {\n            \"spearman_10d\": pl.Float64,\n            \"spearman_30d\": pl.Float64,\n            \"ndcg@40_10d\": pl.Float64,\n            \"ndcg@40_30d\": pl.Float64,\n        }\n    )\n\n    return (\n        predictions_df.group_by([\"date\", \"model\"])\n        .agg([pl.col(col) for col in columns])\n        .with_columns(\n            pl.struct(columns)\n            .map_elements(\n                lambda x: evaluate_hyperliquid_submission(\n                    y_true_10d=x[\"target_10d\"],\n                    y_pred_10d=x[\"pred_10d\"],\n                    y_true_30d=x[\"target_30d\"],\n                    y_pred_30d=x[\"pred_30d\"],\n                ),\n                return_dtype=score_schema,\n            )\n            .alias(\"daily_scores\")\n        )\n    ).unnest(\"daily_scores\")\n\n\ndaily_scores = calculate_daily_scores(combined_preds)\ndaily_scores\n</pre> def calculate_daily_scores(predictions_df: pl.DataFrame) -&gt; pl.DataFrame:     \"\"\"Calculate daily scores for predictions using hyperliquid evaluation metrics.      Args:         predictions_df: DataFrame containing predictions and targets      Returns:         DataFrame with daily evaluation metrics     \"\"\"     columns = [\"pred_10d\", \"target_10d\", \"pred_30d\", \"target_30d\"]      # Define the struct schema for the scoring output     score_schema = pl.Struct(         {             \"spearman_10d\": pl.Float64,             \"spearman_30d\": pl.Float64,             \"ndcg@40_10d\": pl.Float64,             \"ndcg@40_30d\": pl.Float64,         }     )      return (         predictions_df.group_by([\"date\", \"model\"])         .agg([pl.col(col) for col in columns])         .with_columns(             pl.struct(columns)             .map_elements(                 lambda x: evaluate_hyperliquid_submission(                     y_true_10d=x[\"target_10d\"],                     y_pred_10d=x[\"pred_10d\"],                     y_true_30d=x[\"target_30d\"],                     y_pred_30d=x[\"pred_30d\"],                 ),                 return_dtype=score_schema,             )             .alias(\"daily_scores\")         )     ).unnest(\"daily_scores\")   daily_scores = calculate_daily_scores(combined_preds) daily_scores Out[8]: shape: (600, 10)datemodelpred_10dtarget_10dpred_30dtarget_30dspearman_10dspearman_30dndcg@40_10dndcg@40_30ddatetime[\u03bcs]strlist[f64]list[f64]list[f64]list[f64]f64f64f64f642025-04-23 00:00:00\"LSTMRegressor\"[0.504178, 0.454119, \u2026 0.517369][0.838542, 0.46875, \u2026 0.161458][0.502318, 0.46069, \u2026 0.510441][0.890625, 0.442708, \u2026 0.135417]0.0853820.0055180.6369390.5865282025-05-10 00:00:00\"XGBRegressor\"[0.482643, 0.389933, \u2026 0.60705][0.923077, 0.420513, \u2026 0.323077][0.436532, 0.5263, \u2026 0.600579][0.969231, 0.410256, \u2026 0.210256]0.023454-0.0146250.5485430.5165242025-12-18 00:00:00\"LSTMRegressor\"[0.40894, 0.479743, \u2026 0.546537][0.976303, 0.829384, \u2026 0.085308][0.41065, 0.471246, \u2026 0.529192][0.668246, 0.791469, \u2026 0.890995]-0.130748-0.0979710.4794730.4760822025-12-28 00:00:00\"LSTMRegressor\"[0.434099, 0.512211, \u2026 0.499404][0.028436, 0.123223, \u2026 0.739336][0.426927, 0.510298, \u2026 0.475056][0.075829, 0.781991, \u2026 0.976303]-0.0426750.0497570.4992360.528082025-04-22 00:00:00\"LSTMRegressor\"[0.501167, 0.455346, \u2026 0.515472][0.82199, 0.361257, \u2026 0.13089][0.49875, 0.453371, \u2026 0.510702][0.879581, 0.39267, \u2026 0.151832]0.115555-0.0178720.6182310.576474\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u20262025-03-09 00:00:00\"LSTMRegressor\"[0.480994, 0.50544, \u2026 0.516951][0.091892, 0.481081, \u2026 0.994595][0.47823, 0.494458, \u2026 0.504098][0.172973, 0.189189, \u2026 0.989189]-0.0159140.149820.517750.6006342025-07-13 00:00:00\"LSTMRegressor\"[0.519002, 0.471842, \u2026 0.487213][0.10101, 0.328283, \u2026 0.070707][0.510259, 0.476054, \u2026 0.485446][0.393939, 0.494949, \u2026 0.515152]0.1642650.1775850.6463220.6063262025-10-09 00:00:00\"LSTMRegressor\"[0.539149, 0.470603, \u2026 0.528085][0.362319, 0.31401, \u2026 0.164251][0.535295, 0.454981, \u2026 0.512345][0.429952, 0.487923, \u2026 0.458937]-0.0115840.2020590.5737760.6217842025-03-28 00:00:00\"XGBRegressor\"[0.539257, 0.514973, \u2026 0.729769][0.258065, 0.188172, \u2026 0.83871][0.46656, 0.577899, \u2026 0.658475][0.198925, 0.284946, \u2026 0.645161]0.154568-0.0148390.6210860.5092212025-03-15 00:00:00\"LSTMRegressor\"[0.485251, 0.486415, \u2026 0.486219][0.372973, 0.535135, \u2026 0.945946][0.471388, 0.485451, \u2026 0.473527][0.405405, 0.172973, \u2026 0.859459]-0.2459330.191420.4132130.649429 In\u00a0[9]: Copied! <pre>from centimators.feature_transformers import MovingAverageTransformer\n\ndaily_scores = daily_scores.sort(\"date\")\nma_transformer = MovingAverageTransformer(\n    feature_names=[\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"],\n    windows=[7, 30],\n)\nma_columns = ma_transformer.fit_transform(\n    daily_scores, ticker_series=daily_scores[\"model\"]\n)\ndaily_scores_df = daily_scores.with_columns(ma_columns)\n</pre> from centimators.feature_transformers import MovingAverageTransformer  daily_scores = daily_scores.sort(\"date\") ma_transformer = MovingAverageTransformer(     feature_names=[\"spearman_10d\", \"spearman_30d\", \"ndcg@40_10d\", \"ndcg@40_30d\"],     windows=[7, 30], ) ma_columns = ma_transformer.fit_transform(     daily_scores, ticker_series=daily_scores[\"model\"] ) daily_scores_df = daily_scores.with_columns(ma_columns) In\u00a0[10]: Copied! <pre>def plot_metric_comparison(df, metric_name, anchor_ref=0, width=400, height=200):\n    \"\"\"Plot a single metric across 10d and 30d timeframes side by side with shared y-axis.\n\n    Args:\n        df: Polars DataFrame with daily scores\n        metric_name: Base metric name ('spearman' or 'ndcg@40')\n        width: Chart width in pixels per timeframe\n        height: Chart height\n    \"\"\"\n    # Convert to pandas once\n    pdf = df.to_pandas()\n\n    # Shared selection for zooming\n    brush = alt.selection_interval(bind=\"scales\", encodings=[\"x\"])\n\n    # Define timeframes and colors\n    timeframes = [\"10d\", \"30d\"]\n\n    # Get column names for this metric\n    col_10d = f\"{metric_name}_10d\"\n    col_30d = f\"{metric_name}_30d\"\n\n    # Calculate shared y-axis domain\n    min_val = min(df[col_10d].min(), df[col_30d].min())\n    max_val = max(df[col_10d].max(), df[col_30d].max())\n    y_domain = [min_val * 0.95, max_val * 1.05]\n\n    charts = []\n    for timeframe in timeframes:\n        col_name = f\"{metric_name}_{timeframe}\"\n\n        # Calculate per-model statistics\n        model_stats = (\n            df.group_by(\"model\")\n            .agg(\n                [\n                    pl.col(col_name).mean().alias(\"mean\"),\n                    pl.col(col_name).std().alias(\"std\"),\n                ]\n            )\n            .with_columns((pl.col(\"mean\") / pl.col(\"std\")).alias(\"sharpe\"))\n        )\n\n        # Create concise title with mean values\n        mean_values = model_stats.to_pandas()\n        mean_text = \" | \".join(\n            [f\"{row['model']}: {row['mean']:.3f}\" for _, row in mean_values.iterrows()]\n        )\n        title_text = f\"{metric_name.upper()} {timeframe}\\nMeans: {mean_text}\"\n\n        chart = (\n            alt.Chart(pdf)\n            .add_params(brush)\n            .mark_point(opacity=0.6)\n            .encode(\n                x=alt.X(\"date:T\", title=\"Date\"),\n                y=alt.Y(\n                    f\"{col_name}:Q\",\n                    title=f\"{metric_name.upper()} {timeframe}\",\n                    scale=alt.Scale(domain=y_domain),\n                ),\n                color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),\n                tooltip=[\n                    \"date:T\",\n                    \"model:N\",\n                    f\"{col_name}:Q\",\n                    alt.Tooltip(\"model:N\", title=\"Model\"),\n                ],\n            )\n            .properties(\n                width=width,\n                height=height,\n                title=alt.TitleParams(\n                    text=title_text,\n                    fontSize=10,\n                    anchor=\"start\",\n                ),\n            )\n        )\n\n        moving_average_chart = (\n            alt.Chart(pdf)\n            .mark_line(strokeWidth=2, opacity=1)\n            .encode(\n                x=alt.X(\"date:T\"),\n                y=alt.Y(f\"{col_name}_ma30:Q\", scale=alt.Scale(domain=y_domain)),\n                color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),\n                tooltip=[\n                    \"date:T\",\n                    \"model:N\",\n                    f\"{col_name}_ma30:Q\",\n                    alt.Tooltip(\"model:N\", title=\"Model\"),\n                ],\n            )\n        )\n\n        # Add per-model mean reference lines\n        mean_lines = []\n        for row in model_stats.iter_rows(named=True):\n            model = row[\"model\"]\n            mean_val = row[\"mean\"]\n\n            mean_line = (\n                alt.Chart(pdf[pdf[\"model\"] == model])\n                .mark_rule(strokeDash=[5, 5], opacity=1, strokeWidth=2)\n                .encode(\n                    y=alt.datum(mean_val),\n                    color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=0.7)),\n                )\n            )\n            mean_lines.append(mean_line)\n\n        # Add anchor reference line\n        anchor_line = (\n            alt.Chart(pdf)\n            .mark_rule(strokeDash=[2, 2], opacity=0.7, color=\"gray\", strokeWidth=1)\n            .encode(y=alt.datum(anchor_ref))\n        )\n\n        combined_chart = chart + moving_average_chart + anchor_line\n        for mean_line in mean_lines:\n            combined_chart += mean_line\n\n        charts.append(combined_chart)\n\n    return alt.hconcat(*charts, spacing=10).resolve_scale(x=\"shared\")\n\n\n# Plot both metrics\nspearman_chart = plot_metric_comparison(daily_scores_df, \"spearman\", anchor_ref=0)\nndcg_chart = plot_metric_comparison(daily_scores_df, \"ndcg@40\", anchor_ref=0.5)\n\n# Combine vertically\nalt.vconcat(spearman_chart, ndcg_chart, spacing=20)\n</pre> def plot_metric_comparison(df, metric_name, anchor_ref=0, width=400, height=200):     \"\"\"Plot a single metric across 10d and 30d timeframes side by side with shared y-axis.      Args:         df: Polars DataFrame with daily scores         metric_name: Base metric name ('spearman' or 'ndcg@40')         width: Chart width in pixels per timeframe         height: Chart height     \"\"\"     # Convert to pandas once     pdf = df.to_pandas()      # Shared selection for zooming     brush = alt.selection_interval(bind=\"scales\", encodings=[\"x\"])      # Define timeframes and colors     timeframes = [\"10d\", \"30d\"]      # Get column names for this metric     col_10d = f\"{metric_name}_10d\"     col_30d = f\"{metric_name}_30d\"      # Calculate shared y-axis domain     min_val = min(df[col_10d].min(), df[col_30d].min())     max_val = max(df[col_10d].max(), df[col_30d].max())     y_domain = [min_val * 0.95, max_val * 1.05]      charts = []     for timeframe in timeframes:         col_name = f\"{metric_name}_{timeframe}\"          # Calculate per-model statistics         model_stats = (             df.group_by(\"model\")             .agg(                 [                     pl.col(col_name).mean().alias(\"mean\"),                     pl.col(col_name).std().alias(\"std\"),                 ]             )             .with_columns((pl.col(\"mean\") / pl.col(\"std\")).alias(\"sharpe\"))         )          # Create concise title with mean values         mean_values = model_stats.to_pandas()         mean_text = \" | \".join(             [f\"{row['model']}: {row['mean']:.3f}\" for _, row in mean_values.iterrows()]         )         title_text = f\"{metric_name.upper()} {timeframe}\\nMeans: {mean_text}\"          chart = (             alt.Chart(pdf)             .add_params(brush)             .mark_point(opacity=0.6)             .encode(                 x=alt.X(\"date:T\", title=\"Date\"),                 y=alt.Y(                     f\"{col_name}:Q\",                     title=f\"{metric_name.upper()} {timeframe}\",                     scale=alt.Scale(domain=y_domain),                 ),                 color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),                 tooltip=[                     \"date:T\",                     \"model:N\",                     f\"{col_name}:Q\",                     alt.Tooltip(\"model:N\", title=\"Model\"),                 ],             )             .properties(                 width=width,                 height=height,                 title=alt.TitleParams(                     text=title_text,                     fontSize=10,                     anchor=\"start\",                 ),             )         )          moving_average_chart = (             alt.Chart(pdf)             .mark_line(strokeWidth=2, opacity=1)             .encode(                 x=alt.X(\"date:T\"),                 y=alt.Y(f\"{col_name}_ma30:Q\", scale=alt.Scale(domain=y_domain)),                 color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=1.0)),                 tooltip=[                     \"date:T\",                     \"model:N\",                     f\"{col_name}_ma30:Q\",                     alt.Tooltip(\"model:N\", title=\"Model\"),                 ],             )         )          # Add per-model mean reference lines         mean_lines = []         for row in model_stats.iter_rows(named=True):             model = row[\"model\"]             mean_val = row[\"mean\"]              mean_line = (                 alt.Chart(pdf[pdf[\"model\"] == model])                 .mark_rule(strokeDash=[5, 5], opacity=1, strokeWidth=2)                 .encode(                     y=alt.datum(mean_val),                     color=alt.Color(\"model:N\", legend=alt.Legend(symbolOpacity=0.7)),                 )             )             mean_lines.append(mean_line)          # Add anchor reference line         anchor_line = (             alt.Chart(pdf)             .mark_rule(strokeDash=[2, 2], opacity=0.7, color=\"gray\", strokeWidth=1)             .encode(y=alt.datum(anchor_ref))         )          combined_chart = chart + moving_average_chart + anchor_line         for mean_line in mean_lines:             combined_chart += mean_line          charts.append(combined_chart)      return alt.hconcat(*charts, spacing=10).resolve_scale(x=\"shared\")   # Plot both metrics spearman_chart = plot_metric_comparison(daily_scores_df, \"spearman\", anchor_ref=0) ndcg_chart = plot_metric_comparison(daily_scores_df, \"ndcg@40\", anchor_ref=0.5)  # Combine vertically alt.vconcat(spearman_chart, ndcg_chart, spacing=20) <pre>/tmp/ipykernel_3825538/3460603390.py:130: UserWarning: Automatically deduplicated selection parameter with identical configuration. If you want independent parameters, explicitly name them differently (e.g., name='param1', name='param2'). See https://github.com/vega/altair/issues/3891\n  spearman_chart = plot_metric_comparison(daily_scores_df, \"spearman\", anchor_ref=0)\n/tmp/ipykernel_3825538/3460603390.py:131: UserWarning: Automatically deduplicated selection parameter with identical configuration. If you want independent parameters, explicitly name them differently (e.g., name='param1', name='param2'). See https://github.com/vega/altair/issues/3891\n  ndcg_chart = plot_metric_comparison(daily_scores_df, \"ndcg@40\", anchor_ref=0.5)\n/home/exx/projects/crowdcent-challenge/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3701: UserWarning: Automatically deduplicated selection parameter with identical configuration. If you want independent parameters, explicitly name them differently (e.g., name='param1', name='param2'). See https://github.com/vega/altair/issues/3891\n  exec(code_obj, self.user_global_ns, self.user_ns)\n</pre> Out[10]:"},{"location":"tutorials/advanced-cv-lstm/#key-insight-sequential-feature-processing","title":"Key Insight: Sequential Feature Processing\u00b6","text":"<p>CrowdCent's training and inference data contains features with a defined temporal sequence through lag windows (e.g., <code>feature_1_lag15</code>, <code>feature_1_lag10</code>, <code>feature_1_lag5</code>, <code>feature_1_lag0</code>). While traditional models like XGBoost treat these as independent features, an LSTM can process them sequentially, capturing temporal dependencies.</p>"},{"location":"tutorials/advanced-cv-lstm/#model-comparison","title":"Model Comparison\u00b6","text":"<ul> <li>XGBoost: Treats all features as independent inputs, achieving ~0.06 average 30-day Spearman correlation</li> <li>LSTM (from Centimators): Processes features sequentially by reshaping them along the lag axis, achieving ~0.19 average 30-day Spearman correlation - over 3x improvement!</li> </ul>"},{"location":"tutorials/advanced-cv-lstm/#feature-reshaping-example","title":"Feature Reshaping Example\u00b6","text":"<p>The LSTM transforms the flat feature vector:</p> <pre><code>[feature_1_lag10, feature_1_lag5, feature_1_lag0, feature_2_lag10, feature_2_lag5, feature_2_lag0, ...]\n</code></pre> <p>into a sequential 2D tensor:</p> <pre><code>[[feature_1_lag10, feature_2_lag10],\n [feature_1_lag5, feature_2_lag5],\n [feature_1_lag0, feature_2_lag0]]\n</code></pre>"},{"location":"tutorials/advanced-cv-lstm/#what-youll-learn","title":"What You'll Learn\u00b6","text":"<ol> <li>How to set up proper time-series cross-validation with gap periods to prevent leakage</li> <li>How to train both XGBoost and LSTM models on the same features for fair comparison</li> <li>How to evaluate models using CrowdCent's official scoring metrics (Spearman correlation and NDCG@40)</li> <li>How to visualize performance over time with moving averages</li> </ol>"},{"location":"tutorials/advanced-cv-lstm/#key-findings","title":"Key Findings\u00b6","text":"<ul> <li>10-day predictions achieve higher raw scores than 30-day predictions (e.g., 0.255 vs 0.19 spearman corr for LSTM)</li> <li>The LSTM significantly outperforms XGBoost without any hyperparameter tuning (experimental results)</li> <li>All results shown are out-of-sample using proper time-series cross-validation (TimeGapSplit from sklego)</li> </ul> <p>Note: This comparison uses identical features for both models and no hyperparameter optimization, demonstrating the power of sequential processing for this dataset.</p>"},{"location":"tutorials/advanced-cv-lstm/#initialize-the-client","title":"Initialize the client\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#get-crowdcents-training-data","title":"Get CrowdCent's training data\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#create-cross-validation-folds","title":"Create cross validation folds\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#define-features-targets-and-lag-windows","title":"Define features, targets, and lag windows\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#train-models-on-each-train-fold-and-predict-validation","title":"Train models on each train fold and predict validation\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#calculate-scores","title":"Calculate scores\u00b6","text":""},{"location":"tutorials/advanced-cv-lstm/#plot-validation-metrics-combining-all-folds","title":"Plot validation metrics (combining all folds)\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/","title":"Hyperliquid End-to-End","text":"In\u00a0[1]: Copied! <pre>!pip install crowdcent-challenge\nimport crowdcent_challenge as cc\nimport polars as pl\nfrom xgboost import XGBRegressor\n</pre> !pip install crowdcent-challenge import crowdcent_challenge as cc import polars as pl from xgboost import XGBRegressor <pre>/bin/bash: pip: command not found\n</pre> <p>For this tutorial, you will need:</p> <ol> <li>CrowdCent account: register for free</li> <li>CrowdCent API Key: generate an API key from your user profile</li> </ol> In\u00a0[2]: Copied! <pre>CROWDCENT_API_KEY = \"API_KEY_HERE\"\n</pre> CROWDCENT_API_KEY = \"API_KEY_HERE\" In\u00a0[\u00a0]: Copied! <pre>client = cc.ChallengeClient(\n    challenge_slug=\"hyperliquid-ranking\",\n    api_key=CROWDCENT_API_KEY,\n)\n</pre> client = cc.ChallengeClient(     challenge_slug=\"hyperliquid-ranking\",     api_key=CROWDCENT_API_KEY, ) <pre>2026-02-04 21:19:40,206 - INFO - ChallengeClient initialized for 'hyperliquid-ranking' at URL: https://crowdcent.com/api\n</pre> In\u00a0[4]: Copied! <pre>client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")\n\ntraining_data = pl.read_parquet(\"training_data.parquet\")\ntraining_data.head()\n</pre> client.download_training_dataset(version=\"latest\", dest_path=\"training_data.parquet\")  training_data = pl.read_parquet(\"training_data.parquet\") training_data.head() <pre>2026-02-04 21:19:40,508 - INFO - Downloading training data v2.0 to training_data.parquet\nDownloading training_data.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 124M/124M [00:02&lt;00:00, 55.0MB/s] \n2026-02-04 21:19:43,278 - INFO - Successfully downloaded training data v2.0 to training_data.parquet\n</pre> Out[4]: shape: (5, 85)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_5_lag5feature_6_lag5feature_7_lag5feature_12_lag5feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0target_10dtarget_30dstrstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\"0G\"\"0G-USD.CC\"2025-11-16 00:00:000.1576920.1563360.2397620.3133490.0519230.1351220.2697350.3003530.1894230.2000070.2277810.3365930.1615380.1452040.2380610.2911210.6057690.5878160.5668550.5161270.2644230.2110580.2416170.2967680.3298080.1908650.2565350.319130.4076920.2985580.2770470.3219810.2576920.209615\u20260.3533490.2442350.339430.5490110.4783520.3391790.3445930.4961170.3769050.2610540.3119140.2701460.3244960.4561560.4752740.5224880.50930.3601790.3282060.6114830.4941870.3425260.3510890.5368420.5429260.4207420.3313810.5244020.5102590.3599370.3226470.5885170.4293320.460820.4946380.8325360.186603\"0G\"\"0G-USD.CC\"2025-11-17 00:00:000.1605770.1451920.2382970.2911240.0355770.1341350.2571640.2809250.0211540.2004810.2243950.3120740.0336540.1461540.2247680.2672060.6096150.6033650.6031320.5120470.2576920.2091350.2365360.2776810.3326920.1841350.251690.3177940.4173080.2192310.2717170.3028910.3346150.184135\u20260.4207930.2774640.3326120.6398560.5285820.3645310.3339540.5942810.4644480.3053010.3096410.4317310.3384620.4709130.4747990.5244020.510720.3599270.3226390.4708130.4898530.3369940.3481870.4669860.5534210.3863260.3338760.412440.5033610.3437480.3289640.4009570.4163440.4218740.4714460.7799040.167464\"0G\"\"0G-USD.CC\"2025-11-18 00:00:000.0326920.1456730.2250030.2672080.2250.2288460.2805940.3055610.2153850.2817310.2640130.3136560.2259620.2283650.2684890.2918430.7692310.6394230.6164180.5366840.3346150.1836540.254310.2839110.3326920.2788460.2975830.3413450.3923080.3038460.3137810.3241640.3355770.280769\u20260.3394530.284150.3208110.628510.5104090.396070.3300110.5014580.4185180.3234420.3229180.2422620.2437270.4415750.4747930.412440.5038210.3437370.3290760.4842110.4152120.3470290.3483680.3550240.4917670.3978070.3381660.4535890.4775230.3791460.3438820.4765550.3594080.433310.481850.8851670.22488\"0G\"\"0G-USD.CC\"2025-11-19 00:00:000.2250.2278850.2687240.2918450.3278850.2706730.3024420.3190210.4096150.3774040.308220.3380530.2615380.2177880.2706170.2961660.6076920.5846150.5883950.5359520.3355770.2802880.299720.308670.3721150.350.2903990.3597330.3942310.4019230.3558110.3478370.4951920.378365\u20260.3167030.2936880.3285850.4897590.4419950.4096990.3358650.3359220.4155570.3166730.3241010.2287220.2504190.4175170.4717780.4535890.4779840.3791360.3439940.5674640.4143770.3821890.3474650.365550.4276550.4147890.325520.4401910.3880570.3832110.3250190.5521530.3904380.4151710.4858460.8708130.301435\"0G\"\"0G-USD.CC\"2025-11-20 00:00:000.2605770.2173080.2708530.2961680.3298080.2519230.2891520.3185310.4144230.3004810.2833310.3385290.2653850.2115380.2413810.2967640.4451920.5788460.5644720.5133690.4951920.3778850.3014140.3324640.3778850.3538460.2430440.3388340.5461540.4802880.3466810.34460.4951920.380288\u20260.4023390.3271310.3281320.3617220.4539380.3772090.319160.5224880.508840.3601890.3280940.4191390.3427420.4607940.473960.4401910.3885170.3832010.3251310.4028710.4148330.3843390.3278870.424880.3933010.4367950.3324150.3081340.4153110.39780.3185430.4277510.4234450.3896070.4902310.7990430.124402 In\u00a0[5]: Copied! <pre>xgb_regressor = XGBRegressor(n_estimators=200, device=\"cuda\")\nfeature_names = [col for col in training_data.columns if col.startswith(\"feature\")]\n\nxgb_regressor.fit(\n    training_data[feature_names],\n    training_data[[\"target_10d\", \"target_30d\"]],\n)\n</pre> xgb_regressor = XGBRegressor(n_estimators=200, device=\"cuda\") feature_names = [col for col in training_data.columns if col.startswith(\"feature\")]  xgb_regressor.fit(     training_data[feature_names],     training_data[[\"target_10d\", \"target_30d\"]], ) Out[5]: <pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n             n_jobs=None, num_parallel_tree=None, ...)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor?Documentation for XGBRegressoriFitted Parameters              objective             objective: typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]Specify the learning task and the corresponding learning objective or a customobjective function to be used.For custom objective, see :doc:`/tutorials/custom_metric_obj` and:ref:`custom-obj-metric` for more information, along with the end note forfunction signatures. 'reg:squarederror'              base_score             base_score: typing.Union[float, typing.List[float], NoneType]The initial prediction score of all instances, global bias. None booster None              callbacks             callbacks: typing.Optional[typing.List[xgboost.callback.TrainingCallback]]List of callback functions that are applied at end of each iteration.It is possible to use predefined callbacks by using:ref:`Callback API `... note::   States in callback are not preserved during training, which means callback   objects can not be reused for multiple training sessions without   reinitialization or deepcopy... code-block:: python    for params in parameters_grid:        # be sure to (re)initialize the callbacks before each run        callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]        reg = xgboost.XGBRegressor(**params, callbacks=callbacks)        reg.fit(X, y) None              colsample_bylevel             colsample_bylevel: typing.Optional[float]Subsample ratio of columns for each level. None              colsample_bynode             colsample_bynode: typing.Optional[float]Subsample ratio of columns for each split. None              colsample_bytree             colsample_bytree: typing.Optional[float]Subsample ratio of columns when constructing each tree. None              device             device: typing.Optional[str].. versionadded:: 2.0.0Device ordinal, available options are `cpu`, `cuda`, and `gpu`. 'cuda'              early_stopping_rounds             early_stopping_rounds: typing.Optional[int].. versionadded:: 1.6.0- Activates early stopping. Validation metric needs to improve at least once in  every **early_stopping_rounds** round(s) to continue training.  Requires at  least one item in **eval_set** in :py:meth:`fit`.- If early stopping occurs, the model will have two additional attributes:  :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the  :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal  number of trees during inference. If users want to access the full model  (including trees built after early stopping), they can specify the  `iteration_range` in these inference methods. In addition, other utilities  like model plotting can also use the entire model.- If you prefer to discard the trees after `best_iteration`, consider using the  callback function :py:class:`xgboost.callback.EarlyStopping`.- If there's more than one item in **eval_set**, the last entry will be used for  early stopping.  If there's more than one metric in **eval_metric**, the last  metric will be used for early stopping. None              enable_categorical             enable_categorical: boolSee the same parameter of :py:class:`DMatrix` for details. False              eval_metric             eval_metric: typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType].. versionadded:: 1.6.0Metric used for monitoring the training result and early stopping.  It can be astring or list of strings as names of predefined metric in XGBoost (See:doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or anyother user defined metric that looks like `sklearn.metrics`.If custom objective is also provided, then custom metric should implement thecorresponding reverse link function.Unlike the `scoring` parameter commonly used in scikit-learn, when a callableobject is provided, it's assumed to be a cost function and by default XGBoostwill minimize the result during early stopping.For advanced usage on Early stopping like directly choosing to maximize insteadof minimize, see :py:obj:`xgboost.callback.EarlyStopping`.See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for moreinformation... code-block:: python    from sklearn.datasets import load_diabetes    from sklearn.metrics import mean_absolute_error    X, y = load_diabetes(return_X_y=True)    reg = xgb.XGBRegressor(        tree_method=\"hist\",        eval_metric=mean_absolute_error,    )    reg.fit(X, y, eval_set=[(X, y)]) None              feature_types             feature_types: typing.Optional[typing.Sequence[str]].. versionadded:: 1.7.0Used for specifying feature types without constructing a dataframe. Seethe :py:class:`DMatrix` for details. None              feature_weights             feature_weights: Optional[ArrayLike]Weight for each feature, defines the probability of each feature being selectedwhen colsample is being used.  All values must be greater than 0, otherwise a`ValueError` is thrown. None              gamma             gamma: typing.Optional[float](min_split_loss) Minimum loss reduction required to make a further partition ona leaf node of the tree. None              grow_policy             grow_policy: typing.Optional[str]Tree growing policy.- depthwise: Favors splitting at nodes closest to the node,- lossguide: Favors splitting at nodes with highest loss change. None importance_type None              interaction_constraints             interaction_constraints: typing.Union[str, typing.List[typing.Tuple[str]], NoneType]Constraints for interaction representing permitted interactions.  Theconstraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,3, 4]]``, where each inner list is a group of indices of features that areallowed to interact with each other.  See :doc:`tutorial` for more information None              learning_rate             learning_rate: typing.Optional[float]Boosting learning rate (xgb's \"eta\") None              max_bin             max_bin: typing.Optional[int]If using histogram-based algorithm, maximum number of bins per feature None              max_cat_threshold             max_cat_threshold: typing.Optional[int].. versionadded:: 1.7.0.. note:: This parameter is experimentalMaximum number of categories considered for each split. Used only bypartition-based splits for preventing over-fitting. Also, `enable_categorical`needs to be set to have categorical feature support. See :doc:`Categorical Data` and :ref:`cat-param` for details. None              max_cat_to_onehot             max_cat_to_onehot: Optional[int].. versionadded:: 1.6.0.. note:: This parameter is experimentalA threshold for deciding whether XGBoost should use one-hot encoding based splitfor categorical data.  When number of categories is lesser than the thresholdthen one-hot encoding is chosen, otherwise the categories will be partitionedinto children nodes. Also, `enable_categorical` needs to be set to havecategorical feature support. See :doc:`Categorical Data` and :ref:`cat-param` for details. None              max_delta_step             max_delta_step: typing.Optional[float]Maximum delta step we allow each tree's weight estimation to be. None              max_depth             max_depth:  typing.Optional[int]Maximum tree depth for base learners. None              max_leaves             max_leaves: typing.Optional[int]Maximum number of leaves; 0 indicates no limit. None              min_child_weight             min_child_weight: typing.Optional[float]Minimum sum of instance weight(hessian) needed in a child. None              missing             missing: floatValue in the data which needs to be present as a missing value. Default to:py:data:`numpy.nan`. nan              monotone_constraints             monotone_constraints: typing.Union[typing.Dict[str, int], str, NoneType]Constraint of variable monotonicity.  See :doc:`tutorial `for more information. None              multi_strategy             multi_strategy: typing.Optional[str].. versionadded:: 2.0.0.. note:: This parameter is working-in-progress.The strategy used for training multi-target models, including multi-targetregression and multi-class classification. See :doc:`/tutorials/multioutput` formore information.- ``one_output_per_tree``: One model for each target.- ``multi_output_tree``:  Use multi-target trees. None              n_estimators             n_estimators: typing.Optional[int]Number of gradient boosted trees.  Equivalent to number of boostingrounds. 200              n_jobs             n_jobs: typing.Optional[int]Number of parallel threads used to run xgboost.  When used with otherScikit-Learn algorithms like grid search, you may choose which algorithm toparallelize and balance the threads.  Creating thread contention willsignificantly slow down both algorithms. None num_parallel_tree None              random_state             random_state: typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]Random number seed... note::   Using gblinear booster with shotgun updater is nondeterministic as   it uses Hogwild algorithm. None              reg_alpha             reg_alpha: typing.Optional[float]L1 regularization term on weights (xgb's alpha). None              reg_lambda             reg_lambda: typing.Optional[float]L2 regularization term on weights (xgb's lambda). None              sampling_method             sampling_method: typing.Optional[str]Sampling method. Used only by the GPU version of ``hist`` tree method.- ``uniform``: Select random training instances uniformly.- ``gradient_based``: Select random training instances with higher probability    when the gradient and hessian are larger. (cf. CatBoost) None              scale_pos_weight             scale_pos_weight: typing.Optional[float]Balancing of positive and negative weights. None              subsample             subsample: typing.Optional[float]Subsample ratio of the training instance. None              tree_method             tree_method: typing.Optional[str]Specify which tree method to use.  Default to auto.  If this parameter is set todefault, XGBoost will choose the most conservative option available.  It'srecommended to study this option from the parameters document :doc:`tree method` None              validate_parameters             validate_parameters: typing.Optional[bool]Give warnings for unknown parameter. None              verbosity             verbosity: typing.Optional[int]The degree of verbosity. Valid values are 0 (silent) - 3 (debug). None In\u00a0[6]: Copied! <pre>client.download_inference_data(\"latest\", \"inference_data.parquet\")\n\ninference_data = pl.read_parquet(\"inference_data.parquet\")\ninference_data.head()\n</pre> client.download_inference_data(\"latest\", \"inference_data.parquet\")  inference_data = pl.read_parquet(\"inference_data.parquet\") inference_data.head() <pre>2026-02-04 21:19:46,297 - INFO - Downloading inference data 2026-02-04 to inference_data.parquet\nDownloading inference_data.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 138k/138k [00:00&lt;00:00, 65.9MB/s]\n2026-02-04 21:19:46,465 - INFO - Successfully downloaded inference data 2026-02-04 to inference_data.parquet\n</pre> Out[6]: shape: (5, 83)ideodhd_iddatefeature_16_lag15feature_13_lag15feature_14_lag15feature_15_lag15feature_8_lag15feature_5_lag15feature_6_lag15feature_7_lag15feature_12_lag15feature_9_lag15feature_10_lag15feature_11_lag15feature_4_lag15feature_1_lag15feature_2_lag15feature_3_lag15feature_20_lag15feature_17_lag15feature_18_lag15feature_19_lag15feature_16_lag10feature_13_lag10feature_14_lag10feature_15_lag10feature_8_lag10feature_5_lag10feature_6_lag10feature_7_lag10feature_12_lag10feature_9_lag10feature_10_lag10feature_11_lag10feature_4_lag10feature_1_lag10\u2026feature_15_lag5feature_8_lag5feature_5_lag5feature_6_lag5feature_7_lag5feature_12_lag5feature_9_lag5feature_10_lag5feature_11_lag5feature_4_lag5feature_1_lag5feature_2_lag5feature_3_lag5feature_20_lag5feature_17_lag5feature_18_lag5feature_19_lag5feature_16_lag0feature_13_lag0feature_14_lag0feature_15_lag0feature_8_lag0feature_5_lag0feature_6_lag0feature_7_lag0feature_12_lag0feature_9_lag0feature_10_lag0feature_11_lag0feature_4_lag0feature_1_lag0feature_2_lag0feature_3_lag0feature_20_lag0feature_17_lag0feature_18_lag0feature_19_lag0strstrdatetime[\u03bcs]f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\u2026f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64f64\"BIO\"\"BIO.CC\"2026-02-03 00:00:000.5989250.468370.4506710.4873630.5150540.550320.5021870.5183910.679570.5474720.5061680.5159220.6526880.5469980.4872830.4874250.5301080.5126710.4987680.47060.7032260.6510750.5167480.5109420.7053760.6102150.5339360.5434550.7365590.7080650.5597680.5406950.6860220.669355\u20260.4961180.2473120.4763440.5133320.5182880.1634410.450.4987360.5008540.20.4430110.4950040.4852050.3892470.4768820.4947760.4918480.281650.310180.4806270.4649730.2404470.2438790.4270470.4655060.2165310.1899860.4490250.4583260.3017020.2508510.4601030.4538640.5017370.4454920.4964020.476321\"JUP\"\"JUP29210-USD.CC\"2026-02-03 00:00:000.5193550.515870.5514490.5386970.4311830.512990.5548740.5244640.5075270.5129930.5432530.5191940.4419350.5157050.5378520.5368990.4849460.511450.5173470.5061060.5397850.529570.5334220.5607650.559140.4951610.5379020.5613830.5344090.5209680.5263150.5480220.5946240.51828\u20260.5619270.5978490.5784950.5457430.5580060.7333330.6338710.5734320.5549590.6225810.6086020.5621530.5554010.6290320.5537630.5326070.5150870.501660.5911530.5603610.5451750.4708440.5343470.5147540.5329430.535880.6346070.5777870.5438360.4874930.5550370.5366580.5386350.4977370.5633850.5225530.509872\"BABY\"\"BABY32198-USD.CC\"2026-02-03 00:00:000.6483870.5730110.4983970.5096040.6892470.4845220.4690180.5038330.6881720.51240.5124160.5156680.6838710.5516450.4993360.5054790.5731180.4680030.4731910.4722710.6365590.6424730.5648340.5284980.6516130.670430.5451640.5210960.60.6440860.5707950.5218840.5784950.631183\u20260.5204130.6774190.6645160.5745190.5259080.6731180.6365590.574480.5416990.7032260.640860.5962530.5315050.540860.5134410.4907220.484280.687770.6266810.6345770.5463850.6268640.6521420.6612860.5418340.7228110.6979650.6710250.5743480.6132990.6582620.6447230.5501940.3138010.4273310.478450.451884\"CHILLGUY\"\"CHILLGUY-USD.CC\"2026-02-03 00:00:000.2290320.4074750.5623860.5159230.2397850.3671550.5468210.523140.2688170.3901390.5658810.5211840.2505380.3322990.5375010.5152370.538710.493110.4954740.5238180.2247310.2268820.4047850.4884740.3311830.2854840.4117790.4950780.1430110.2059140.4113630.4660870.3473120.298925\u20260.4923090.5225810.4268820.3970180.4999960.5333330.3381720.3641560.4804560.4021510.3747310.3535150.479730.4344090.4779570.4855330.511010.4230530.4733550.3501180.4818120.5011820.5118810.3986830.4979140.5005080.5169210.3614170.4905870.5470520.4746010.3867630.4879770.5612310.497820.5139640.512921\"SPX\"\"SPX28081-USD.CC\"2026-02-03 00:00:000.3236560.4800260.507310.4840610.218280.4184540.4705780.4431270.2580650.4379980.4725120.4565260.2634410.4189970.4738230.4761010.50.4936380.5149270.4920580.1849460.2543010.3773310.453260.1096770.1639780.3252950.430620.2225810.2403230.3681580.454890.1569890.210215\u20260.4477190.420430.2650540.3417540.4356070.4731180.3478490.3929240.4534890.418280.2876340.3533160.4416580.4139780.5043010.498970.5082690.3758540.3884640.3213830.4361870.3428870.3816580.2728180.4117110.51760.4953590.3678410.4567020.4208260.4195530.3148840.432910.5396730.4768260.5120690.511539 In\u00a0[7]: Copied! <pre>preds = xgb_regressor.predict(inference_data[feature_names])\npred_df = pl.from_numpy(preds, [\"pred_10d\", \"pred_30d\"])\n</pre> preds = xgb_regressor.predict(inference_data[feature_names]) pred_df = pl.from_numpy(preds, [\"pred_10d\", \"pred_30d\"]) <pre>/home/exx/projects/crowdcent-challenge/.venv/lib/python3.12/site-packages/xgboost/core.py:774: UserWarning: [21:19:46] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  return func(**kwargs)\n</pre> In\u00a0[8]: Copied! <pre>pred_df = pred_df.with_columns(inference_data[\"id\"]).select(\n    [\"id\", \"pred_10d\", \"pred_30d\"]\n)\n\n# ensure predictions are between 0 and 1\npred_df = pred_df.with_columns(pl.col([\"pred_10d\", \"pred_30d\"]).clip(0, 1))\n\n\nwith pl.Config(tbl_rows=20):\n    display(pred_df.sort(\"pred_30d\", descending=True))\n</pre> pred_df = pred_df.with_columns(inference_data[\"id\"]).select(     [\"id\", \"pred_10d\", \"pred_30d\"] )  # ensure predictions are between 0 and 1 pred_df = pred_df.with_columns(pl.col([\"pred_10d\", \"pred_30d\"]).clip(0, 1))   with pl.Config(tbl_rows=20):     display(pred_df.sort(\"pred_30d\", descending=True)) shape: (186, 3)idpred_10dpred_30dstrf32f32\"SOPH\"0.4819240.825876\"BERA\"0.3633610.745414\"HYPE\"0.442650.693696\"XPL\"0.7059750.667599\"MERL\"0.6245240.642958\"STRK\"0.5993830.632716\"ALGO\"0.5418230.620179\"TRX\"0.4944620.608403\"LDO\"0.5408880.608148\"ZRO\"0.4472250.607567\u2026\u2026\u2026\"JTO\"0.2946150.351979\"TNSR\"0.4101360.350033\"AVNT\"0.1981440.342407\"XMR\"0.4953070.339069\"BIO\"0.3162030.298175\"LAYER\"0.6309560.290479\"DASH\"0.3093540.289107\"BLUR\"0.2414450.286826\"CC\"0.6768660.24413\"AXS\"0.5363690.209781 In\u00a0[9]: Copied! <pre># directly submit a dataframe to slot 1\nclient.submit_predictions(df=pred_df, slot=1)\n</pre> # directly submit a dataframe to slot 1 client.submit_predictions(df=pred_df, slot=1) <pre>2026-02-04 21:19:46,617 - INFO - Wrote DataFrame to temporary file: submission.parquet\n2026-02-04 21:19:46,618 - INFO - Submitting predictions from submission.parquet to challenge 'hyperliquid-ranking' (Slot: 1)\n2026-02-04 21:19:47,091 - INFO - Submission queued (slot 1)\n</pre> Out[9]: <pre>{'match_info': {'matched_ids': 186,\n  'unmatched_ids': 0,\n  'message': '186 IDs matched inference data'},\n 'status': 'queued',\n 'message': 'Submission queued for slot 1. Will be automatically submitted when next period opens.',\n 'slot': 1,\n 'challenge': 'hyperliquid-ranking'}</pre>"},{"location":"tutorials/hyperliquid-end-to-end/#build-a-model-on-crowdcents-training-data-and-submit","title":"Build a model on CrowdCent's training data and submit\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#load-api-key","title":"Load API key\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#initialize-the-client","title":"Initialize the client\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#get-crowdcents-training-data","title":"Get CrowdCent's training data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#train-a-model-on-the-training-data","title":"Train a model on the training data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#get-crowdcents-latest-inference-data","title":"Get CrowdCent's latest inference data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#make-predictions-on-the-inference-data","title":"Make predictions on the inference data\u00b6","text":""},{"location":"tutorials/hyperliquid-end-to-end/#submit-to-the-hyperliquid-ranking-challenge-on-crowdcent","title":"Submit to the <code>hyperliquid-ranking</code> challenge on CrowdCent\u00b6","text":""},{"location":"tutorials/submission-automation/","title":"Submission Automation","text":""},{"location":"tutorials/submission-automation/#scheduling-a-kaggle-notebook","title":"Scheduling a Kaggle notebook","text":"<p>If you're just starting out, we recommend using Kaggle Notebooks to schedule your submissions.</p> <ol> <li>Settings (\u2699) \u2192 Schedule a notebook run \u2192 On </li> <li>Choose Frequency (daily / weekly / monthly), Start date, Runs \u2264 10 \u2192 Save </li> <li>A clock icon appears; each run writes a new Version with full logs &amp; outputs  </li> <li>Limits: CPU-only \u2022 \u2264 9 h per run \u2022 1 private / 5 public schedules active </li> <li>Pause or delete the job anytime from the same Settings card  </li> </ol> <p><sub>Need GPUs? Trigger notebook commits with the Kaggle API from cron/GitHub Actions.</sub></p>"},{"location":"tutorials/submission-automation/#scheduling-a-google-colab-vertex-ai-notebook","title":"Scheduling a Google Colab (Vertex AI) notebook","text":"<p>https://www.youtube.com/watch?v=ypGah2gRYck</p> <ol> <li>Create a Google Cloud account if you don't have one already</li> <li>Go to Google Colab Notebooks in Vertex AI</li> <li>Set up a schedule:</li> <li>Open your notebook in Colab</li> <li>Click Runtime \u2192 Manage sessions</li> <li>Select Recurring and configure your schedule</li> <li>Set frequency (daily/weekly/monthly) and duration</li> <li>Click Save</li> <li>Authentication options:</li> <li>Use service account keys stored securely</li> <li>Set up environment variables in the Vertex AI console</li> <li>Use Google Cloud's Secret Manager for API keys</li> </ol> <p><sub>Note: Scheduled Colab notebooks run on Google Cloud and may incur charges based on your usage.</sub></p>"},{"location":"tutorials/track-your-performance/","title":"Track Your Performance","text":"<p>This tutorial shows how to retrieve and analyze your submission history programmaticaly, track scores over time, and identify your best-performing strategies.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install crowdcent-challenge altair polars\nimport crowdcent_challenge as cc\nimport polars as pl\nimport altair as alt\n</pre> %pip install crowdcent-challenge altair polars import crowdcent_challenge as cc import polars as pl import altair as alt <p>For this tutorial, you will need:</p> <ol> <li>CrowdCent account: register for free</li> <li>CrowdCent API Key: generate an API key from your user profile</li> <li>Some scored submissions: Submit predictions and wait for scoring</li> </ol> In\u00a0[2]: Copied! <pre>CROWDCENT_API_KEY = \"API_KEY_HERE\"\n</pre> CROWDCENT_API_KEY = \"API_KEY_HERE\" In\u00a0[\u00a0]: Copied! <pre>client = cc.ChallengeClient(\n    challenge_slug=\"hyperliquid-ranking\",\n    api_key=CROWDCENT_API_KEY\n)\n</pre> client = cc.ChallengeClient(     challenge_slug=\"hyperliquid-ranking\",     api_key=CROWDCENT_API_KEY ) <pre>2025-12-30 15:08:08,401 - INFO - ChallengeClient initialized for 'hyperliquid-ranking' at URL: https://crowdcent.com/api\n</pre> In\u00a0[4]: Copied! <pre>history = client.get_performance()\nhistory_df = pl.DataFrame(history).with_columns(\n    pl.col(\"release_date\").str.to_date(),\n    pl.col(\"submitted_at\").str.to_datetime(),\n)\nhistory_df\n</pre> history = client.get_performance() history_df = pl.DataFrame(history).with_columns(     pl.col(\"release_date\").str.to_date(),     pl.col(\"submitted_at\").str.to_datetime(), ) history_df <pre>2025-12-30 15:08:08,479 - INFO - Fetching submission history for 'hyperliquid-ranking'...\n2025-12-30 15:08:10,410 - INFO - Loaded 949 scored submissions.\n</pre> Out[4]: shape: (949, 14)idslotrelease_datesubmitted_atstatusscore_ndcg@40_10dscore_ndcg@40_30dscore_spearman_10dscore_spearman_30dpercentile_ndcg@40_10dpercentile_ndcg@40_30dpercentile_spearman_10dpercentile_spearman_30dcomposite_percentilei64i64datedatetime[\u03bcs, UTC]strf64f64f64f64f64f64f64f64f64588052025-12-282025-12-28 14:44:37.055198 UTC\"pending\"0.3377160.340841-0.533123-0.5232278.1081088.10810810.81081110.8108119.459459587942025-12-282025-12-28 14:44:36.327841 UTC\"pending\"0.4216110.419226-0.417002-0.38445362.16216248.64864970.2702762.16216260.810811587832025-12-282025-12-28 14:44:35.920831 UTC\"pending\"0.3855170.388523-0.499784-0.46401316.21621618.91891927.02702735.13513524.324324587722025-12-282025-12-28 14:44:34.445854 UTC\"pending\"0.4529580.423994-0.47497-0.46916681.08108154.05405459.45945929.7297356.081081587612025-12-282025-12-28 14:44:34.074837 UTC\"pending\"0.4147940.492791-0.415315-0.24205356.75675789.18918972.97297383.78378475.675676\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u20261422025-06-062025-06-06 16:04:20.309350 UTC\"evaluated\"0.6478970.634970.1451750.13358866.66666766.66666766.66666766.666667null1312025-06-062025-06-06 16:04:19.820281 UTC\"evaluated\"0.6478970.634970.1451750.13358866.66666766.66666766.66666766.666667null1222025-06-052025-06-05 15:06:38.631117 UTC\"evaluated\"0.5405380.6514260.0043780.11945966.66666766.66666766.66666766.666667null1132025-06-052025-06-05 15:04:02.653942 UTC\"evaluated\"0.558550.6904960.0512450.227518100.0100.0100.0100.0null1012025-06-052025-06-05 14:37:09.450635 UTC\"evaluated\"0.5405380.6514260.0043780.11945966.66666766.66666766.66666766.666667null <p>Each dict includes:</p> <ul> <li><code>id</code>, <code>slot</code>, <code>release_date</code>, <code>submitted_at</code>, <code>status</code> \u2014 submission metadata</li> <li><code>score_*</code> keys \u2014 raw metric scores (e.g., <code>score_spearman_10d</code>)</li> <li><code>percentile_*</code> keys \u2014 your percentile rank vs other participants</li> <li><code>composite_percentile</code> \u2014 overall ranking (if available)</li> </ul> In\u00a0[5]: Copied! <pre># Average percentile by slot\nslot_performance = (\n    history_df.group_by(\"slot\")\n    .agg(\n        pl.col(\"composite_percentile\").mean().alias(\"avg_percentile\"),\n        pl.col(\"composite_percentile\").std().alias(\"std_percentile\"),\n        pl.col(\"composite_percentile\").count().alias(\"submissions\"),\n    )\n    .sort(\"avg_percentile\", descending=True)\n)\nslot_performance\n</pre> # Average percentile by slot slot_performance = (     history_df.group_by(\"slot\")     .agg(         pl.col(\"composite_percentile\").mean().alias(\"avg_percentile\"),         pl.col(\"composite_percentile\").std().alias(\"std_percentile\"),         pl.col(\"composite_percentile\").count().alias(\"submissions\"),     )     .sort(\"avg_percentile\", descending=True) ) slot_performance Out[5]: shape: (5, 4)slotavg_percentilestd_percentilesubmissionsi64f64f64u32368.60137620.018743181467.55586422.640585181562.62479326.325876168253.48671821.407269194142.38483321.720835194 In\u00a0[6]: Copied! <pre>window_size = 5\n\n# Add rolling averages for percentile metrics\nhistory_df = history_df.sort(\"release_date\").with_columns(\n    [\n        pl.col(\"composite_percentile\")\n        .rolling_mean(window_size=window_size)\n        .over(\"slot\")\n        .alias(f\"composite_percentile_ma{window_size}\"),\n    ]\n)\n</pre> window_size = 5  # Add rolling averages for percentile metrics history_df = history_df.sort(\"release_date\").with_columns(     [         pl.col(\"composite_percentile\")         .rolling_mean(window_size=window_size)         .over(\"slot\")         .alias(f\"composite_percentile_ma{window_size}\"),     ] ) In\u00a0[7]: Copied! <pre>chart = (\n    alt.Chart(history_df)\n    .mark_line(point=True)\n    .encode(\n        x=alt.X(\"release_date:T\", title=\"Inference Period\"),\n        y=alt.Y(\"composite_percentile_ma5:Q\", title=\"Percentile\"),\n        color=alt.Color(\"slot:N\", title=\"Slot\"),\n        tooltip=[\"release_date\", \"slot\", \"composite_percentile\"],\n    )\n    .properties(title=\"Performance Over Time\", width=600, height=300)\n)\nchart\n</pre> chart = (     alt.Chart(history_df)     .mark_line(point=True)     .encode(         x=alt.X(\"release_date:T\", title=\"Inference Period\"),         y=alt.Y(\"composite_percentile_ma5:Q\", title=\"Percentile\"),         color=alt.Color(\"slot:N\", title=\"Slot\"),         tooltip=[\"release_date\", \"slot\", \"composite_percentile\"],     )     .properties(title=\"Performance Over Time\", width=600, height=300) ) chart Out[7]:"},{"location":"tutorials/track-your-performance/#load-api-key","title":"Load API key\u00b6","text":""},{"location":"tutorials/track-your-performance/#initialize-the-client","title":"Initialize the client\u00b6","text":""},{"location":"tutorials/track-your-performance/#get-performance-history","title":"Get Performance History\u00b6","text":"<p>Use <code>get_performance()</code> to retrieve all your scored submissions as a list of dicts:</p>"},{"location":"tutorials/track-your-performance/#analyze-performance-by-slot","title":"Analyze Performance by Slot\u00b6","text":"<p>If you're using multiple slots for different strategies, compare them:</p>"},{"location":"tutorials/track-your-performance/#visualize-performance-over-time","title":"Visualize Performance Over Time\u00b6","text":"<p>Plot your percentile trajectory with a moving average:</p>"}]}